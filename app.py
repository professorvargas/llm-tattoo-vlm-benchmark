"""
Ponto de entrada da aplica√ß√£o Streamlit.

Respons√°vel por:
- Inicializa√ß√£o da UI
- Controle de estado do chat
- Orquestra√ß√£o entre RAG e Vision
"""

import streamlit as st

from config.settings import setup_page
from ui.sidebar import render_sidebar
from core.embeddings import get_vectorstore
from core.rag_chain import build_rag_chain
from services.document_loader import load_documents
from services.chat_service import chat_stream
from core.llm_vision import analyze_image_stream
from core.history import get_session_history

# ==================================================
# Configura√ß√£o inicial
# ==================================================
setup_page()
st.title("üíâüé®üñºÔ∏è InkVision: Multimodal Tattoo Chatbot")

user_id = st.text_input("User ID", "clayton")

# ==================================================
# Estado da sess√£o (UI)
# ==================================================
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "image_docs" not in st.session_state:
    st.session_state.image_docs = []

# ==================================================
# Bot√£o para nova conversa
# ==================================================
if st.button("üßπ Nova conversa"):
    st.session_state.chat_history = []
    st.session_state.image_docs = []
    get_session_history(user_id).clear()

# ==================================================
# Renderiza√ß√£o do hist√≥rico do chat
# ==================================================
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ==================================================
# VectorStore + RAG
# ==================================================
vectorstore = get_vectorstore()
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
rag = build_rag_chain(retriever)

# ==================================================
# Sidebar ‚Äì Upload e indexa√ß√£o
# ==================================================
uploaded_files = render_sidebar()

if uploaded_files and st.sidebar.button("üì• Indexar documentos"):
    with st.spinner("Processando documentos..."):
        text_docs, image_docs = load_documents(uploaded_files)

        if text_docs:
            from langchain_text_splitters import RecursiveCharacterTextSplitter

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=80,
            )

            chunks = splitter.split_documents(text_docs)
            vectorstore.add_documents(chunks)
            st.sidebar.success("‚úÖ Documentos textuais indexados!")

        if image_docs:
            st.session_state.image_docs.extend(image_docs)
            st.sidebar.success("üñºÔ∏è Imagens carregadas para interpreta√ß√£o!")

# ==================================================
# Entrada do usu√°rio (BARRA DE CHAT)
# ==================================================
user_prompt = st.chat_input(
    "Pergunte algo sobre documentos ou desenhos..."
)

# ==================================================
# Processamento do chat
# ==================================================
if user_prompt:
    # ---- Mensagem do usu√°rio ----
    st.session_state.chat_history.append(
        {"role": "user", "content": user_prompt}
    )

    with st.chat_message("user"):
        st.markdown(user_prompt)

    # ---- RESPOSTA DO ASSISTENTE ----
    with st.chat_message("assistant"):

        # Caso exista imagem carregada ‚Üí Vision
        if st.session_state.image_docs:
            response = st.write_stream(
                analyze_image_stream(
                    st.session_state.image_docs[-1]["path"],
                    user_prompt,
                )
            )

        # Caso contr√°rio ‚Üí RAG textual
        else:
            response = st.write_stream(
                chat_stream(
                    rag,
                    user_id,
                    user_prompt,
                )
            )

    # ---- Salva resposta no hist√≥rico ----
    st.session_state.chat_history.append(
        {"role": "assistant", "content": response}
    )
