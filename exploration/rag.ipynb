{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b95f832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# PyTorch: 2.7.1+cu118\n",
    "# CUDA disponível: True\n",
    "# CUDA version: 11.8\n",
    "# GPU: NVIDIA GeForce GTX 1050 Ti\n",
    "# Compute Capability: (6, 1)\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponível: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "print(f\"Arquiteturas suportadas: {torch.cuda.get_arch_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9cc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações globais do pandas para não truncar dados nos resultados\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7854fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "BASE_URL = \"http://localhost:11434\"\n",
    "MODEL = 'gemma3:4b'\n",
    "# MODEL = 'gemma3:12b'\n",
    "# MODEL = 'llama3.2:1b'\n",
    "# model = 'llama3.2:1b'\n",
    "# model = 'sheldon'\n",
    "# model = 'sherlock'\n",
    "\n",
    "# model = 'uncensored'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    # URL do servidor Ollama\n",
    "    base_url=BASE_URL,\n",
    "    # Modelo de linguagem (LLaMA ou Gemma)\n",
    "    model=MODEL,\n",
    "    # ------------------------------------------------------------------\n",
    "    # PARÂMETROS DE CRIATIVIDADE / ALEATORIEDADE\n",
    "    # ------------------------------------------------------------------\n",
    "    # Controla aleatoriedade (0.0 = determinístico)\n",
    "    temperature=0.1,\n",
    "    # Nucleus sampling\n",
    "    top_p=0.9,\n",
    "    # Top-K sampling\n",
    "    top_k=40,\n",
    "    # ------------------------------------------------------------------\n",
    "    # CONTROLE DE REPETIÇÃO\n",
    "    # ------------------------------------------------------------------\n",
    "    # Penaliza repetições\n",
    "    repeat_penalty=1.15,\n",
    "    # Introdução de novos conceitos\n",
    "    presence_penalty=0.0,\n",
    "    # Penalização por frequência\n",
    "    frequency_penalty=0.0,\n",
    "    # ------------------------------------------------------------------\n",
    "    # PARÂMETROS ESTRUTURAIS\n",
    "    # ------------------------------------------------------------------\n",
    "    # Janela de contexto\n",
    "    num_ctx=4096,\n",
    "    # Máx. tokens gerados\n",
    "    num_predict=512,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"O que significa a sigla RAG?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando os Dados (Documentos)\n",
    "# Vamos começar com textos simples (isso vale igualmente para PDFs, Word, etc.).\n",
    "# Em produção, esses textos viriam de arquivos ou banco de dados.\n",
    "\n",
    "documentos = [\n",
    "\"RAG combina recuperação de informação com modelos de linguagem.\",\n",
    "\"LLMs podem alucinar quando não possuem contexto externo.\",\n",
    "\"FAISS é uma biblioteca eficiente para busca vetorial local.\",\n",
    "\"Ollama permite rodar modelos LLM localmente via API.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando Embeddings (transformar texto em vetores)\n",
    "# LLMs não fazem busca. Quem faz busca são embeddings.\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Vector Store (FAISS)\n",
    "# O FAISS guarda vetores e permite busca por similaridade.\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "texts=documentos,\n",
    "embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Retriever (busca inteligente)\n",
    "# O retriever define quantos documentos buscar.\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Prompt RAG (parte mais importante)\n",
    "# O prompt obriga o modelo a usar o contexto.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Você é um assistente técnico.\n",
    "Use APENAS o contexto abaixo para responder.\n",
    "Se a resposta não estiver no contexto, diga que não sabe.\n",
    "\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando o RAG com LCEL (forma moderna)\n",
    "# LCEL = LangChain Expression Language.\n",
    "# A pergunta vira embedding\n",
    "# O retriever busca contexto\n",
    "# O LLM responde com base nele\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "{\n",
    "\"context\": retriever,\n",
    "\"question\": lambda x: x\n",
    "}\n",
    "| prompt\n",
    "| llm\n",
    "| StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"O que é alucinação no contexto LLM?\"\n",
    "\n",
    "resposta = rag_chain.invoke(pergunta)\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b08a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "texto_pdf = load_pdf(\"datasets/Open-Set Tattoo Semantic Segmentation - Brilhador 2024.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "\n",
    "def load_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "\n",
    "texto_docx = load_docx(\"documento.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking com LangChain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "chunk_size=800,\n",
    "chunk_overlap=120,\n",
    "separators=[\n",
    "\"\\n\\n\", # parágrafos\n",
    "\"\\n\", # quebras de linha\n",
    "\". \", # frases\n",
    "\" \",\n",
    "\"\"\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb6f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89fc8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks PDF: 165\n",
      "Total de Documents finais: 165\n",
      "Vector store FAISS criado com sucesso.\n",
      "\n",
      "Resposta do RAG:\n",
      "\n",
      "Não sei.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEste é o padrão correto de RAG:\\n\\n- O usuário NÃO fala direto com o LLM\\n- O prompt controla o comportamento\\n- O contexto vem EXCLUSIVAMENTE do retriever\\n- Isso reduz alucinação e permite avaliação objetiva\\n\\nPróximo passo natural:\\n- Avaliação com Recall@k\\n- Avaliação de Faithfulness (RAGAS)\\n- API FastAPI para produção\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tutorial Prático de RAG – Chunking correto de PDFs e Word (com overlap)\n",
    "=====================================================================\n",
    "\n",
    "Objetivo:\n",
    "- Ler PDFs e DOCX\n",
    "- Aplicar chunking correto com overlap\n",
    "- Gerar Documents com metadados\n",
    "- Criar embeddings\n",
    "- Armazenar em FAISS (pronto para RAG)\n",
    "\n",
    "Requisitos:\n",
    "- Python 3.11\n",
    "- Ollama já configurado (não usado diretamente aqui)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. INSTALAÇÃO (executar uma única vez no ambiente)\n",
    "# ============================================================\n",
    "# pip install pypdf python-docx\n",
    "# pip install langchain langchain-community langchain-text-splitters\n",
    "# pip install sentence-transformers faiss-cpu\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. IMPORTS\n",
    "# ============================================================\n",
    "import os \n",
    "\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. FUNÇÕES DE LEITURA DE ARQUIVOS\n",
    "# ============================================================\n",
    "\n",
    "def load_pdf(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lê um arquivo PDF e retorna todo o texto como uma string.\n",
    "    PDFs escaneados exigem OCR (não tratado aqui).\n",
    "    \"\"\"\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_docx(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lê um arquivo Word (.docx) e retorna o texto consolidado.\n",
    "    \"\"\"\n",
    "    doc = Document(path)\n",
    "    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. CONFIGURAÇÃO DO CHUNKING (PARTE MAIS CRÍTICA DO RAG)\n",
    "# ============================================================\n",
    "\n",
    "# Parâmetros recomendados para modelos pequenos/médios\n",
    "CHUNK_SIZE = 800        # tamanho do chunk em caracteres\n",
    "CHUNK_OVERLAP = 120     # overlap (~15%)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\n",
    "        \"\\n\\n\",  # prioriza parágrafos\n",
    "        \"\\n\",\n",
    "        \". \",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. CARREGAMENTO DOS DOCUMENTOS\n",
    "# ============================================================\n",
    "\n",
    "# Ajuste os caminhos conforme necessário\n",
    "pdf_text = load_pdf(\"datasets/tattoo.pdf\")\n",
    "# docx_text = load_docx(\"datasets/COBIT 4 para concursos - Bruno Marota.docx\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. GERAÇÃO DOS CHUNKS\n",
    "# ============================================================\n",
    "\n",
    "pdf_chunks = text_splitter.split_text(pdf_text)\n",
    "# docx_chunks = text_splitter.split_text(docx_text)\n",
    "\n",
    "print(f\"Chunks PDF: {len(pdf_chunks)}\")\n",
    "# print(f\"Chunks DOCX: {len(docx_chunks)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. CRIAÇÃO DE DOCUMENTS COM METADADOS\n",
    "# ============================================================\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Chunks do PDF\n",
    "for i, chunk in enumerate(pdf_chunks):\n",
    "    documents.append(\n",
    "        LCDocument(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": \"documento.pdf\",\n",
    "                \"chunk_id\": i,\n",
    "                \"type\": \"pdf\",\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chunks do DOCX\n",
    "# for i, chunk in enumerate(docx_chunks):\n",
    "#     documents.append(\n",
    "#         LCDocument(\n",
    "#             page_content=chunk,\n",
    "#             metadata={\n",
    "#                 \"source\": \"documento.docx\",\n",
    "#                 \"chunk_id\": i,\n",
    "#                 \"type\": \"docx\",\n",
    "#             },\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "print(f\"Total de Documents finais: {len(documents)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. EMBEDDINGS (TRANSFORMA TEXTO EM VETORES)\n",
    "# ============================================================\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. VECTOR STORE (FAISS)\n",
    "# ============================================================\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "print(\"Vector store FAISS criado com sucesso.\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "\n",
    "# VECTORSTORE_DIR = \"vectorstore_faiss\"\n",
    "# os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "# vectorstore.save_local(VECTORSTORE_DIR)\n",
    "# print(f\"Vector store salvo em: {VECTORSTORE_DIR}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. PROMPT RAG (SUBSTITUI QUERY DIRETA)\n",
    "# ============================================================\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Você é um assistente técnico.\n",
    "Use APENAS o contexto abaixo para responder.\n",
    "Se a resposta não estiver no contexto, diga que não sabe.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11. FUNÇÃO AUXILIAR PARA FORMATAR CONTEXTO\n",
    "# ============================================================\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12. PIPELINE RAG (LCEL – PADRÃO MODERNO)\n",
    "# ============================================================\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": lambda x: x,\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 13. EXECUÇÃO\n",
    "# ============================================================\n",
    "\n",
    "pergunta = \"Para que serve as tatuagens?\"\n",
    "\n",
    "resposta = rag_chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\nResposta do RAG:\\n\")\n",
    "print(resposta)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 14. OBSERVAÇÃO PROFISSIONAL\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Este é o padrão correto de RAG:\n",
    "\n",
    "- O usuário NÃO fala direto com o LLM\n",
    "- O prompt controla o comportamento\n",
    "- O contexto vem EXCLUSIVAMENTE do retriever\n",
    "- Isso reduz alucinação e permite avaliação objetiva\n",
    "\n",
    "Próximo passo natural:\n",
    "- Avaliação com Recall@k\n",
    "- Avaliação de Faithfulness (RAGAS)\n",
    "- API FastAPI para produção\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60fb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta do RAG:\n",
      "\n",
      "Não sei.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. EXECUÇÃO\n",
    "# ============================================================\n",
    "\n",
    "pergunta = \"Quem são os autores do artigo científico?\"\n",
    "\n",
    "resposta = rag_chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\nResposta do RAG:\\n\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c4ad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks no vectorstore: 165\n",
      "================================================================================\n",
      "Doc ID: 04c585af-e686-4517-af6d-5a41b4d30513\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 0\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 752\n",
      "--------------------------------------------------------------------------------\n",
      "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n",
      "Digital Object Identifier 10.1109/ACCESS.2017.DOI\n",
      "Open-Set Tattoo Semantic Segmentation\n",
      "ANDERSON BRILHADOR 1, RODRIGO TCHALSKI DA SIL VA 1, CARLOS ROBERTO\n",
      "MODINEZ-JUNIOR 1, GABRIEL DE ALMEIDA SP ADAFORA 1, HEITOR SIL VÉRIO\n",
      "LOPES 1, AND ANDRÉ EUGÊNIO LAZZARETTI 1, (Member, IEEE).\n",
      "1Federal University of Technology - Paraná, Av. Sete de Setembro, 3165, Curitiba, 80230-901, Paraná, Brazil.\n",
      "Corresponding author: Anderson Brilhador (e-mail: andersonbrilhador@gmail.com).\n",
      "ABSTRACT Tattoos can serve as an essential source of biometric information for public security, aiding\n",
      "in identifying suspects and victims. In order to automate tattoo classification, tasks like classification\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 43de1b50-e279-4175-8273-2788b772be78\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 1\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 733\n",
      "--------------------------------------------------------------------------------\n",
      "in identifying suspects and victims. In order to automate tattoo classification, tasks like classification\n",
      "require more detailed image content analysis, such as semantic segmentation. However, a dataset with\n",
      "appropriate semantic segmentation annotations is currently lacking. Also, there are countless ways to\n",
      "categorize tattoo classes, and many are not directly categorizable, either because they belong to a specific\n",
      "artistic trait or characterize an object with previously undefined semantics. An effective way to overcome\n",
      "these limitations is to build recognition systems based on open-set assumptions. Nevertheless, state-of-\n",
      "the-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ab2de4df-bdf1-4525-afe0-14527b3a987e\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 2\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 723\n",
      "--------------------------------------------------------------------------------\n",
      "the-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to\n",
      "the significant class imbalance (predominant background). To the best of our knowledge, this paper is\n",
      "the first to explore semantic segmentation in closed and open-set scenarios for tattoos. In this sense, this\n",
      "paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set\n",
      "classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible\n",
      "test set, enabling comparisons and future research in this area. The proposed approach outperforms other\n",
      "methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d48bc504-839a-4442-86d3-2ebd4b1405b7\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 3\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 779\n",
      "--------------------------------------------------------------------------------\n",
      "methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU\n",
      "for the unknown class, demonstrating the feasibility of this approach for automatic tattoo analysis. The\n",
      "paper also highlights key limitations and open research areas in this challenging field. Dataset and codes\n",
      "are available at https://github.com/Brilhador/tssd2023.\n",
      "INDEX TERMS open-world, open-set, semantic segmentation, large-margin learning, tattoo classifica-\n",
      "tion.\n",
      "I. INTRODUCTION\n",
      "Tattoos are forms of human expression and are also con-\n",
      "sidered an art. In their almost unique features, tattoos\n",
      "go beyond artistic expressions and can serve as essential\n",
      "sources of biometric information. Consequently, it can be\n",
      "useful in identifying their bearers, mainly for public security\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e3624847-6390-41d3-aae7-8fea05c2a161\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 4\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 741\n",
      "--------------------------------------------------------------------------------\n",
      "useful in identifying their bearers, mainly for public security\n",
      "[1], [2] because tattoos can be used to identify not only\n",
      "suspects but also victims [3], [4]. In addition, the subject\n",
      "has raised studies on ethical and social issues that may\n",
      "encompass the topic [5].\n",
      "Compared to other biometrics, tattoos bring a series of\n",
      "characteristics that make them very difficult to recognize.\n",
      "Other biometrics usually have well-defined standards, robust\n",
      "techniques, well-established methods for their treatment and\n",
      "recognition, standardized data capture and storage, and other\n",
      "factors that help their reliability and robustness. However,\n",
      "tattoos still need to have such characteristics and require-\n",
      "ments. Apart from the issues related to processing and\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: baa82ae0-067d-47e1-8fa7-db283c88e070\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 5\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 745\n",
      "--------------------------------------------------------------------------------\n",
      "tattoos still need to have such characteristics and require-\n",
      "ments. Apart from the issues related to processing and\n",
      "using general biometrics, tattoo recognition has a singular\n",
      "complexity because it can be divided into several sub-\n",
      "problems, each equally significant [6].\n",
      "First, an image can be submitted to detect, locate, and\n",
      "segment (outline or instance) the tattoo contained therein.\n",
      "Subsequently, the image can be classified, de-identified,\n",
      "or re-identified (image-to-image, sketch-to-image, partial,\n",
      "or similar). After preprocessing an image, the best result\n",
      "could be a well-segmented tattoo without any pollution or\n",
      "background. Then, for all classification, de-identification, or\n",
      "re-identification tasks, these images contain only the most\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 23ad47a2-c7b0-4a22-92e7-8a81a2f47224\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 6\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 726\n",
      "--------------------------------------------------------------------------------\n",
      "re-identification tasks, these images contain only the most\n",
      "essential information to store and, later, process [6].\n",
      "Nonetheless, tasks that consider the meaning of the\n",
      "content of images, such as classification, may require a\n",
      "more detailed separation of objects in a tattoo image, called\n",
      "semantic segmentation [7]. At this point, a segmented tattoo\n",
      "could identify and detach each object in the tattoo, after\n",
      "which each could be analyzed separately. For instance,\n",
      "security and biometric recognition systems could benefit\n",
      "from tattoo semantic split at the pixel-level for fine-grained\n",
      "VOLUME 4, 2016 1\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5e13d2a5-db26-463e-9254-6bc5106d049b\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 7\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 774\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "feature extraction by semantic classes, reduce false posi-\n",
      "tives by precisely delineating the boundaries of biometrics\n",
      "features, and allow selective anonymization of regions of an\n",
      "image. Moreover, tattoo biometrics systems may require the\n",
      "identification of multiple semantic classes in multiple areas\n",
      "of the image, which can be performed accurately by tattoo\n",
      "semantic segmentations.\n",
      "Although this topic has been widely explored in studies\n",
      "related to images and videos in many different areas, se-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 66ec5651-63c1-440f-a804-042d44957741\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 8\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 757\n",
      "--------------------------------------------------------------------------------\n",
      "Although this topic has been widely explored in studies\n",
      "related to images and videos in many different areas, se-\n",
      "mantic segmentation is still underexplored in the context\n",
      "of tattoos. Several works only focus on tattoo classification\n",
      "and detection [6], [8], and only a few researches deal with\n",
      "segmentation [9], [10] without identifying the semantics of\n",
      "the components that compose the tattoos.\n",
      "One of the reasons that make semantic segmentation dif-\n",
      "ficult is related to the complexity that tattooing can have. As\n",
      "mentioned, tattoos are expressions of art, and their features\n",
      "can be as varied as possible and imaginable. In this way, ob-\n",
      "jects can be positioned very closely, mixed, overlapped, and\n",
      "distorted, and abstract images can also be present, among\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: cdef0e28-5686-4e09-85e3-58594a582a3f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 9\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 743\n",
      "--------------------------------------------------------------------------------\n",
      "jects can be positioned very closely, mixed, overlapped, and\n",
      "distorted, and abstract images can also be present, among\n",
      "many other hindering factors [11]. Additionally, the lack\n",
      "of public and comprehensive datasets makes it even more\n",
      "challenging to develop efficient methods for segmentation.\n",
      "Some recent works propose public datasets, such as [12]\n",
      "and [13]. However, in the case of [13], using semi-synthetic\n",
      "images without employing the semantics associated with\n",
      "each class makes it difficult to generalize the proposed –\n",
      "characteristic also observed in [12].\n",
      "Still, in this context, it is essential to emphasize the\n",
      "complexity and semantic variability of tattoo classes. Tattoo\n",
      "categories can vary significantly, and some are not easily cat-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: acf9ccbc-ea62-4e46-a605-6c5592cfb987\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 10\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 781\n",
      "--------------------------------------------------------------------------------\n",
      "categories can vary significantly, and some are not easily cat-\n",
      "egorized due to their association with specific artistic styles\n",
      "or objects with undefined semantics. The tattoo recognition\n",
      "scenario, especially from a public safety perspective, is also\n",
      "somewhat challenging, especially given the circumstances\n",
      "in which the information is obtained and analyzed. It is not\n",
      "uncommon for tattoo information to be obtained partially,\n",
      "and, therefore, semantic segmentation has great relevance\n",
      "in the identification process, as in the following scenarios:\n",
      "(i) semantic segmentation can be used to create databases\n",
      "with automatic textual annotations, as it is common for a\n",
      "witness or victim to remember or have visual contact with\n",
      "only parts of a tattoo of a wanted person, and, in this way,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 27f48db4-5a12-4df0-b9ce-27b1eeb55293\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 11\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 788\n",
      "--------------------------------------------------------------------------------\n",
      "witness or victim to remember or have visual contact with\n",
      "only parts of a tattoo of a wanted person, and, in this way,\n",
      "from the description, it would be possible to identify tattoos\n",
      "with those parts visualized; (ii) semantic segmentation is\n",
      "important in the pre-processing of images in preparation\n",
      "for information recognition processes, such as for partial\n",
      "re-identification of tattoos, wherein an automatic process,\n",
      "the segmented parts can be recovered separately in cases of\n",
      "partial image collections [6].\n",
      "An effective way to overcome these limitations is to\n",
      "build recognition systems based on dynamic and open-set\n",
      "perception. These systems are designed to handle objects\n",
      "from unknown classes commonly encountered in real-world\n",
      "applications. Open-set recognition has extensively studied\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a95b5f16-99ef-41fc-81e8-1871647b281e\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 12\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 763\n",
      "--------------------------------------------------------------------------------\n",
      "from unknown classes commonly encountered in real-world\n",
      "applications. Open-set recognition has extensively studied\n",
      "the ability to recognize new classes [14]. Open-set semantic\n",
      "segmentation, in turn, is an approach that incorporates open-\n",
      "set perception into semantic segmentation. The main differ-\n",
      "ence with closed-set semantic segmentation is that open-\n",
      "set semantic segmentation must correctly classify samples\n",
      "belonging to known classes while rejecting those belonging\n",
      "to unknown classes. In the context of this work, open-set\n",
      "semantic segmentation can be an ally in improving databases\n",
      "and models for identifying and classifying tattoo objects,\n",
      "allowing the improvement of annotations and descriptions\n",
      "of complex tattoos. Therefore, semantic segmentation must\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: b6e712ba-218b-4c08-a4f9-d7415bb7cfbd\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 13\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 741\n",
      "--------------------------------------------------------------------------------\n",
      "allowing the improvement of annotations and descriptions\n",
      "of complex tattoos. Therefore, semantic segmentation must\n",
      "also be seen as a middle process, not just as an end process\n",
      "in the tattoo recognition roadmap.\n",
      "Studies have explored the use of open-set semantic seg-\n",
      "mentation in different applications [15], [16]. These studies\n",
      "focus on adapting or building open-set classifiers to make\n",
      "closed-set semantic segmentation models capable of recog-\n",
      "nizing unknown classes. While the outcomes of these studies\n",
      "are promising, the performance of these approaches is lim-\n",
      "ited due to the low representation of the obtained features,\n",
      "resulting in an “irregular” logit space with low discrimi-\n",
      "nation among the classes. Recent research [17], [18] has\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1cd3f915-c9a8-413a-a4bc-e2188a2b9554\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 14\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 749\n",
      "--------------------------------------------------------------------------------\n",
      "resulting in an “irregular” logit space with low discrimi-\n",
      "nation among the classes. Recent research [17], [18] has\n",
      "demonstrated that incorporating metric learning techniques\n",
      "can enhance open-set recognition. Metric learning aids in\n",
      "obtaining more discriminative features and building a logit\n",
      "space that tightly clusters known classes while maintaining\n",
      "a considerable distance from unknown classes. However, it\n",
      "is essential to acknowledge that applying metric learning\n",
      "in the context of semantic segmentation can be impractical.\n",
      "This is primarily due to the exponential complexity of the\n",
      "task, as calculating pairwise distances among logit vectors\n",
      "of pixels becomes computationally expensive.\n",
      "Recent studies have investigated the potential of large-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: fb7c79a3-0521-4780-8417-7acef63c6b7c\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 15\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 792\n",
      "--------------------------------------------------------------------------------\n",
      "of pixels becomes computationally expensive.\n",
      "Recent studies have investigated the potential of large-\n",
      "margin learning to acquire more discriminative fea-\n",
      "tures, yielding improved outcomes in image classification\n",
      "tasks [19]–[21]. Hence, strategies established on large-\n",
      "margin learning present promising and viable alternatives\n",
      "for building a well-defined logit space that enhances the\n",
      "separation among decision boundaries of semantic classes.\n",
      "Motivated by these results, our study explores the dis-\n",
      "criminative capabilities of large-margin learning to produce\n",
      "more distinctive features for tattoo semantic segmentation.\n",
      "This approach effectively increases the spatial separation\n",
      "among decision boundaries to different semantic classes,\n",
      "forcing the build of ideal logit space as illustrated in\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 03dd09df-f5a9-435f-b213-32207b36b7fd\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 16\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 787\n",
      "--------------------------------------------------------------------------------\n",
      "among decision boundaries to different semantic classes,\n",
      "forcing the build of ideal logit space as illustrated in\n",
      "Figure 1b. Furthermore, this expanded separation among\n",
      "decision boundaries will set the stage for accommodating\n",
      "unknown classes in the future, enhancing the effectiveness\n",
      "of tattoo semantic segmentation within an open-set scenario.\n",
      "Visual comparisons depicting the differences in logit space\n",
      "resulting from the presence of unknown classes can be\n",
      "observed in Figure 1.\n",
      "Given the limitations presented so far and the fact that,\n",
      "to the extent of our knowledge, open-set classification has\n",
      "not been used in the context of tattoo recognition and is\n",
      "an open research gap, this work aims to propose a new\n",
      "large-margin-based loss function adapted to the context of\n",
      "2 VOLUME 4, 2016\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d6e28ee4-0e4b-45df-bbd1-0bdee90648b7\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 17\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 793\n",
      "--------------------------------------------------------------------------------\n",
      "large-margin-based loss function adapted to the context of\n",
      "2 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "Logit Space Label Space\n",
      "Closed Set \n",
      "Segmentation\n",
      "Closed Set \n",
      "Segmentation with \n",
      "Unknown classes\n",
      "Open Set \n",
      "Segmentation with \n",
      "“Ideal” Logit Space\n",
      "(a) (b) (c) (d)\n",
      "Background\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "Unknown \n",
      "classes\n",
      "E.g. Bee. was \n",
      "defined as \n",
      "Unknown Class.\n",
      "Open Set \n",
      "Segmentation with \n",
      "“Irregular” Logit Space\n",
      "Stem\n",
      "Leaf\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d2acda9c-d704-4478-a2fe-69c6217ce096\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 18\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 790\n",
      "--------------------------------------------------------------------------------\n",
      "classes\n",
      "E.g. Bee. was \n",
      "defined as \n",
      "Unknown Class.\n",
      "Open Set \n",
      "Segmentation with \n",
      "“Irregular” Logit Space\n",
      "Stem\n",
      "Leaf\n",
      "Flower\n",
      "Background\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "Background\n",
      "Input Image\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "Background\n",
      "Unk. Class\n",
      "Stem\n",
      "Leaf\n",
      "Flower\n",
      "Background\n",
      "Unk. Class\n",
      "Flower Stem\n",
      "Leaf\n",
      "Unk. Class\n",
      "Background\n",
      "Building\n",
      "Imp. Surfaces\n",
      "Car\n",
      "Tree\n",
      "Unk. Class\n",
      "Open Space\n",
      "Flower Stem\n",
      "Leaf\n",
      "Unk. ClassBackground\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "Background\n",
      "Unk. Class\n",
      "Unk. Class\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "Background\n",
      "Background\n",
      "FIGURE 1. Closed set and open set in dense labeling scenarios. The label space represents the pixel-level predictions, and logit space refers to a subset of pixel\n",
      "samples in a 2D manifold separated by labels and decision boundaries for each class. a) Closed set without the presence of unknown classes. b) Closed set with\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ea945283-2f31-498b-b6a6-3561f47360b2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 19\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 761\n",
      "--------------------------------------------------------------------------------\n",
      "the presence of unknown classes. These unknown classes result in misclassification, being segmented as known classes. c) Open set segmentation with “irregular”\n",
      "logit space. The term “irregular” suggests undefined or overlapping decision boundaries among classes. d) Open set segmentation with “ideal” logit space. The\n",
      "term “ideal” refers to well-defined decision regions between the classes and enough space (Open space) between them to include new classes [14], [18].\n",
      "open set semantic segmentation in tattoos. This novel loss\n",
      "function seeks to overeat the closed set segmentation results\n",
      "and enable and increase the open set segmentation results\n",
      "compared to other state-of-the-art semantic segmentation\n",
      "loss functions. Furthermore, we propose using a publicly\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f885ab85-ac8f-42d3-bd22-171c483757a3\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 20\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 766\n",
      "--------------------------------------------------------------------------------\n",
      "compared to other state-of-the-art semantic segmentation\n",
      "loss functions. Furthermore, we propose using a publicly\n",
      "available test dataset, with an annotation aimed at semantic\n",
      "segmentation in an open-set context, whose high complexity\n",
      "will serve as a benchmark for comparing methods in this\n",
      "area. The main contributions of this paper are then summa-\n",
      "rized as follows:\n",
      "• Test set publicly available for the dataset, allowing\n",
      "comparisons and future work in the open-set and\n",
      "closed-set scenarios;\n",
      "• Novel class semantic augmentation method to expand\n",
      "the tattoo samples;\n",
      "• Novel large-margin loss function for open-set tattoo\n",
      "semantic segmentation to build more discriminative\n",
      "features and handle the class imbalance;\n",
      "• A generalized open-set classifier approach based on\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 13b5f6db-dc3a-476f-a976-05b62b7c3743\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 21\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 788\n",
      "--------------------------------------------------------------------------------\n",
      "features and handle the class imbalance;\n",
      "• A generalized open-set classifier approach based on\n",
      "open principal component scoring with incremental\n",
      "learning called G-OpenIPCS;\n",
      "• Detailed and in-depth comparison with different state-\n",
      "of-the-art loss functions and open-set semantic segmen-\n",
      "tation methods;\n",
      "• Statement of the main challenges for the open-set tattoo\n",
      "semantic segmentation.\n",
      "This paper is organized as follows. Section II presents\n",
      "related works, mainly including related datasets, tattoo\n",
      "segmentation, and open-set semantic segmentation. Our\n",
      "proposed methods, particularly the novel tattoo semantic\n",
      "segmentation dataset, novel tattoo semantic augmentation\n",
      "method, and the novel loss function and open set classifier,\n",
      "are detailed in Section III. The experiment setup is discussed\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e3080cf5-02ca-4105-a2a6-44cf89aa1036\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 22\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 794\n",
      "--------------------------------------------------------------------------------\n",
      "are detailed in Section III. The experiment setup is discussed\n",
      "in Section IV, with results, discussions, and comparisons\n",
      "with state-of-the-art approaches presented in Section V.\n",
      "Finally, Section VI shows the conclusions, with the open\n",
      "challenges in open-set tattoo semantic segmentation dis-\n",
      "cussed in Section VII.\n",
      "II. RELATED WORKS\n",
      "In order to present a general overview of the state-of-the-\n",
      "art in the context of tattoo segmentation, we divided the\n",
      "related works into three parts where this paper presents\n",
      "main contributions: datasets, tattoo segmentation, and open\n",
      "set semantic segmentation. The following subsections detail\n",
      "each of these works, pointing out the innovative aspects of\n",
      "this work on each front.\n",
      "A. AVAILABLE DATASETS\n",
      "Regarding the datasets, we chose to organize in the Table 1\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a6f29fcc-c6e7-4119-996e-060b8c49810f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 23\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 792\n",
      "--------------------------------------------------------------------------------\n",
      "this work on each front.\n",
      "A. AVAILABLE DATASETS\n",
      "Regarding the datasets, we chose to organize in the Table 1\n",
      "a summary of the main characteristics of the most common\n",
      "datasets in tattoo detection, classification, and segmentation\n",
      "problems. These criteria include the number of samples,\n",
      "public availability, and type of annotation (classification,\n",
      "detection of objects with bounding box – BB, or segmen-\n",
      "tation). Regarding the annotation focused on segmentation,\n",
      "we also included whether it contains semantic segmentation.\n",
      "TABLE 1. Tattoo datasets.\n",
      "Ref. Total Public? Annot. Semantic?\n",
      "[12] 7,526 No BB -\n",
      "[9] 890 Yes Seg. No\n",
      "[22] 5,740 Yes Class. -\n",
      "[23] 5,000 Yes BB -\n",
      "[24] 210 Yes Class. -\n",
      "[13] 5,500 Yes Seg. No\n",
      "Ours 2,106 315 Seg. Yes\n",
      "In the case of [12], despite the relatively large number of\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 11037263-4152-42dd-8751-1d95c8cac053\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 24\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 760\n",
      "--------------------------------------------------------------------------------\n",
      "[13] 5,500 Yes Seg. No\n",
      "Ours 2,106 315 Seg. Yes\n",
      "In the case of [12], despite the relatively large number of\n",
      "samples and an annotation focused on tattoo detection, the\n",
      "VOLUME 4, 2016 3\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "dataset is private, which makes comparisons, establishing\n",
      "benchmarks, and analyzing new methods for the dataset\n",
      "difficult. On the other hand, the dataset proposed in [23]\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 70cb4654-452f-4641-831e-230df3b2cb27\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 25\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 780\n",
      "--------------------------------------------------------------------------------\n",
      "benchmarks, and analyzing new methods for the dataset\n",
      "difficult. On the other hand, the dataset proposed in [23]\n",
      "is publicly available and annotated only with BB, i.e., it\n",
      "is not possible to use it in the context of segmentation.\n",
      "The datasets proposed in [24] and [22], in turn, are focused\n",
      "exclusively on classification and provide only one label for\n",
      "each image or image patch, restricting their use to multi-\n",
      "class classification problems, without the location of the\n",
      "tattoo in the image.\n",
      "[9] were the first to address tattoo segmentation. The\n",
      "proposed method aimed to de-identify soft biometric identi-\n",
      "fiers (tattoos) by discriminating tattoo and non-tattoo image\n",
      "patches with a deep neural network. In this sense, the\n",
      "proposed dataset presents a pixel-level annotation of the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 4b84fdec-3b0b-4d05-8f61-f487d3e59d8d\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 26\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 787\n",
      "--------------------------------------------------------------------------------\n",
      "patches with a deep neural network. In this sense, the\n",
      "proposed dataset presents a pixel-level annotation of the\n",
      "presence or absence of a tattoo in the image. However,\n",
      "the authors did not individualize the tattoo classes in the\n",
      "proposed annotation. The same is observed in the dataset\n",
      "presented in [13]. Furthermore, the authors presented a\n",
      "proposal using semi-synthetic images. This characteristic\n",
      "can sometimes lead to an image far from a real tattoo,\n",
      "compromising the segmentation approach.\n",
      "Our dataset, in turn, presents some original and innovative\n",
      "features that can complement currently available datasets:\n",
      "(i) Inclusion of a semantic segmentation annotation; (ii)\n",
      "Several classes of tattoos in the same image, increasing\n",
      "complexity; (iii) Quite varied sizes of tattoos and classes\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 389e2940-295d-45de-bc5a-fb5665b945de\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 27\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 770\n",
      "--------------------------------------------------------------------------------\n",
      "Several classes of tattoos in the same image, increasing\n",
      "complexity; (iii) Quite varied sizes of tattoos and classes\n",
      "of tattoos in the same image; and (iv) Tattoos in different\n",
      "regions of the body, maintaining the variability that exists in\n",
      "real situations. As will be detailed later, only the test set is\n",
      "made publicly available since most of the images available\n",
      "in these scenarios contain public use restrictions. However,\n",
      "we believe that as it is the first dataset with semantic\n",
      "annotation in segmentation, it will allow the comparison and\n",
      "evaluation of different approaches to this problem, which is\n",
      "significantly challenging.\n",
      "B. TATTOO SEGMENTATION\n",
      "Tattoo segmentation methods were presented in many stud-\n",
      "ies, but their results were suppressed, maybe because seg-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5ccb8b2f-5c84-45d7-bcbe-d869543e6191\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 28\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 750\n",
      "--------------------------------------------------------------------------------\n",
      "Tattoo segmentation methods were presented in many stud-\n",
      "ies, but their results were suppressed, maybe because seg-\n",
      "mentation was not the main focus. Furthermore, as many\n",
      "of them were carried out a long time ago, the research\n",
      "did not address deep learning methods, for example, and\n",
      "the methods cannot be directly compared with the approach\n",
      "adopted here.\n",
      "In such cases, authors have performed their researches\n",
      "using methods based on: (i) Content-Base Image Retrieval\n",
      "(CBIR) and Edge Direction Coherence Vector (EDCV) [25];\n",
      "(ii) 3×3 Sobel filter [26], Active Contour CBIR (ACCBIR),\n",
      "and Vector Field Convolution (VFC) [1]; (iii) a complex\n",
      "system combining bottom-up and top-down priorities that\n",
      "transfer tattoo segmentation to detection split-merge skin\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1f9dfaf6-cc58-4f3a-8efd-70261624c184\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 29\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 759\n",
      "--------------------------------------------------------------------------------\n",
      "system combining bottom-up and top-down priorities that\n",
      "transfer tattoo segmentation to detection split-merge skin\n",
      "detection, followed by figure-ground tattoo segmentation\n",
      "[27]; (iv) LoG (Laplacian of Gaussian) and Sobel kernel\n",
      "filters called quasi-connected components (QCC), using the\n",
      "GrabCut algorithm to produce the final segmented tattoo\n",
      "image [28]; (v) a negative image method with HSV (hue,\n",
      "saturation, and value, or lighting) model [29]; (vi) identifi-\n",
      "cation of pixels of skin in regions close to the tattoos and a\n",
      "graph-cut model based on skin color and a visual bump map\n",
      "[30], and (vii) a k-means cluster used in LAB color space to\n",
      "detect the skin area with a morphology processing used to\n",
      "smooth the clear graphic of the tattoo image segment [10].\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7a448cfc-640d-4868-8743-18f8270f009f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 30\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 752\n",
      "--------------------------------------------------------------------------------\n",
      "detect the skin area with a morphology processing used to\n",
      "smooth the clear graphic of the tattoo image segment [10].\n",
      "The main limitation of these hand-crafted-based methods is\n",
      "that the feature extractor may have adequate performance\n",
      "for some classes and datasets but significantly lower perfor-\n",
      "mance for others, with compromised generalization. This\n",
      "is accentuated for datasets with greater variability in tattoo\n",
      "images.\n",
      "As far as our research has reached, only two studies\n",
      "have used deep learning methods for the problem of tattoo\n",
      "segmentation.\n",
      "TABLE 2. Tattoo segmentation models.\n",
      "Ref. Network Semantic? Open-Set?\n",
      "[9] ConvNet No No\n",
      "[31] AlexNet+VGG No No\n",
      "[13] ViT-based No No\n",
      "Ours SegFormer Yes Yes\n",
      "Based on the study on CNNs, [9] used the structure\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: c7d5dac3-122c-479b-aedf-33865c1e327a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 31\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 758\n",
      "--------------------------------------------------------------------------------\n",
      "[31] AlexNet+VGG No No\n",
      "[13] ViT-based No No\n",
      "Ours SegFormer Yes Yes\n",
      "Based on the study on CNNs, [9] used the structure\n",
      "of a ConvNet network to train small pieces of images to\n",
      "learn to identify which ones have or do not have pieces of\n",
      "tattoos. After training the network, a sliding window was\n",
      "passed through the image to be tested, and each segment of\n",
      "the sliding window was tested as a piece with a tattoo or\n",
      "not, marking the positive pieces. Parts with possible tattoos\n",
      "would be segmented piece by piece at the end of the slide.\n",
      "[31] proposed a continuation of work presented in\n",
      "[9], this time testing three different networks for tattoo\n",
      "segmentation: (i) an architecture consisting only of multiple\n",
      "fully connected layers, without convolutional layers; (ii)\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 18d7e985-0b93-4ecf-83a5-b192193770d1\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 32\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 755\n",
      "--------------------------------------------------------------------------------\n",
      "fully connected layers, without convolutional layers; (ii)\n",
      "an architecture inspired by the AlexNet network; and (iii)\n",
      "an architecture inspired by the VGGNet network. On the\n",
      "other hand, state-of-the-art segmentation models based on\n",
      "Vision Transformer (ViT) were evaluated in [13]; however,\n",
      "the main idea of that work was the unsupervised tattoo\n",
      "generator that allowed the creation of many semi-synthetic\n",
      "images with tattooed subjects. Hence, as shown in Table 2,\n",
      "related approaches used still needed to follow a semantic\n",
      "segmentation methodology and did not use an open-set\n",
      "semantic segmentation view, which is the focus of our\n",
      "current study and detailed as follows.\n",
      "C. OPEN-SET SEMANTIC SEGMENTATION\n",
      "The success of the fully convolutional network (FCN) in\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 30575194-9329-4eaa-a1c2-4c2cf1a3d42f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 33\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 684\n",
      "--------------------------------------------------------------------------------\n",
      "C. OPEN-SET SEMANTIC SEGMENTATION\n",
      "The success of the fully convolutional network (FCN) in\n",
      "closed-set semantic segmentation [32] has led to the suc-\n",
      "cessful implementation of various neural network models\n",
      "for closed-set semantic segmentation on different applica-\n",
      "tions [7]. However, these methods are unsuitable for open-\n",
      "set scenarios, which are common in real-world computer\n",
      "vision. This is because the closed-set perception fails when\n",
      "4 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 484d8438-309a-404a-8f2b-60c1c0f28f03\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 34\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 747\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "unknown classes from training are found in the test phase\n",
      "[33]. Due to that, several proposals have been developed\n",
      "for the open-set context in different applications, mainly au-\n",
      "tonomous driving, remote sensing, and data collection [34].\n",
      "For open-set semantic segmentation, the loss function\n",
      "selection to guide the optimization process is an important\n",
      "aspect of the achieved results [35]. In general, studies limit\n",
      "themselves to using the successful and widely employed\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e62a8dd9-83ef-4229-ad63-39bc5884b718\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 35\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 755\n",
      "--------------------------------------------------------------------------------\n",
      "aspect of the achieved results [35]. In general, studies limit\n",
      "themselves to using the successful and widely employed\n",
      "cross-entropy loss (CE) [15], [16], [33], [36]–[40]. This\n",
      "loss function measures the disparity between the predicted\n",
      "values and the ground truth, guiding the model’s learning\n",
      "process based on labeled data. Hence, the studies focus on\n",
      "adapting or building open-set classifiers to make closed-\n",
      "set semantic segmentation models capable of recognizing\n",
      "unknown classes, even in label noise scenarios [41].\n",
      "In this sense, metric learning has recently gained signif-\n",
      "icant attention in addressing the open-set problem. Metric\n",
      "learning is an approach based on learning a distance met-\n",
      "ric that reinforces similarity between objects in the latent\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 081a5050-a19d-420b-aafb-e375c0e03e83\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 36\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 754\n",
      "--------------------------------------------------------------------------------\n",
      "learning is an approach based on learning a distance met-\n",
      "ric that reinforces similarity between objects in the latent\n",
      "space. It has been studied in various fields, such as image\n",
      "classification [14], semantic segmentation [42], [43], and\n",
      "zero-shot segmentation [44]. According to [42], the metric\n",
      "learning strategy aims to direct the feature extraction process\n",
      "to obtain a well-controlled latent space, maximizing inter-\n",
      "class spacing and minimizing intra-class spacing based on\n",
      "a distance metric. Thus, the unknown samples are repelled\n",
      "into open space, as can be seen in Figure 4.\n",
      "Similarly, the large-margin-based loss (LM) functions\n",
      "[19]–[21] maximize the margins between classes by impos-\n",
      "ing a regularization on the logit vectors of pixels to induce\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: abcfe16e-1682-4c58-b9c6-be2538060e8b\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 37\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 765\n",
      "--------------------------------------------------------------------------------\n",
      "[19]–[21] maximize the margins between classes by impos-\n",
      "ing a regularization on the logit vectors of pixels to induce\n",
      "an increased separation between the boundary regions of\n",
      "the semantic classes. This strategy improves the general-\n",
      "ization of models and provides open space between classes\n",
      "that can be valuable in the context of open-set semantic\n",
      "segmentation, allowing unknown samples to be projected\n",
      "into these open spaces. Furthermore, the large-margin loss\n",
      "adopts a more efficient training strategy than other metric\n",
      "learning approaches, such as those involving cubic costs for\n",
      "computing pairwise distances between the logit vectors of\n",
      "pixels [45].\n",
      "Recent studies [40], [46] have made improved open-set\n",
      "semantic segmentation for autonomous driving applications\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 269c3411-7287-4d4a-b695-5cb3b3ffdfa2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 38\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 750\n",
      "--------------------------------------------------------------------------------\n",
      "Recent studies [40], [46] have made improved open-set\n",
      "semantic segmentation for autonomous driving applications\n",
      "by employing negative auxiliary data. Unlike these stud-\n",
      "ies, our approach uses no auxiliary data to enhance the\n",
      "performance of open-set semantic segmentation. Table 3\n",
      "summarizes the approaches used for open-set semantic\n",
      "segmentation.\n",
      "In the context of tattoo semantic segmentation, as stated\n",
      "in the previous Section, to the extent of our knowledge,\n",
      "no works focus on the open set context—the most closely\n",
      "related works are [23] and [47]. [23] built a tattoo search\n",
      "approach that can learn tattoo detection and compact rep-\n",
      "resentation jointly in a single CNN via multi-task learning\n",
      "is presented. However, the compactness proposed by the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5e81be11-516f-4cdb-b826-2c3114f7f068\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 39\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 772\n",
      "--------------------------------------------------------------------------------\n",
      "resentation jointly in a single CNN via multi-task learning\n",
      "is presented. However, the compactness proposed by the\n",
      "authors is more focused on the compressive yet discrim-\n",
      "inative feature learning for large-scale visual search and\n",
      "TABLE 3. Open-set semantic segmentation approaches.\n",
      "Ref. Loss Context Aux. data?\n",
      "[15], [16], [36], [38], [39] CE Remote Sensing No\n",
      "[33], [37] CE General No\n",
      "[33] CE Synthetic Data No\n",
      "[40], [46] CE Aut. Driving Yes\n",
      "[42] Metric Aut. Driving No\n",
      "[44] Metric General No\n",
      "[43] Metric General No\n",
      "Ours LM Tattoo No\n",
      "instance retrieval applications, i.e., the efficiency of the\n",
      "search procedure. Open-set classification is not presented,\n",
      "and discussions of the proposed multi-task procedure are\n",
      "not encouraged for other applications. While [47] presented\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5749daef-d843-4018-b93c-ba94befbe8ae\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 40\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 747\n",
      "--------------------------------------------------------------------------------\n",
      "and discussions of the proposed multi-task procedure are\n",
      "not encouraged for other applications. While [47] presented\n",
      "a classification method based on the Extreme Value Theory\n",
      "for tattoo classification. However, the focus of the proposed\n",
      "approach was the mid-level representations as a tool to\n",
      "adjust the trade-off between accuracy and efficiency. Hence,\n",
      "the results are mainly dedicated to real-world computer\n",
      "vision systems, where high accuracy is maintained even\n",
      "on commodity hardware with a low computational budget.\n",
      "Details regarding open-set are also not addressed.\n",
      "To the best of our knowledge, this is the first work to\n",
      "explore tattoo semantic segmentation in closed and open-set\n",
      "scenarios, establishing benchmarks for both conditions using\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: b4f059b0-4188-493b-a6da-a5f1e14827ef\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 41\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 791\n",
      "--------------------------------------------------------------------------------\n",
      "scenarios, establishing benchmarks for both conditions using\n",
      "our publicly available test set. Our approach introduces the\n",
      "large-margin loss function as an efficient learning strategy to\n",
      "build a well-defined logit space and handle class imbalance,\n",
      "using contemporary network architecture based on trans-\n",
      "formers and presenting the generalist approach to integrating\n",
      "a robust open-set classifier for semantic segmentation tasks.\n",
      "III. PROPOSED METHODS\n",
      "This section outlines the proposed methods. Firstly, in\n",
      "Section III-A, a novel-built TSSD2023 dataset is presented.\n",
      "Then, Section III-B introduces a novel tattoo semantic\n",
      "augmentation to expand tattoo samples of the TSSD2023\n",
      "dataset. Subsequently, in Section III-C, a novel large-margin\n",
      "loss function is proposed to handle class imbalance and\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d951f933-a77b-440c-9506-ff627852691c\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 42\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "dataset. Subsequently, in Section III-C, a novel large-margin\n",
      "loss function is proposed to handle class imbalance and\n",
      "enhance the discriminative of the classes in the TSSD2023\n",
      "dataset. Lastly, in Section III-D, a generalized approach for\n",
      "OpenIPCS is proposed for open-set semantic segmentation.\n",
      "A. TATTOO SEMANTIC SEGMENTATION DATASET\n",
      "Identifying the various things that make up a tattoo can be\n",
      "defined as a semantic segmentation task. This process allows\n",
      "machines to comprehend the meaning behind a tattoo better.\n",
      "Its usefulness is particularly evident in security systems,\n",
      "where it assists in identifying and monitoring individu-\n",
      "als through surveillance systems, for instance, locating a\n",
      "person based on a brief tattoo description. However, as\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ad2a9ac2-180a-4367-a2d8-34748ff74466\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 43\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 782\n",
      "--------------------------------------------------------------------------------\n",
      "als through surveillance systems, for instance, locating a\n",
      "person based on a brief tattoo description. However, as\n",
      "previously presented, current tattoo datasets are limited to\n",
      "image classification [22], [24], object detection [12], [23], or\n",
      "tattoo segmentation [9], [13], which only separate the tattoo\n",
      "VOLUME 4, 2016 5\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "from the background and fail to provide a comprehensive\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d7b033c2-faf8-4e8a-b497-b6f7be9b9f01\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 44\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 777\n",
      "--------------------------------------------------------------------------------\n",
      "from the background and fail to provide a comprehensive\n",
      "understanding of the significance of the tattoos.\n",
      "In this manner, we created the Tattoo Semantic Segmen-\n",
      "tation Dataset (TSSD2023), respecting the copyrights of\n",
      "image owners to the greatest extent possible. To build this\n",
      "dataset, web scraping was conducted on Flickr 1. Numerous\n",
      "terms related to tattoos were used as search queries on\n",
      "the platform. Then, a visual inspection was performed by\n",
      "humans to confirm whether the images obtained contained\n",
      "tattoos and whether their content was suitable for sharing.\n",
      "Finally, the licenses users attribute to these images when\n",
      "sharing them on the platform were considered to define the\n",
      "training, validation, and test sets.\n",
      "As a result, the test dataset exclusively comprises images\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: c9456824-7cdb-4357-8207-5d803ed319d2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 45\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 799\n",
      "--------------------------------------------------------------------------------\n",
      "training, validation, and test sets.\n",
      "As a result, the test dataset exclusively comprises images\n",
      "for which sharing permissions had been granted. In contrast,\n",
      "the training and validation datasets consist only of images\n",
      "for which sharing was unauthorized. This division strategy\n",
      "was adopted due to the limited availability of images with\n",
      "public sharing licenses. Thus, the test sets will be publicly\n",
      "available for comparison and development of future work.\n",
      "However, the training and validation sets will be kept private\n",
      "to ensure that none of the authors’ copyrights are infringed.\n",
      "Figure 2 shows 33 classes 2 for Known Known Class\n",
      "(KKC) tattoos and 23 classes for Unknown Unknown Class\n",
      "(UUC) tattoos selected for annotation in TSSD2023 3. The\n",
      "motivation for choosing this split for KKC and UUC classes\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e42f1df6-ab72-4f08-8353-e7e10412aea2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 46\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 765\n",
      "--------------------------------------------------------------------------------\n",
      "(UUC) tattoos selected for annotation in TSSD2023 3. The\n",
      "motivation for choosing this split for KKC and UUC classes\n",
      "was mainly based on the number of images available for\n",
      "each semantic class. Classes with reduced representation in\n",
      "the universe of available tattoos and, consequently, public\n",
      "samples were selected exclusively to compose the UUCs due\n",
      "to the impossibility of successfully training the segmentation\n",
      "models on these classes. Furthermore, the proposed division\n",
      "guarantees that the test semantic classes defined as UUC\n",
      "are not found on the training and validation sets. Hence,\n",
      "we consider the proposed division sufficient to evaluate the\n",
      "open-set methods proposed in this work. Once the data were\n",
      "divided, UUCs were chosen to form part of the test set for\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7c158161-af22-4651-81c8-b4513ff30daf\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 47\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 761\n",
      "--------------------------------------------------------------------------------\n",
      "open-set methods proposed in this work. Once the data were\n",
      "divided, UUCs were chosen to form part of the test set for\n",
      "open-set evaluation. This approach enabled the representa-\n",
      "tion of similar and dissimilar semantic classes compared\n",
      "to KKCs, allowing for evaluation in straightforward and\n",
      "complex scenarios.\n",
      "Each KKC class was meticulously labeled with unique\n",
      "identifiers to enable the model to distinguish each object\n",
      "semantically. In contrast, all UUC tattoos were assigned the\n",
      "same “unknown” class label. Notably, all KKC classes are\n",
      "represented in the training and validation sets and the test\n",
      "1A photo and video hosting platform established in 2004. Available\n",
      "in: https://flickr.com/\n",
      "2The dataset also includes annotations for the ‘stem/branch’ and ‘rope’\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: fbc79aad-1a7c-419b-b5ff-c9cbc0c22d7a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 48\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 778\n",
      "--------------------------------------------------------------------------------\n",
      "in: https://flickr.com/\n",
      "2The dataset also includes annotations for the ‘stem/branch’ and ‘rope’\n",
      "classes, which were omitted from the analysis due to the limited number\n",
      "of annotations.\n",
      "3The closed-set approach assumes that the training and testing pixels\n",
      "belong to the same label space (defined as KKCs), meaning that the train\n",
      "and test sets contain the same classes. However, this assumption does not\n",
      "hold in real-world scenarios, especially in earth observation applications.\n",
      "During the prediction phase, the model may face pixels from classes not\n",
      "seen during the training phase (UUCs). We direct the reader to [14] for a\n",
      "deeper reading about these definitions.\n",
      "set. However, UUC tattoos are exclusively found in the test\n",
      "open-set. A human inspection process was also conducted\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: fd261006-e78b-4099-abb7-5c69c667cde2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 49\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "set. However, UUC tattoos are exclusively found in the test\n",
      "open-set. A human inspection process was also conducted\n",
      "to ensure these open-set recognition conditions [14].\n",
      "Table 4 presents four possible tattoo composition sit-\n",
      "uations found in TSSD2023: (i) Single tattoo: only one\n",
      "tattoo class is present in the image; (ii) Multiple tattoos:\n",
      "multiple tattoo classes can be found in a single image;\n",
      "(iii) Multiple tattoos (with overlap): similar to the previous\n",
      "scenario, but the tattoos are overlapped with each other.\n",
      "Due to this overlap, this scenario is more challenging\n",
      "than the previous one [33]; and (iv) Tattoos (unlabeled)\n",
      "as background: Tattoos (unlabeled) as background: Specific\n",
      "tattoo classes were categorized as background classes due\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ae5768a7-c5ed-4c83-b518-3720395c9a68\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 50\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 771\n",
      "--------------------------------------------------------------------------------\n",
      "as background: Tattoos (unlabeled) as background: Specific\n",
      "tattoo classes were categorized as background classes due\n",
      "to two primary reasons. First, this decision was necessary\n",
      "because, at times, it was impossible to determine their spe-\n",
      "cific semantic tattoo class. Second, due to a limited number\n",
      "of available samples, assigning individual semantic labels\n",
      "to these classes was not feasible. These situations provide a\n",
      "comprehensive representation of tattoo compositions within\n",
      "the dataset.\n",
      "TABLE 4. Samples of the images and annotations of the TSSD2023 dataset.\n",
      "Single\n",
      "tattoo\n",
      "Multiple\n",
      "tattoos\n",
      "Multiple\n",
      "tattoos\n",
      "(with overlap)\n",
      "Tattoos\n",
      "(unlabeled)\n",
      "as background\n",
      "In conclusion, TSSD2023 contains 2,106 tattoo images\n",
      "without specific image resolution standards that have been\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d31e3ceb-21a1-4110-aeb7-3451cfc901e8\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 51\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 753\n",
      "--------------------------------------------------------------------------------\n",
      "In conclusion, TSSD2023 contains 2,106 tattoo images\n",
      "without specific image resolution standards that have been\n",
      "annotated at the pixel level. These images are divided into\n",
      "the following subsets: 1,404 for training, 387 for validation,\n",
      "and 254 for the closed-set test. Additionally, 61 images with\n",
      "UUC tattoos are included to form the test open-set with\n",
      "315 images. Figure 3 illustrates the distribution of pixel\n",
      "percentages for each class within TSSD2023, demonstrating\n",
      "that the dataset is notably imbalanced, particularly in the\n",
      "background class, which accounts for an average of ≈\n",
      "80% of pixels across the subsets. It is also important to\n",
      "highlight that the unknown class represents approximately\n",
      "3% of the pixels present in the test open-set. Samples\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: aad87f52-226b-465e-85a6-ff797b337a0a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 52\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 770\n",
      "--------------------------------------------------------------------------------\n",
      "highlight that the unknown class represents approximately\n",
      "3% of the pixels present in the test open-set. Samples\n",
      "of images from TSSD2023 can be observed in Table 4,\n",
      "showcasing tattoos presented in diverse scenarios, sizes,\n",
      "styles, positions, and combinations. These variations aim to\n",
      "maintain the variability of real-world situations.\n",
      "6 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "KKC Tattoos\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 4f4770a0-5655-4187-8d6c-d39a071ce6da\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 53\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 698\n",
      "--------------------------------------------------------------------------------\n",
      "KKC Tattoos\n",
      "Objects Animals Insects Plants Elements Weapons\n",
      "Anchor\n",
      "Crown\n",
      "Bird\n",
      "Cat\n",
      "Dog\n",
      "Eagle\n",
      "Fish\n",
      "Fox\n",
      "Lion\n",
      "Octopus\n",
      "Owl\n",
      "Shark\n",
      "Snake\n",
      "Tiger\n",
      "Wolf\n",
      "UUC Tattoos\n",
      "Animals Insects Plants Architectural Uncategorized\n",
      "Crocodile\n",
      "Bear\n",
      "Cow/Bull\n",
      "Chicken\n",
      "Elephant\n",
      "Hippopotamus\n",
      "Horse\n",
      "Monkey\n",
      "Pinguim\n",
      "Pork\n",
      "Sheep\n",
      "Turtle\n",
      "Classes Classification\n",
      "Diamond\n",
      "Key\n",
      "Ribbon\n",
      "Rope\n",
      "Butterfly\n",
      "Scorpion\n",
      "Spider\n",
      "Flower\n",
      "Leaf\n",
      "Stem/Branch\n",
      "Fire\n",
      "Water\n",
      "Gun\n",
      "Knife/Sword\n",
      "Shield\n",
      "Bee\n",
      "Beetle\n",
      "Ladybug\n",
      "Glass Bridge\n",
      "Castle\n",
      "Iceberg\n",
      "Joker\n",
      "Robot\n",
      "Ship\n",
      "Ying-Yang\n",
      "Uncategorized\n",
      "Background\n",
      "Heart\n",
      "Mermaid\n",
      "Skull\n",
      "Star\n",
      "FIGURE 2. TSSD2023 Classes. KKCs tattoos correspond to the 33 labeled classes, while the UUCs tattoos are represented by the 23 classes defined as\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a8528aba-075e-4deb-8b55-e8e0191c5051\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 54\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 760\n",
      "--------------------------------------------------------------------------------\n",
      "“unknown” (black label). These classes can be viewed within a conceptual taxonomy, facilitating an understanding of the domain coverage provided by the dataset.\n",
      "0,00%\n",
      "1,00%\n",
      "2,00%\n",
      "3,00%\n",
      "4,00%\n",
      "5,00%\n",
      "tigerskulleaglewolf lion\n",
      "octopus\n",
      "owl cat\n",
      "flowerscorpion\n",
      "fish dogsnakeribbon\n",
      "bird\n",
      "mermaidbutterflycrownanchorsharkgunheartwater\n",
      "fox leafknifeshieldspider\n",
      "key fire\n",
      "diamond\n",
      "star\n",
      "unknown\n",
      "Test open-set Test closed-set Validation Set Training Set\n",
      "FIGURE 3. The distribution of pixels per class and set. These values are organized based on their quantities in the training set. The values of background classes\n",
      "were suppressed due to discrepancies with the other classes. The reference values for the background are 79.8% in training, 79.4% validation images, 81.4% test\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: c7b286d3-5dad-43d9-a7bb-d98c76aff0ff\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 55\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 762\n",
      "--------------------------------------------------------------------------------\n",
      "closed-set images, and 81.7% test open-set images.\n",
      "B. CLASS SEMANTIC AUGMENTATION\n",
      "Motivated by the limited number of samples within\n",
      "TSSD2023 compared to the extensive diversity of real-world\n",
      "tattoos, this work introduces a novel data augmentation\n",
      "named class semantic augmentation (CSA) to increase the\n",
      "variety of tattoos contained in TSSD2023. This method ap-\n",
      "plies pixel-level transformations to distinct classes, enabling\n",
      "unique augmentation for different classes within an image,\n",
      "as illustrated in Table 5. In this instance, the pixels of\n",
      "the heart class go through different transformations of col-\n",
      "orations, gray styles, and color tones, trying to approximate\n",
      "the infinite possibilities of representing tattoos in the real\n",
      "world from limited semantic data.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 34a3ca70-1b0f-49ad-8722-61e189d3f087\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 56\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 765\n",
      "--------------------------------------------------------------------------------\n",
      "the infinite possibilities of representing tattoos in the real\n",
      "world from limited semantic data.\n",
      "TABLE 5. Samples of the data augmentations on only the heart class, except\n",
      "the mix column that applies the data augmentations on all semantic classes.\n",
      "Original RGB\n",
      "shift\n",
      "Random\n",
      "tune To gray Mix\n",
      "Algorithm 1 details the proposed implementation of tattoo\n",
      "semantic augmentation. The algorithm takes five parameters:\n",
      "the input image X, the ground truth Y , a list of pixel-\n",
      "level transformations to increase the variability of semantic\n",
      "classes t, a list containing the probabilities of executing\n",
      "each transformation p, and a list of semantic class indices\n",
      "ic that remain unchanged during data augmentation. It is\n",
      "important to note that the parameters p and ic are optional.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f3500fae-5f08-430b-9170-6cad81cdf8b6\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 57\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 773\n",
      "--------------------------------------------------------------------------------\n",
      "ic that remain unchanged during data augmentation. It is\n",
      "important to note that the parameters p and ic are optional.\n",
      "The algorithm first identifies the indices of semantic classes\n",
      "within the image. Subsequently, it iterates through each\n",
      "index, applying one of the transformations specified in t.\n",
      "As shown in Table 5, a heart can be represented in various\n",
      "ways while other classes remain unchanged. It demonstrates\n",
      "that this semantic augmentation allows specific adjustments\n",
      "for each class in the dataset, making it useful in various\n",
      "application domains, such as autonomous driving [42] and\n",
      "fashion images [48]. In addition, different pixel-level trans-\n",
      "formations can be selected for more suitable application\n",
      "contexts. For instance, the color of a flower may vary. At the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5ab20350-7b94-46cd-be11-b3f78d21ef90\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 58\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 783\n",
      "--------------------------------------------------------------------------------\n",
      "formations can be selected for more suitable application\n",
      "contexts. For instance, the color of a flower may vary. At the\n",
      "same time, the leaf and stem could be confined to modifying\n",
      "the original color tones, preserving the real-world patterns.\n",
      "On the other hand, all semantic classes in an image can be\n",
      "changed without restrictions, as observed in the mix column\n",
      "of Table 5 and as employed in this study. It is worth noting\n",
      "that this augmentation technique is limited to pixel-level\n",
      "transformations.\n",
      "C. PROPOSED LOSS FUNCTION\n",
      "Classical convolutional neural networks (CNNs) based se-\n",
      "mantic segmentation networks [32] can be divided into two\n",
      "VOLUME 4, 2016 7\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 171ace4e-ab0a-4a5e-90dd-603262ca5c1f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 59\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "Algorithm 1: Class semantic augmentation.\n",
      "Input: Image (X)\n",
      "Input: Ground truth ( Y )\n",
      "Input: List of transforms ( t)\n",
      "Input: List of probability of choice ( p)\n",
      "Input: List of ignore classes ( ic)\n",
      "Output: Transformed image ( ˆX)\n",
      "1: seg_ids = unique(Y )\n",
      "2: seg_ids = remove_ic(seg_ids, ic)\n",
      "3: ˆX = copy(X)\n",
      "4: Foreach: i ∈ seg_ids do\n",
      "5: y = new_array_zeros(Y.shape)\n",
      "6: y[Y == i] = 1\n",
      "7: T = random_choice(t, p)\n",
      "8: ˆX = apply_transf orm(T, ˆX, y)\n",
      "9: end foreach\n",
      "10: return ˆX\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 013c9d43-7ac0-4a2b-8424-dc72566e7ffd\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 60\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 768\n",
      "--------------------------------------------------------------------------------\n",
      "6: y[Y == i] = 1\n",
      "7: T = random_choice(t, p)\n",
      "8: ˆX = apply_transf orm(T, ˆX, y)\n",
      "9: end foreach\n",
      "10: return ˆX\n",
      "parts: a feature extractor that includes several convolution\n",
      "layers followed by max-pooling and activation function, and\n",
      "the linear classifier f = W ⊤x+b in the last fully-connected\n",
      "layer applied on the feature vector x of the penultimate layer\n",
      "for obtaining the logit vector f ∈ RC of each pixel of the\n",
      "input image, in which C represents the number of classes.\n",
      "To solve the overfitting problem and produce more discrim-\n",
      "inative logit vectors f in training, classifier margin has been\n",
      "exploited [19], [21], [49]. Following [50], the classification\n",
      "margin is the difference between the predicted score fc∗\n",
      "and target score fy, where y indicates the ground truth\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5fb23aa8-4e4a-4ea5-8ec0-f6b98b98ebb0\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 61\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 790\n",
      "--------------------------------------------------------------------------------\n",
      "margin is the difference between the predicted score fc∗\n",
      "and target score fy, where y indicates the ground truth\n",
      "class labels and c∗ = argmaxc̸=yfc. Based on the margin\n",
      "fy − fc∗, the traditional classifier margin loss function can\n",
      "be expressed as follows:\n",
      "Lmargin = L\n",
      "\u0012\n",
      "max\n",
      "c̸=y\n",
      "fc − fy + ρ\n",
      "\u0013\n",
      ", (1)\n",
      "in which ρ is a boundary control parameter, usually ρ ≥ 0.\n",
      "Thus, increasing the value of ρ results in a larger classifi-\n",
      "cation distance between labels.\n",
      "According to [20], the cross-entropy loss ( LCE ) can\n",
      "partially encourage the development of a large-margin clas-\n",
      "sifier within the CNNs. Based on this analysis, a sym-\n",
      "metric Kullback-Leibler (KL) divergence term LLM was\n",
      "introduced as a regularization component for LCE , inducing\n",
      "a more large-margin classifier in the original LCE . The\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1da015c6-9e41-46c6-a0db-2962129aac03\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 62\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 789\n",
      "--------------------------------------------------------------------------------\n",
      "introduced as a regularization component for LCE , inducing\n",
      "a more large-margin classifier in the original LCE . The\n",
      "combination was termed the large-margin cross-entropy loss\n",
      "(LCE +LM ), and it is the formula is presented as follows:\n",
      "LCE +LM = LCE + LLM , (2)\n",
      "in which LCE is defined as:\n",
      "LCE = −log py(f ) = −log exp(fy)PC\n",
      "c=1 exp(fc)\n",
      ", (3)\n",
      "and LLM is expressed as:\n",
      "LLM = λ\n",
      "2\n",
      "X\n",
      "c̸=y\n",
      "(\n",
      "exp(fc)P\n",
      "c′̸=y exp(fc′ ) − 1\n",
      "C − 1\n",
      ")\n",
      "×log\n",
      "(\n",
      "exp(fc)P\n",
      "c′̸=y exp(fc′ )\n",
      ")\n",
      ",\n",
      "(4)\n",
      "where λ is a regularization parameter. Increasing the value\n",
      "of the λ enlarges the space between classes, thereby more\n",
      "resistance faced by the learning objectives. The detailed\n",
      "derivation has been explored in [19].\n",
      "Considering the class imbalance between the background\n",
      "and the foreground classes in the tattoo semantic segmen-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 86f0b998-5389-48ce-a74d-3982439ba6ce\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 63\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 777\n",
      "--------------------------------------------------------------------------------\n",
      "Considering the class imbalance between the background\n",
      "and the foreground classes in the tattoo semantic segmen-\n",
      "tation dataset, we modify the LCE +LM replacing the LCE\n",
      "by the focal loss LF CL. This choice is based on the fact\n",
      "that LF CL is a variant of the LCE that preserves the\n",
      "discriminative capacity of the original loss while dealing\n",
      "with the imbalance among the classes [51]. Thus, our\n",
      "proposed large-margin focal loss ( LF CL+LM ) is described\n",
      "by:\n",
      "LF CL+LM (ours) = LF CL + LLM , (5)\n",
      "LF CL is defined as:\n",
      "LF CL = α(1 − py(f ))γ × log py(f ). (6)\n",
      "The LF CL applies a modulating term to the LCE to focus\n",
      "learning on hard examples and down-weight the numerous\n",
      "easy examples, where α control the class weights and γ\n",
      "reduce the loss contribution from easy examples. Thus, we\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 88b94250-251e-4186-8a4b-0543a2055573\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 64\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 752\n",
      "--------------------------------------------------------------------------------\n",
      "easy examples, where α control the class weights and γ\n",
      "reduce the loss contribution from easy examples. Thus, we\n",
      "obtain a loss that deals with imbalances between classes\n",
      "while increasing the classification margin to improve the\n",
      "discriminability of the trained model.\n",
      "D. OPEN-SET CLASSIFIER FOR SEMANTIC\n",
      "SEGMENTATION\n",
      "In semantic segmentation networks, the logit vector f ∈\n",
      "RC are commonly normalized using the softmax function\n",
      "into a probability distribution for each class y ∈ { 1, ..., C}\n",
      "to perform the final classification of each pixel.Therefore,\n",
      "the final classification of each pixel is defined as ˆY close\n",
      "f =\n",
      "argmax py(f ).\n",
      "This learnable classifier cannot recognize UUC, making it\n",
      "unsuitable for open-set recognition as it assigns all features\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 46ab0395-e1f9-4172-afc1-bedbc6df2a15\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 65\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 782\n",
      "--------------------------------------------------------------------------------\n",
      "unsuitable for open-set recognition as it assigns all features\n",
      "to KKCs [42]. Thus, open-set classifiers must be developed\n",
      "to classify KKCs while recognizing UUCs accurately. Pre-\n",
      "vious studies have investigated this development in semantic\n",
      "segmentation task [16], [38], [42], [52]. Based on those\n",
      "studies, one can present the general open set classifier as\n",
      "follows:\n",
      "ˆY open\n",
      "f =\n",
      "(\n",
      "CU U C max(py(f )) ⩽ λout,\n",
      "ˆY close\n",
      "f max(py(f )) > λ out. (7)\n",
      "where CU U C denotes the UUC, and λout the cutoff thresh-\n",
      "old to determinate the UUC pixels.\n",
      "8 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 418d857c-4789-474c-b0d5-bf9f315e5b7e\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 66\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 797\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "A recent study [38] presented an open-set classifier\n",
      "method based on principal components analysis (PCA) on\n",
      "the internal logic vectors of CNNs to provide an open-set\n",
      "semantic segmentation, named Open Principal Component\n",
      "Scoring with Incremental Learning (OpenIPCS). Its training\n",
      "approach is efficient, which is crucial due to the exponential\n",
      "nature of pixel-level classification compared to image clas-\n",
      "sification. Furthermore, the study notes that OpenIPCS out-\n",
      "performs open-set semantic segmentation strategies based\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1c9d36d9-9a03-45d0-a4ff-a88af342b94d\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 67\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 770\n",
      "--------------------------------------------------------------------------------\n",
      "sification. Furthermore, the study notes that OpenIPCS out-\n",
      "performs open-set semantic segmentation strategies based\n",
      "on probability maps by a significant margin.\n",
      "OpenIPCS was inspired by the Conditional Gaussian\n",
      "Distribution Learning (CGDL) proposed in [53], which is\n",
      "a Variational Autoencoder (V AE) model for conditional\n",
      "Gaussian distribution estimation, capable of learning con-\n",
      "ditional distributions of KKC and rejecting UUC examples.\n",
      "In distinction, the OpenIPCS replaced the V AE with PCA\n",
      "and uses multiple internal activation layers to adjust the\n",
      "generative model with validation samples only.\n",
      "[54] showed that logit vectors get closer to the label\n",
      "space as CNN layers deepen. Thus, in addition to the last\n",
      "layer f, OpenIPCS considers the enabling aspects of the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: c924963c-30db-4bdb-b25c-4bd2594166b2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 68\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 767\n",
      "--------------------------------------------------------------------------------\n",
      "space as CNN layers deepen. Thus, in addition to the last\n",
      "layer f, OpenIPCS considers the enabling aspects of the\n",
      "previous layers f, f 1, . . . , fL, in which L denotes the total\n",
      "number of the CNNs layers. This approach combines low-\n",
      "level and high-level semantic information, thus enhancing\n",
      "the discrimination capability of the model.\n",
      "Then, for each pixel, a corresponding feature vector\n",
      "ˆftrain is built by concatenating the network’s internal logit\n",
      "vectors f, f 1, . . . , fL. Such a concatenation produces high-\n",
      "dimensional and redundant features due to the hundreds or\n",
      "thousands of activation channels present in the CNN and\n",
      "the FCN layers [55], [56].\n",
      "As described by [57], PCA can serve a dual purpose.\n",
      "Apart from its primary role in reducing dimensionality, it\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e97348d2-b2fa-41c8-b139-99697a489958\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 69\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 750\n",
      "--------------------------------------------------------------------------------\n",
      "As described by [57], PCA can serve a dual purpose.\n",
      "Apart from its primary role in reducing dimensionality, it\n",
      "can also act as a probability density estimator with Gaussian\n",
      "priors. These features allow OpenIPCs to use PCA as a\n",
      "generative model ( G) for UUC recognition while solving\n",
      "the high-dimensionality problem of feature vectors.\n",
      "This G is incrementally adjusted using only the validation\n",
      "images. This process consists of adjusting the PCA with a\n",
      "batch of samples from the validation set. The classification\n",
      "step using the G consists of projecting the feature vector\n",
      "ˆftest of the test images in the latent space obtained by\n",
      "adjusting the PCA on the validation images and performing\n",
      "the inverse process to obtaining the ˆf G\n",
      "test. The difference\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f7b377e7-321a-49a1-99a9-d1b172b2c8cc\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 70\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 794\n",
      "--------------------------------------------------------------------------------\n",
      "the inverse process to obtaining the ˆf G\n",
      "test. The difference\n",
      "between the original feature vector ˆftest and the ˆf G\n",
      "test is\n",
      "calculated. Consequently, pixels of KKCs have low differ-\n",
      "ence values, while pixels belonging to the UUCs have high\n",
      "difference values. Thus, in accordance with [38], open-set\n",
      "recognition from OpenIPCS can be achieved as follows:\n",
      "ˆY openipcs\n",
      "f =\n",
      "(\n",
      "CU U C abs( ˆftest − ˆf G\n",
      "test) > λ out,\n",
      "ˆY close\n",
      "f abs( ˆftest − ˆf G\n",
      "test) ⩽ λout. (8)\n",
      "The λout do not represent equal statistical entities with\n",
      "the Equation 7. Thus, as in [38], the λout value was defined\n",
      "from preset values of True Positive Rate (TPR).\n",
      "As outlined in [54], the deeper layers are closer to the\n",
      "label space. Based on this analysis, this work proposes\n",
      "a generalized version of OpenIPCS (G-OpenIPCS) that\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 57eb6b3d-49cb-4d58-998a-b1d0cdc65986\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 71\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 743\n",
      "--------------------------------------------------------------------------------\n",
      "label space. Based on this analysis, this work proposes\n",
      "a generalized version of OpenIPCS (G-OpenIPCS) that\n",
      "focuses only on the last and penultimate layers of CNNs.\n",
      "These layers contain more substantial semantic information,\n",
      "which proves highly valuable for training OpenIPCS. Con-\n",
      "sequently, it becomes possible to disregard the other network\n",
      "layers within the CNNs. This aspect simplifies our approach\n",
      "and is easy to incorporate into different network architec-\n",
      "tures, unlike the original OpenIPCS building exclusively on\n",
      "the FCN decoder.\n",
      "IV . EXPERIMENTS SETUP\n",
      "Due to the open-set recognition process being still depen-\n",
      "dent on models built in a closed-set [33], [38], our exper-\n",
      "iments are divided into three parts. First, in Section IV-A,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 75332305-c815-463a-b7a0-b09dbe7d26b6\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 72\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 789\n",
      "--------------------------------------------------------------------------------\n",
      "iments are divided into three parts. First, in Section IV-A,\n",
      "we compare our proposed loss function, described in Sec-\n",
      "tion III-C, with other significant loss functions widely used\n",
      "in semantic segmentation or recently introduced to improve\n",
      "discriminability in deep neural networks. Subsequently, in\n",
      "Section IV-B, we employ the proposed G-OpenIPCS ap-\n",
      "proach detailed in Section III-D to evaluate the performance\n",
      "of models trained with the loss functions from the previous\n",
      "experiment in an open-set tattoo semantic segmentation\n",
      "scenario. Finally, we compare the performance of our open-\n",
      "set semantic segmentation approach with other state-of-\n",
      "the-art open-set semantic segmentation methods. Figure 4\n",
      "provides an overview of the proposed method. Each stage\n",
      "of Figure 4 is detailed as follows.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 713b9ad5-4888-4ff3-8ceb-2821a4306eb9\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 73\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 789\n",
      "--------------------------------------------------------------------------------\n",
      "provides an overview of the proposed method. Each stage\n",
      "of Figure 4 is detailed as follows.\n",
      "A. CLOSED-SET SEMANTIC SEGMENTATION\n",
      "Datasets. A total of 2,045 images from TSSD2023 were\n",
      "used to train and evaluate the closed-set models. These\n",
      "images follow the following division: 1,404 for training, 387\n",
      "for validation, and 254 for closed set testing, as described\n",
      "in Section III-A.\n",
      "Pre-processing and Data augmentations . This step in-\n",
      "volves a sequence of transformations to train and improve\n",
      "the semantic segmentation network, motivated by the limited\n",
      "dataset of training images available in TSSD2023 around\n",
      "the high diversity of tattoos that virtually have no limits in\n",
      "the real world. This process involves the following steps: a)\n",
      "Implement a resizing operation, which can be either random\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 82975a8b-f37f-4873-81be-5dfc693b2768\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 74\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 774\n",
      "--------------------------------------------------------------------------------\n",
      "Implement a resizing operation, which can be either random\n",
      "or center crop, or resize to get an image with a resolution of\n",
      "224 × 224; b) Next, there is a 50% probability of applying\n",
      "one of the geometric transformations to the image, including\n",
      "horizontal and vertical flipping, shifting, scaling, or rotation.\n",
      "These transformations have a scale and shift limit of 0.2, and\n",
      "rotation is limited to a maximum of positive or negative\n",
      "45 degrees; c) Executing the tattoo semantic augmentation\n",
      "method proposed by this work, as described in more detail\n",
      "below. This augmentation involves RGB shifting, conversion\n",
      "to grayscale, and the application of random tone curve\n",
      "transformations. Each of these transformations has a 25%\n",
      "probability of being applied to each class within an image;\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: abdca7db-414c-4e83-8ef3-87d14725489e\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 75\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 795\n",
      "--------------------------------------------------------------------------------\n",
      "transformations. Each of these transformations has a 25%\n",
      "probability of being applied to each class within an image;\n",
      "d) Subsequently, there is a 50% chance of applying certain\n",
      "VOLUME 4, 2016 9\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "Training \n",
      "Feature vector\n",
      "C + 768\n",
      "Logit space\n",
      "Network\n",
      "Input \n",
      "Open-set \n",
      "Semantic \n",
      "Segmentation \n",
      "Closed-set \n",
      "Semantic \n",
      "Segmentation \n",
      "G\n",
      "OpenIPCS\n",
      "Large-Margin\n",
      "Focal Loss\n",
      "3 x H x W C x H x W \n",
      "UUC\n",
      "KKC 1\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1339986f-ecfe-43a0-8162-bd184f90dbaf\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 76\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 748\n",
      "--------------------------------------------------------------------------------\n",
      "Segmentation \n",
      "Closed-set \n",
      "Semantic \n",
      "Segmentation \n",
      "G\n",
      "OpenIPCS\n",
      "Large-Margin\n",
      "Focal Loss\n",
      "3 x H x W C x H x W \n",
      "UUC\n",
      "KKC 1\n",
      "KKC 2\n",
      "OpenIPCS \n",
      "training\n",
      "OpenIPCS \n",
      "testing\n",
      "Test \n",
      "Open-set classifier\n",
      "KKC 3\n",
      "3 x H x W \n",
      "Closed-set \n",
      "Semantic Segmentation \n",
      "Large-Margin Focal Loss\n",
      "C x H x W \n",
      "Logit vectorsInput SegFormer\n",
      "(768 + C) x H x W \n",
      "Feature vectors \n",
      "Feature vector\n",
      "768 + C\n",
      "Open-set \n",
      "Semantic Segmentation \n",
      "Test \n",
      "Training \n",
      "UUC\n",
      "KKC 1\n",
      "KKC 2\n",
      "KKC 3\n",
      "Logit space\n",
      "OpenIPCS\n",
      "64 x H x W \n",
      "(768 + C) x H x W \n",
      "Projected feat. vec.\n",
      "Projection \n",
      "dissimilarities\n",
      "G-OpenIPCS method\n",
      "Data Aug. \n",
      "Ŷ openipcs\n",
      "Ŷ close\n",
      "Open \n",
      "Space\n",
      "Large\n",
      "Margin\n",
      "FIGURE 4. Overview of the proposed methods. Closed-set segmentations are obtained from models trained by different loss functions. The proposed\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7824b494-7941-4c9b-acaf-2d8a51635c51\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 77\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 767\n",
      "--------------------------------------------------------------------------------\n",
      "Large-Margin Focal Loss expands the decision boundaries between classes to improve class discrimination while dealing with the problem of imbalance between\n",
      "classes. Increasing the spacing between classes allows unknown samples to be projected into open space. The open-set segmentation process is highlighted\n",
      "within the green block. The proposed G-OpenIPCS recognizes UUC pixels, leaning on discrepancies between feature vectors. This approach uses only the last two\n",
      "layers of the segmentation model. The penultimate layer equals 768 channels for SegFormer and 512 channels for Swin+UPNet.\n",
      "global modifications to the image. These modifications can\n",
      "consist of random adjustments to brightness and contrast\n",
      "with a limit of 0.3, Gaussian blur with a 5-neighbor mask, or\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 6dc85c26-698e-4405-bd99-a2005347205d\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 78\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 775\n",
      "--------------------------------------------------------------------------------\n",
      "consist of random adjustments to brightness and contrast\n",
      "with a limit of 0.3, Gaussian blur with a 5-neighbor mask, or\n",
      "fancy PCA transformation; e) Next, coarse dropout transfor-\n",
      "mation [58] with a maximum of 8 plots per image with the\n",
      "maximum resolution of 32x32 is applied; f) Next, the images\n",
      "are normalized using the mean and standard deviation values\n",
      "obtained from the ImageNet [59]; g) Finally, there is a\n",
      "50% probability of applying the CutMix transformation\n",
      "[60] at the batch level to blend the images. All of these\n",
      "transformations were applied or developed with the on of\n",
      "the Albumentations library 4.\n",
      "Implementation details. Due to recent advances presented\n",
      "by the semantic segmentation networks based on trans-\n",
      "formers [61]–[63], this work uses the SegFormer [63],\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a9f48e82-c1ac-4a1b-8056-503a42041d29\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 79\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 763\n",
      "--------------------------------------------------------------------------------\n",
      "by the semantic segmentation networks based on trans-\n",
      "formers [61]–[63], this work uses the SegFormer [63],\n",
      "which consists of a hierarchical Transformer encoder with\n",
      "lightweight multilayer perceptron (MLP) decoders. The\n",
      "model was pre-trained on the ImageNet dataset [59] and\n",
      "was acquired through the Hugging Face library 5. During\n",
      "closed-set training, a batch size of 16 images on an RTX\n",
      "3090 with 500 epochs was utilized, employing a patience\n",
      "factor of 10 epochs. The Nadam optimizer [64] was used,\n",
      "with an initial learning rate set at 0.0001 and a weight decay\n",
      "coefficient of 0.00001.\n",
      "Baseline loss functions . In order to investigate the perfor-\n",
      "mance of large-margin focal loss (LLM F CL) in both closed-\n",
      "set and open-set semantic segmentation, we conducted a\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7fe39bef-77cc-4834-b926-4ea7682f6e79\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 80\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 760\n",
      "--------------------------------------------------------------------------------\n",
      "mance of large-margin focal loss (LLM F CL) in both closed-\n",
      "set and open-set semantic segmentation, we conducted a\n",
      "comparative study with five other loss functions: i) cross-\n",
      "entropy loss ( LCE ) – it was selected because it is widely\n",
      "used and a standard choice for semantic segmentation; ii)\n",
      "focal loss ( LF CL) [51] – this is a commonly used variation\n",
      "4https://albumentations.ai/\n",
      "5https://huggingface.co/\n",
      "of cross-entropy that is effective in addressing class imbal-\n",
      "ance; iii) Distance-based cross-entropy loss ( LDCE ) [65]\n",
      "and iv) Distance-based cross-entropy loss combined with\n",
      "Variance Loss (LDCE +V L) [42] were selected as they have\n",
      "been proposed to enhance the results of open-set recognition\n",
      "by increasing the inter-class distance and decreasing the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8e20a892-1b7c-4c20-aace-57ca7eca8413\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 81\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 778\n",
      "--------------------------------------------------------------------------------\n",
      "been proposed to enhance the results of open-set recognition\n",
      "by increasing the inter-class distance and decreasing the\n",
      "intra-class distance to produce more discriminative features;\n",
      "v) label-distribution-aware margin loss ( LLDAM ) [66] was\n",
      "chosen as a key loss for long-tailed recognition. It increases\n",
      "the margin for tail classes and decreases for main classes\n",
      "based on class frequency; vi) dice similarity coefficient loss\n",
      "(LDSC ++) [67] recently introduced loss that combines the\n",
      "robustness of the LDSC to class imbalance while penalizing\n",
      "overconfident predictions, hence improving the performance\n",
      "of semantic segmentation on out-of-distribution data; vii)\n",
      "large-margin cross-entropy loss ( LLM CE) [20] was chosen\n",
      "because it has recently been proposed to improve the results\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: cefcc80b-b702-411b-bdf3-1419e00809cc\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 82\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 750\n",
      "--------------------------------------------------------------------------------\n",
      "large-margin cross-entropy loss ( LLM CE) [20] was chosen\n",
      "because it has recently been proposed to improve the results\n",
      "of closed-set classifiers by increasing the space between the\n",
      "decision boundaries of each class. Moreover, we utilized\n",
      "the LLM CE as a foundation to develop our proposed loss\n",
      "function for the open-set tattoo semantic segmentation on a\n",
      "scenario that presents various challenges, such as high intra-\n",
      "class variability, class imbalance, and small tattoos, while\n",
      "enhanced discriminative capabilities in closed-set and open-\n",
      "set semantic segmentation. Table 6 presents the loss function\n",
      "parameters used for model training.\n",
      "Metrics. In order to assess the results of closed-set se-\n",
      "mantic segmentation, followed by open-set segmentation,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1f34a252-1945-42dd-bd1d-af4da0d737a8\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 83\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 649\n",
      "--------------------------------------------------------------------------------\n",
      "Metrics. In order to assess the results of closed-set se-\n",
      "mantic segmentation, followed by open-set segmentation,\n",
      "we employed the following metrics. Firstly, we used the\n",
      "Area Under the ROC Curve (AUROC) because it has been\n",
      "recently utilized to assess open-set semantic segmentation\n",
      "[38], [42]. This metric aids in measuring the model’s ability\n",
      "to distinguish between classes, particularly with respect\n",
      "10 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 28a9cd96-aafc-4624-95a1-3989fe090dfe\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 84\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 787\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "TABLE 6. Parameters\n",
      "per each loss function\n",
      "utilized for the model\n",
      "training.\n",
      "Ref. Loss Parameters\n",
      "- LCE -\n",
      "[51] LF CL α: 1.0, γ: 2.0\n",
      "[65] LDCE T : 3\n",
      "[66] LLDAM s: 30, λmax: 0.5\n",
      "[20] LLM CE λ: 0.3\n",
      "[42] LDCE +V L T : 3, λvl: 0.01\n",
      "[67] LDSC ++ γ: 2.0\n",
      "Ours LLM F CL λ: 0.3, α: 1.0, γ: 2.0\n",
      "to UUC. Secondly, the macro-averaged F1-Score (Macro\n",
      "F1), recommended in previous works for open-class clas-\n",
      "sification [68], [69]. It helps evaluate the model’s preci-\n",
      "sion and recall of the segmented pixels. Lastly, the Mean\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7ea5cc14-2611-42fb-8ea5-347e0930c8cb\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 85\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 754\n",
      "--------------------------------------------------------------------------------\n",
      "sification [68], [69]. It helps evaluate the model’s preci-\n",
      "sion and recall of the segmented pixels. Lastly, the Mean\n",
      "Intersection over Union (mIoU) is a standard metric for\n",
      "semantic segmentation evaluation that provides an overview\n",
      "of the model’s performance across all classes, including both\n",
      "KKCs and UUC.\n",
      "B. OPEN-SET SEMANTIC SEGMENTATION\n",
      "Datasets. G-OpenIPCS was trained using just the 387\n",
      "validation images. The test open-set with 315 images was\n",
      "used to evaluate the proposed method, as detailed in Sec-\n",
      "tion III-A.\n",
      "Implementation details . For each model trained using the\n",
      "evaluated loss functions in closed-set semantic segmenta-\n",
      "tion, a G-OpenIPCS model was trained to perform semantic\n",
      "segmentation in an open-set scenario. As depicted in Figure\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a3269347-64c1-4b9e-9d58-60cadb039801\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 86\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 743\n",
      "--------------------------------------------------------------------------------\n",
      "tion, a G-OpenIPCS model was trained to perform semantic\n",
      "segmentation in an open-set scenario. As depicted in Figure\n",
      "4, our approach utilizes the last two connected layers of\n",
      "the segmentation network to construct feature vectors for\n",
      "each pixel. In the case of SegFormer, this feature vector\n",
      "is equal to the number of KKCs plus the logit vector of\n",
      "the inner layer, with a length of 768. Subsequently, we\n",
      "defined 64 principal components for the adjustment in G-\n",
      "OpenIPCS. It is worth noting that the feature vector size and\n",
      "the number of principal components may vary depending\n",
      "on the semantic segmentation model and the application\n",
      "domain. Once G-OpenIPCS is trained, it becomes possible\n",
      "to recognize UUCs by considering the discrepancy between\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ee9a72ad-c409-4137-a464-e07c53bdd7bd\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 87\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 755\n",
      "--------------------------------------------------------------------------------\n",
      "domain. Once G-OpenIPCS is trained, it becomes possible\n",
      "to recognize UUCs by considering the discrepancy between\n",
      "the raw feature vector and the projected feature vector, as\n",
      "outlined in Equation 8.\n",
      "Baseline. Initially, a comparative analysis of the open-\n",
      "set semantic segmentation results was conducted using G-\n",
      "OpenIPCS trained for each evaluated loss function. This\n",
      "step serves to validate the performance of the proposed\n",
      "loss function in the open-set scenario. Subsequently, our\n",
      "approach was compared with other significant open-set\n",
      "segmentation methods proposed in the state-of-the-art lit-\n",
      "erature. These methods include OpenIPCS [38], Anoma-\n",
      "lous Probability Map (APM) [42], Maximum Unnormalized\n",
      "Logit (MaxLogit) [39], and Maximum Softmax Probability\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a4e6fde2-6bee-4e53-934d-385508dbd7b1\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 88\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 741\n",
      "--------------------------------------------------------------------------------\n",
      "lous Probability Map (APM) [42], Maximum Unnormalized\n",
      "Logit (MaxLogit) [39], and Maximum Softmax Probability\n",
      "(MSP) [43]. For these methods, training followed the same\n",
      "configurations as described in Section IV-A, except that\n",
      "only geometric transformations were applied. All networks\n",
      "were pre-trained on the ImageNet dataset [59]. Specifically,\n",
      "for DRN50+PSPNet [42] and RN101+PSPNet [39], images\n",
      "were resized to dimensions of 250, 300, 350, 400, and 450,\n",
      "with a down-sampling factor of 8 for DRN50. Additionally,\n",
      "we implemented SoftMax-Thresholding (SoftMax-T) [38]\n",
      "and OpenMax [36] as the baselines for our implementation.\n",
      "As this is the first study on tattoo semantic segmentation\n",
      "and without other tattoo semantic datasets, we only compare\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 90d7efce-39b2-4e68-addd-fca09f12b067\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 89\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 752\n",
      "--------------------------------------------------------------------------------\n",
      "As this is the first study on tattoo semantic segmentation\n",
      "and without other tattoo semantic datasets, we only compare\n",
      "our approach with state-of-the-art methods that do not use\n",
      "auxiliary data to supply their open-set recognition models.\n",
      "Finally, to demonstrate the generalization of our approach\n",
      "with baselines, we replicated the experiments, replacing the\n",
      "SegFormer with the Swin+UperNet model [70].\n",
      "Cutoff thresholds . All the methods compared in Sec-\n",
      "tion V-C require a cutoff threshold to distinguish between\n",
      "KKC and UUC pixels. To realistically undertake an open-set\n",
      "recognition task, these limits are defined empirically based\n",
      "on the available KKCs during the validation data following\n",
      "the conditions of the original papers. No information about\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 2f79dfeb-4014-4a7e-b146-6e57b6818e76\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 90\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 768\n",
      "--------------------------------------------------------------------------------\n",
      "on the available KKCs during the validation data following\n",
      "the conditions of the original papers. No information about\n",
      "the UUCs is used to select the cutoff thresholds for the open-\n",
      "set classifiers. For the proposed G-OpenIPCS, the preset\n",
      "values of the cutoff thresholds ( λout) are determined based\n",
      "on TPR quantiles, as outlined in [38].\n",
      "Metrics. To evaluate the semantic segmentation results in an\n",
      "open set, we kept the AUROC, Macro F1, and mIoU used\n",
      "in the experiment in a closed set, described in Section IV-A.\n",
      "Additionally, we highlight the results involving UUCs.\n",
      "V . RESULTS AND DISCUSSIONS\n",
      "This section presents the results and discussions obtained\n",
      "from our experiments. The purpose is to validate the im-\n",
      "pact of the proposed Large-Margin Focal Loss on closed\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 19af0792-8d67-4f6c-9de0-a5e1206ce427\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 91\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 793\n",
      "--------------------------------------------------------------------------------\n",
      "from our experiments. The purpose is to validate the im-\n",
      "pact of the proposed Large-Margin Focal Loss on closed\n",
      "and open-set tattoo semantic segmentation, in addition\n",
      "to demonstrating the effectiveness of the proposed G-\n",
      "OpenIPCS for open-set semantic segmentation. Initially, in\n",
      "Section V-A, we present the results of the loss functions\n",
      "evaluated in this work in a closed set scenario. Next,\n",
      "in Section V-B, we evaluate the performance of models\n",
      "trained using the proposed G-OpenIPCS method. Finally,\n",
      "in Section V-C, we compare our proposed approach with\n",
      "other state-of-the-art techniques.\n",
      "A. CLOSED-SET SEMANTIC SEGMENTATION\n",
      "This section focuses on evaluating the performance im-\n",
      "pact of the proposed LLM F CL in a closed-set semantic\n",
      "segmentation scenario. To demonstrate this, several loss\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 9a38fd58-2264-4b57-b439-4584f8c13d6c\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 92\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 792\n",
      "--------------------------------------------------------------------------------\n",
      "pact of the proposed LLM F CL in a closed-set semantic\n",
      "segmentation scenario. To demonstrate this, several loss\n",
      "functions from the literature were compared, as described\n",
      "in Section IV-A. Table 7 presents the overall results for all\n",
      "evaluated loss functions in terms of AUROC, Macro F1, and\n",
      "mIoU. When evaluating the performance of loss functions,\n",
      "LLM F CL outperforms all other metrics in terms of Macro\n",
      "F1 and maintains consistent results across AUROC and\n",
      "mIoU. This makes LLM F CL a suitable choice for closed-\n",
      "set semantic segmentation. However, LLDAM and LLM CE\n",
      "exhibit an outperform over LLM F CL in terms of AUROC\n",
      "and mIoU, respectively. Additionally, LF CL stood out by\n",
      "producing results that closely resemble those of LLM F CL\n",
      "and LLM F CL, despite not incorporating a large margin in\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 60c764c9-286e-4470-a18d-9564b8434260\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 93\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 782\n",
      "--------------------------------------------------------------------------------\n",
      "producing results that closely resemble those of LLM F CL\n",
      "and LLM F CL, despite not incorporating a large margin in\n",
      "VOLUME 4, 2016 11\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "TABLE 7. Comparison of the closed-set segmentation results\n",
      "achieved per each loss function. Bold values indicate the best\n",
      "overall results, including all loss functions. The results were\n",
      "obtained using SegFormer.\n",
      "Ref. Loss AUROC Macro F1 mIoU\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d8c9ef18-f7bf-4117-b749-824638070507\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 94\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 798\n",
      "--------------------------------------------------------------------------------\n",
      "overall results, including all loss functions. The results were\n",
      "obtained using SegFormer.\n",
      "Ref. Loss AUROC Macro F1 mIoU\n",
      "- LCE .7966±.122 .6179±.224 .4805±.216\n",
      "[51] LF CL .8024±.111 .6433±.213 .5053±.209\n",
      "[65] LDCE .8030±.118 .6108±.219 .4724±.218\n",
      "[66] LLDAM .8295±.105 .6497±.181 .5057±.191\n",
      "[20] LLM CE .8110±.112 .6457±.217 .5097±.216\n",
      "[42] LDCE +V L .7843±.120 .5933±.217 .4519±.205\n",
      "[67] LDSC ++ .8108±.108 .6303±.200 .4874±.196\n",
      "Ours LLM F CL .8142±.102 .6514±.189 .5090±.194\n",
      "its formulation. This emphasizes its capability to address\n",
      "class imbalance encountered in TSSD2023.\n",
      "On the other hand, LCE , LDSC ++, and particularly\n",
      "LDCE and LDCE +V L exhibit lower effectiveness compared\n",
      "to the other loss functions. Among these less favorable\n",
      "outcomes, LDCE +V L yielded the poorest results. This could\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8d315415-8e8d-494c-b13e-b71c0e331484\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 95\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 793\n",
      "--------------------------------------------------------------------------------\n",
      "to the other loss functions. Among these less favorable\n",
      "outcomes, LDCE +V L yielded the poorest results. This could\n",
      "indicate that the significant variability present in TSSD2023\n",
      "posed challenges for the metric learning process. Thus,\n",
      "functions based on large-margin, such as proposed LLM F CL\n",
      "and LLM CE, were demonstrated to be more effective for\n",
      "closed-set semantic tattoo segmentations.\n",
      "Table 8 presents the closed-set segmentation results cat-\n",
      "egorized by class obtained from proposed LLM F CL. It\n",
      "emphasizes the top 5 best and top 5 worst results, excluding\n",
      "the background class. The top-5 best have high IoU val-\n",
      "ues, indicating that the model’s segmentation performance\n",
      "for these categories is particularly accurate. Additionally,\n",
      "they exhibit relatively high AUROC and Macro F1 scores.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 896af4fc-26e8-40a3-876c-f5a6740a0d46\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 96\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 778\n",
      "--------------------------------------------------------------------------------\n",
      "for these categories is particularly accurate. Additionally,\n",
      "they exhibit relatively high AUROC and Macro F1 scores.\n",
      "Notably, the classes within the top 5 best results do not\n",
      "necessarily constitute the majority of pixel quantity in\n",
      "TSSD2023, as illustrated in Figure 3. This suggests that the\n",
      "excellent performance comes from other aspects, such as the\n",
      "semantic dissimilarity of this set of tattoos to other classes,\n",
      "and mainly due to the recurrence of these tattoos being tat-\n",
      "tooed individually, without overlapping with other semantic\n",
      "classes, as can be briefly observed in Table 9. In contrast,\n",
      "the top 5 worst classes can be categorized as complementary\n",
      "tattoos, meaning they are rarely encountered in isolation but\n",
      "are typically accompanied by other predominantly dominant\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 9b343537-e310-4ef1-b664-195b2b595bd9\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 97\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 758\n",
      "--------------------------------------------------------------------------------\n",
      "tattoos, meaning they are rarely encountered in isolation but\n",
      "are typically accompanied by other predominantly dominant\n",
      "tattoos in terms of pixel quantity. For instance, a crown is\n",
      "almost always found with another element, such as a human\n",
      "or animal face. Leaves often complement flowers and stems,\n",
      "and water is nearly always associated with aquatic creatures\n",
      "like fishes, sharks, and octopuses.\n",
      "In outline, LLM F CL showed considerable impact in the\n",
      "closed-set semantic segmentation, presenting the best results\n",
      "overall. However, aspects can still be considered to improve\n",
      "performance in closed-set segmentation. In addition to deal-\n",
      "ing with class imbalance, it is still necessary to deal with the\n",
      "overlap between semantic classes, improving the accuracy of\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7e35f37d-10f6-42f5-8dad-2aa7b73ce44a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 98\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 788\n",
      "--------------------------------------------------------------------------------\n",
      "overlap between semantic classes, improving the accuracy of\n",
      "segmentations, especially in complementary tattoo classes.\n",
      "This will be detailed in Section VII.\n",
      "B. OPEN-SET SEMANTIC SEGMENTATION\n",
      "This section evaluates the impact on the performance of\n",
      "the proposed LLM F CL in an open-set scenario. Table 10\n",
      "TABLE 8. Closed-set segmentation results separated by classes obtained\n",
      "from the SegFormer trained using the proposed LLM F CL. The results were\n",
      "sorted in descending order of mIoU↑ values. The top-5 best classes are\n",
      "highlighted in green , while the top-5 worst classes are highlighted in red .\n",
      "Class AUROC Macro F1 mIoU ↑\n",
      "background 0.8886 0.9669 0.9359\n",
      "tiger 0.9632 0.8656 0.7631\n",
      "octopus 0.9102 0.8552 0.7470\n",
      "snake 0.9270 0.8451 0.7318\n",
      "key 0.9136 0.8290 0.7079\n",
      "owl 0.8813 0.8287 0.7076\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: b3db6c6f-4595-4124-8f3a-5023071708f9\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 99\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 781\n",
      "--------------------------------------------------------------------------------\n",
      "octopus 0.9102 0.8552 0.7470\n",
      "snake 0.9270 0.8451 0.7318\n",
      "key 0.9136 0.8290 0.7079\n",
      "owl 0.8813 0.8287 0.7076\n",
      "butterfly 0.8729 0.8164 0.6897\n",
      "lion 0.8714 0.8106 0.6816\n",
      "star 0.8787 0.8028 0.6706\n",
      "dog 0.8863 0.7868 0.6485\n",
      "scorpion 0.9149 0.7586 0.6110\n",
      "flower 0.8212 0.7412 0.5888\n",
      "fox 0.8302 0.7083 0.5483\n",
      "fish 0.8278 0.7042 0.5435\n",
      "shark 0.8604 0.6978 0.5359\n",
      "gun 0.8507 0.6914 0.5284\n",
      "cat 0.7772 0.6878 0.5241\n",
      "anchor 0.8192 0.6565 0.4886\n",
      "bird 0.7641 0.6490 0.4804\n",
      "diamond 0.8530 0.6280 0.4577\n",
      "mermaid 0.9150 0.6256 0.4551\n",
      "eagle 0.8788 0.6180 0.4472\n",
      "spide 0.8606 0.5998 0.4283\n",
      "heart 0.7696 0.5809 0.4093\n",
      "ribbon 0.7572 0.5496 0.3789\n",
      "wolf 0.8008 0.5403 0.3702\n",
      "skull 0.7562 0.5393 0.3693\n",
      "shield 0.7086 0.5375 0.3675\n",
      "crown 0.7241 0.4718 0.3087\n",
      "leaf 0.6564 0.4310 0.2747\n",
      "water 0.6409 0.3939 0.2452\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8e9deac1-cc40-4d2a-85d4-b7133c775708\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 100\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 791\n",
      "--------------------------------------------------------------------------------\n",
      "shield 0.7086 0.5375 0.3675\n",
      "crown 0.7241 0.4718 0.3087\n",
      "leaf 0.6564 0.4310 0.2747\n",
      "water 0.6409 0.3939 0.2452\n",
      "knife 0.5633 0.1796 0.0987\n",
      "fire 0.5262 0.0989 0.0520\n",
      "all 0.8142 0.6514 0.5090\n",
      "compares the open-set segmentation results obtained using\n",
      "various loss functions, all evaluated using the proposed G-\n",
      "OpenIPCS method. Among the loss functions, LLM F CL\n",
      "achieves the highest AUROC score of 0.8013, Macro F1\n",
      "with 0.6318, and also outperforms the mIoU score with\n",
      "0.4900. These results indicate that LLM F CL is the most\n",
      "effective loss function in terms of overall performance. Con-\n",
      "sidering only UUC, although LCE has the best performance\n",
      "in terms of AUROC (UUC). LLM F CL remains competitive\n",
      "in this metric while providing superior performance in terms\n",
      "of the F1-Score (UUC) and mIoU (UUC).\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 38657166-326f-4cab-9d71-424d0a89a582\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 101\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 698\n",
      "--------------------------------------------------------------------------------\n",
      "in this metric while providing superior performance in terms\n",
      "of the F1-Score (UUC) and mIoU (UUC).\n",
      "Table 11 presents an evaluation of class-level performance\n",
      "from open-set semantic segmentation results from proposed\n",
      "LLM F CL. The top-5 best practically remained the same as\n",
      "12 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 01b693d8-f6f6-4785-9693-05377993af53\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 102\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 798\n",
      "--------------------------------------------------------------------------------\n",
      "TABLE 9. Visual result samples of open-set semantic segmentation for each loss function. UUCs are depicted as black pixels. The respective colors for other\n",
      "semantic classes can be found in Figure 2. The UUC has been highlighted in yellow , while the top-5 best classes are highlighted in green , and the top-5 worst\n",
      "classes are highlighted in red . The results were obtained using SegFormer and proposed G-OpenIPCS.\n",
      "Class Image Mask LCE LF CL LDCE LLDAM LLM CE LDCE+V L LDSC++ LLM F CL\n",
      "Top-5 Best\n",
      "tiger\n",
      "key\n",
      "octopus\n",
      "owl\n",
      "star\n",
      "Top-5 Worst\n",
      "wolf\n",
      "leaf\n",
      "water\n",
      "knife\n",
      "fire\n",
      "UUCs\n",
      "TABLE 10. Comparison of the\n",
      "open-set segmentation results\n",
      "achieved per each loss function. All\n",
      "results are obtained on SegFormer\n",
      "and the proposed G-OpenIPCS\n",
      "method. Bold values indicate the best\n",
      "overall results, including all loss\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f6b3af03-3a9b-4132-801b-741acfb437c3\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 103\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 705\n",
      "--------------------------------------------------------------------------------\n",
      "and the proposed G-OpenIPCS\n",
      "method. Bold values indicate the best\n",
      "overall results, including all loss\n",
      "functions.\n",
      "Ref. Loss AUROC AUROC\n",
      "UUC Macro F1 F1 Score\n",
      "UUC mIoU IoU\n",
      "UUC\n",
      "- LCE .7737±.122 .8204 .5892±.234 .3759 .4525 ±.220 .2314\n",
      "[51] LF CL .7865±.112 .7212 .5919 ±.216 .2886 .4505 ±.207 .1686\n",
      "[65] LDCE .7827±.119 .7333 .5770 ±.215 .3782 .4357 ±.209 .2332\n",
      "[66] LLDAM .8077±.118 .4977 .5851 ±.202 .0428 .4399 ±.193 .0219\n",
      "[20] LLM CE .7975±.111 .6916 .6070 ±.209 .4052 .4650 ±.204 .2541\n",
      "[42] LDCE +V L .7693±.116 .7345 .5681 ±.214 .3903 .4254 ±.201 .2425\n",
      "[67] LDSC ++ .7939±.110 .6353 .5822 ±.203 .2605 .4375 ±.197 .1497\n",
      "Ours LLM F CL .8013±.104 .7827 .6318±.201 .4318 .4900 ±.201 .2753\n",
      "VOLUME 4, 2016 13\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 43aca180-35bb-4281-9d20-662788719eec\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 104\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 782\n",
      "--------------------------------------------------------------------------------\n",
      "Ours LLM F CL .8013±.104 .7827 .6318±.201 .4318 .4900 ±.201 .2753\n",
      "VOLUME 4, 2016 13\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "the closed-set results, which is natural given the dependence\n",
      "of the closed-set model of the proposed approach on open-\n",
      "set recognition. However, the ‘star’ class was an exception,\n",
      "replacing the ‘snake’ class among the top 5 in terms of\n",
      "IoU. This is probably due to the low similarity of the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 07e62a92-519e-47e9-9375-c2780397ea0f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 105\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 749\n",
      "--------------------------------------------------------------------------------\n",
      "replacing the ‘snake’ class among the top 5 in terms of\n",
      "IoU. This is probably due to the low similarity of the\n",
      "‘star’ class to the other semantic classes in TSSD2023,\n",
      "which practically maintained the performance obtained in\n",
      "the closed set. Classes that show more significant similarity\n",
      "to other classes, regardless of whether a KKC or a UUC,\n",
      "become more challenging for open-set segmentation, as\n",
      "they generate considerable uncertainty for the segmentation\n",
      "model.\n",
      "This case of declining performance owing to class sim-\n",
      "ilarity is also evident among the top 5 worst classes.\n",
      "Classes such as ‘leaf’, ‘water’, ‘knife’, and ‘fire’ remained\n",
      "in this category, underscoring their significant reliance on\n",
      "the closed-set model. However, there was an exception\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 95f0cf23-5065-4d6a-a425-cb07b9be8cc1\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 106\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 758\n",
      "--------------------------------------------------------------------------------\n",
      "in this category, underscoring their significant reliance on\n",
      "the closed-set model. However, there was an exception\n",
      "in the ‘wolf’ class, which, despite its poor performance\n",
      "in the closed set, experienced a deterioration after open-\n",
      "set segmentation. This decline is attributed to its semantic\n",
      "similarity with other KKC animals, such as ‘dog’ and ‘fox’,\n",
      "and its resemblance to UUCs, particularly in bear tattoos.\n",
      "This provides misclassifications, as depicted in Figure 2.\n",
      "Furthermore, KKCs can face challenges due to the high\n",
      "semantic variability exhibited within tattoos of the same\n",
      "class, arising from the infinite possibilities for represent-\n",
      "ing an object through tattoos. For illustration, consider\n",
      "the representation of a ‘cat’ in various styles, such as\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 25bd6860-485e-4bd9-89eb-686d145be158\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 107\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 798\n",
      "--------------------------------------------------------------------------------\n",
      "ing an object through tattoos. For illustration, consider\n",
      "the representation of a ‘cat’ in various styles, such as\n",
      "cartoonish, realistic, stick-figure, geometric, and so on. It\n",
      "becomes exceedingly challenging to group all these diverse\n",
      "representations to ensure no shape is mistakenly isolated as\n",
      "an outlier and segmented as part of the unknown-unknown\n",
      "class (UUC). We endeavored to construct a robust data\n",
      "augmentation pipeline to address this limitation, as we\n",
      "believe it offers a potential solution (detailed in Section VII).\n",
      "Regarding the UUC, the results obtained using LLM F CL\n",
      "remain somewhat limited but show great promise. Notably,\n",
      "the UUC exhibited superior results in terms of IoU and F1-\n",
      "Score when compared to the top 5 worst classes. However,\n",
      "in terms of AUROC, 12 KKCs are surpassed.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a3917b67-4db2-4df1-b8ec-109eaa14e134\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 108\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 749\n",
      "--------------------------------------------------------------------------------\n",
      "Score when compared to the top 5 worst classes. However,\n",
      "in terms of AUROC, 12 KKCs are surpassed.\n",
      "In conclusion, utilizing the proposed LLM F CL in con-\n",
      "junction with the G-OpenIPCS achieves the best overall re-\n",
      "sults in open-set segmentation tasks. Its robust segmentation\n",
      "performance, capable of effectively distinguishing between\n",
      "UUCs and KKCs, makes it a promising choice for challeng-\n",
      "ing scenarios with high semantic variability between classes.\n",
      "However, it is essential to note that there are still tough\n",
      "situations and areas where further improvements are needed\n",
      "in the context of open-set segmentation for TSSD2023.\n",
      "C. COMP ARISON WITH OTHERS OPEN-SET SEMANTIC\n",
      "SEGMENTATION METHODS\n",
      "Table 12 comprehensively compares different state-of-the-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 2c124e31-10d0-4d82-a43b-62b0945d53e1\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 109\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 788\n",
      "--------------------------------------------------------------------------------\n",
      "SEGMENTATION METHODS\n",
      "Table 12 comprehensively compares different state-of-the-\n",
      "art open-set semantic segmentation methods, including the\n",
      "proposed approach, and two more baselines: SoftMax-T and\n",
      "OpenMax. The proposed approach using LLM F CL and G-\n",
      "TABLE 11. Open-set segmentation results are separated by classes obtained\n",
      "from the SegFormer trained using the proposed LLM F CL. All results are\n",
      "obtained on the proposed G-OpenIPCS method. The results were sorted in\n",
      "descending order of mIoU↑ values. The UUC has been highlighted in yellow ,\n",
      "while the top-5 best classes are highlighted in green , and the top-5 worst\n",
      "classes are highlighted in red .\n",
      "Class AUROC Macro F1 mIoU ↑\n",
      "background 0.9025 0.9643 0.9310\n",
      "tiger 0.9599 0.8828 0.7901\n",
      "key 0.9118 0.8589 0.7527\n",
      "octopus 0.8978 0.8417 0.7267\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 0752b86f-6d80-42d0-8b70-1c7f9f1b4557\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 110\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 790\n",
      "--------------------------------------------------------------------------------\n",
      "background 0.9025 0.9643 0.9310\n",
      "tiger 0.9599 0.8828 0.7901\n",
      "key 0.9118 0.8589 0.7527\n",
      "octopus 0.8978 0.8417 0.7267\n",
      "owl 0.8777 0.8205 0.6956\n",
      "star 0.8594 0.8188 0.6932\n",
      "snake 0.9177 0.8080 0.6779\n",
      "butterfly 0.8714 0.8064 0.6756\n",
      "dog 0.8584 0.7690 0.6247\n",
      "scorpion 0.8940 0.7424 0.5903\n",
      "diamond 0.8420 0.7195 0.5619\n",
      "lion 0.8449 0.7192 0.5615\n",
      "fox 0.8279 0.7124 0.5533\n",
      "flower 0.7971 0.7043 0.5435\n",
      "fish 0.8221 0.6991 0.5374\n",
      "gun 0.8430 0.6972 0.5351\n",
      "cat 0.7758 0.6965 0.5344\n",
      "shark 0.8604 0.6798 0.5150\n",
      "mermaid 0.9158 0.6754 0.5099\n",
      "bird 0.7631 0.6544 0.4863\n",
      "anchor 0.8075 0.6431 0.4740\n",
      "eagle 0.8780 0.5941 0.4226\n",
      "skull 0.7502 0.5733 0.4019\n",
      "shield 0.7063 0.5429 0.3725\n",
      "spide 0.8202 0.5420 0.3718\n",
      "ribbon 0.7594 0.5402 0.3700\n",
      "crown 0.7121 0.4761 0.3124\n",
      "heart 0.6827 0.4573 0.2964\n",
      "unknown 0.7827 0.4318 0.2753\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ad4b2f3c-c778-45fb-af32-2dddb628be99\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 111\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 764\n",
      "--------------------------------------------------------------------------------\n",
      "ribbon 0.7594 0.5402 0.3700\n",
      "crown 0.7121 0.4761 0.3124\n",
      "heart 0.6827 0.4573 0.2964\n",
      "unknown 0.7827 0.4318 0.2753\n",
      "wolf 0.7600 0.4178 0.2641\n",
      "leaf 0.6462 0.4068 0.2553\n",
      "water 0.6416 0.4017 0.2513\n",
      "knife 0.5426 0.1322 0.0708\n",
      "fire 0.5134 0.0520 0.0267\n",
      "all 0.8013 0.6318 0.4900\n",
      "OpenIPCS stands out as the best-performing method. It\n",
      "achieves the highest AUROC, Macro F1, mIoU, and IoU\n",
      "(UUC) values, indicating its superiority in open-set tattoo\n",
      "semantic segmentation. The scores obtained for AUROC\n",
      "of 0.8013, Macro F1 of 0.6318, mIoU of 0.4900, and\n",
      "IoU (UUC) of 0.2753 are significant compared to other\n",
      "approaches in the literature. However, the performance\n",
      "difference is less to the baselines, except for the values\n",
      "obtained from IoU to UUC. This indicates that the proposed\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: c195903f-afce-458b-8717-c58edef63881\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 112\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 785\n",
      "--------------------------------------------------------------------------------\n",
      "difference is less to the baselines, except for the values\n",
      "obtained from IoU to UUC. This indicates that the proposed\n",
      "approach produces more accurate and visually coherent\n",
      "segmentation results.\n",
      "In comparison to the original OpenIPCS, it is essential to\n",
      "emphasize the strengths that make our approach superior.\n",
      "The original OpenIPCS relies on an FCN decoder that\n",
      "combines multiple layers from various levels of the network\n",
      "to construct the feature vector. In contrast, our G-OpenIPCS\n",
      "approach follows a more straightforward and intuitive path,\n",
      "considering that the critical features for class discrimination\n",
      "are primarily situated in the latter layers of the segmenta-\n",
      "tion network. This approach leads to improved results and\n",
      "avoids the potential scrambling of high-level and low-level\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 56cdf36b-0e72-453d-b412-e3adf5941709\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 113\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 793\n",
      "--------------------------------------------------------------------------------\n",
      "tion network. This approach leads to improved results and\n",
      "avoids the potential scrambling of high-level and low-level\n",
      "14 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "TABLE 12. Comparison\n",
      "between the proposed\n",
      "approach and state-of-the-art\n",
      "open-set semantic\n",
      "segmentation techniques.\n",
      "Bold values indicate the best\n",
      "overall results, including all\n",
      "methods.\n",
      "Ref. Loss Network Open Set\n",
      "Classifier AUROC Macro F1 mIoU IoU\n",
      "UUC\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 743a6ab0-2f43-43d1-a593-e67895ae5f04\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 114\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "overall results, including all\n",
      "methods.\n",
      "Ref. Loss Network Open Set\n",
      "Classifier AUROC Macro F1 mIoU IoU\n",
      "UUC\n",
      "Baseline LCE SegFormer SoftMax-T .7855±.126 .5782±.229 .4398±.216 .1350\n",
      "Baseline LCE SegFormer OpenMax .7785±.123 .5781±.227 .4389±.212 .1611\n",
      "Baseline LCE Swin+UPerNet SoftMax-T .7234±.095 .4852±.198 .3428±.188 .1145\n",
      "Baseline LCE Swin+UPerNet OpenMax .7134±.095 .4736±.197 .3325±.181 .1258\n",
      "[38] LCE DN121+FCN OpenIPCS .6523±.098 .3596±.217 .2422±.183 .0632\n",
      "[38] LCE WRN50+FCN OpenIPCS .6695±.098 .3983±.214 .2722±.185 .0470\n",
      "[42] LDCE+V L DRN50+PSPNet APM .7123±.139 .4290±.284 .3152±.244 .1657\n",
      "[39] LCE RN101+PSPNet MaxLogit .7358±.139 .5173±.293 .4031±.268 .1439\n",
      "[43] LCE+P C RN50+DeepLabV3+ MaxLogit .7048±.105 .4471±.211 .3126±.191 .1327\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1e47d4b1-e0eb-4504-b02a-93683d251f47\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 115\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 759\n",
      "--------------------------------------------------------------------------------\n",
      "[43] LCE+P C RN50+DeepLabV3+ MaxLogit .7048±.105 .4471±.211 .3126±.191 .1327\n",
      "[43] LCE+P C RN50+DeepLabV3+ MSP .7401±.097 .4694±.207 .3157±.190 .1354\n",
      "Ours LLM F CL SegFormer G-OpenIPCS .8013±.104 .6318±.201 .4900±.201 .2753\n",
      "Ours LLM F CL Swin+UPerNet* G-OpenIPCS .7921±.116 .5927±.211 .4518±.213 .1981\n",
      "* Evaluation of the generalization of the proposed method using Swin +UPerNet [62], [70].\n",
      "information that can occur when using multiple layers, as\n",
      "in the original OpenIPCS.\n",
      "Due to this more streamlined design choice, our approach\n",
      "integrates with other modern segmentation architectures,\n",
      "such as transform-based structures. For instance, we em-\n",
      "ployed the SegFormer and Swin+UperNet networks, outper-\n",
      "forming FCN-based models in open-set tattoo semantic seg-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d02ddda9-43a8-46b5-8b3a-656bfc5539a2\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 116\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 781\n",
      "--------------------------------------------------------------------------------\n",
      "ployed the SegFormer and Swin+UperNet networks, outper-\n",
      "forming FCN-based models in open-set tattoo semantic seg-\n",
      "mentation. Notably, the combination using the SegFormer\n",
      "outperforms other methods by a significant margin. This\n",
      "underscores that our design choice directly enhances perfor-\n",
      "mance, especially considering the high reliance on features\n",
      "in open-set segmentation models [42].\n",
      "Moreover, it is worth noting that the original OpenIPCS\n",
      "is built upon LCE . While this loss function consistently\n",
      "delivers results in closed-set semantic segmentation, the\n",
      "challenge of open-set segmentation demands segmentation\n",
      "models with a heightened discriminative capacity, which is\n",
      "precisely what our proposed LLM F CL aims to enhance.\n",
      "D. ABLATION STUDIES OF THE DATA AUGMENTATION\n",
      "COMPONENTS\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 015281ed-682b-4fbc-8cf8-4106ddc924dd\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 117\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 781\n",
      "--------------------------------------------------------------------------------\n",
      "precisely what our proposed LLM F CL aims to enhance.\n",
      "D. ABLATION STUDIES OF THE DATA AUGMENTATION\n",
      "COMPONENTS\n",
      "We performed an ablation study to examine the impact\n",
      "of various data augmentation techniques on our approach\n",
      "to open-set recognition for tattoo semantic segmentation.\n",
      "These techniques were divided into four categories, as\n",
      "explained in Section IV-A: geometric, image adjustments,\n",
      "dropout, and our proposed CSA method. We also included\n",
      "a no-augmentation experiment as a baseline for comparison.\n",
      "The results in Table 13 indicate that our approach per-\n",
      "forms better when all data augmentation methods are com-\n",
      "bined. Among these methods, image adjustments consis-\n",
      "tently showed performance gains in mIoU and IoU for UUC.\n",
      "The other approaches, including geometric and proposed\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 44b6f009-97e0-4d08-8d26-d05df6fe3c3d\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 118\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 770\n",
      "--------------------------------------------------------------------------------\n",
      "tently showed performance gains in mIoU and IoU for UUC.\n",
      "The other approaches, including geometric and proposed\n",
      "CSA, improved the values of IoU for UUC, while the\n",
      "dropout method improved the mIoU. Notably, the proposed\n",
      "CSA significantly improved performance when combined\n",
      "with the other three methods. In summary, our approach is\n",
      "appropriate for effective open-set tattoo semantic segmenta-\n",
      "tion.\n",
      "E. EVALUATION OF THE PERFORMANCE IN THE\n",
      "CLOSED-SET TATTOO SEGMENTATION SCENARIO\n",
      "Semantic segmentation focuses on recognizing specific\n",
      "classes of tattoos, such as cats, dogs, stars, and others. Tattoo\n",
      "segmentation only aims to isolate the tattoo region from\n",
      "the background. This distinction is beneficial in situations\n",
      "that do not require detailed semantic information about\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e816f687-1cfc-4955-9525-bf2816720939\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 119\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 749\n",
      "--------------------------------------------------------------------------------\n",
      "the background. This distinction is beneficial in situations\n",
      "that do not require detailed semantic information about\n",
      "tattoos. However, it may be interesting that our approach\n",
      "also behaves adequately in closed-set tattoo segmentation\n",
      "scenarios. Thus, we evaluated the performance of our pro-\n",
      "posed approach in a closed tattoo segmentation scenario\n",
      "using the DeMSI dataset [9], where tattoos were manually\n",
      "annotated without any semantic differentiation. In addition\n",
      "to measuring the IoU for tattoos, we also evaluate the\n",
      "False Positive Rate (FPR), which indicates the proportion of\n",
      "background pixels incorrectly classified as tattoos, and the\n",
      "False Negative Rate (FNR), which means the proportion of\n",
      "tattoo pixels incorrectly classified as background.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 76979c66-5472-4790-b782-fdcafe5b3951\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 120\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "False Negative Rate (FNR), which means the proportion of\n",
      "tattoo pixels incorrectly classified as background.\n",
      "The quantitative results are presented in Table 14. We\n",
      "observed a degradation in the performance of the proposed\n",
      "approach when directly applying the model trained on\n",
      "TSSD2023 and converting the multiclass output to a binary\n",
      "output. This degradation is primarily reflected in the FNR\n",
      "values, indicating that many tattoo pixels were misclassified\n",
      "as background. We attribute this behavior to the labeling\n",
      "design used in TSSD2023. The primary factor is the detailed\n",
      "annotations in TSSD2023, designed to accurately classify\n",
      "the pixel classes, as seen in Table 4. For example, the shark’s\n",
      "mouth was separated from the shadow of the tattoo. In\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 6219c1f5-2536-426e-b27c-84391144ba7b\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 121\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 789\n",
      "--------------------------------------------------------------------------------\n",
      "the pixel classes, as seen in Table 4. For example, the shark’s\n",
      "mouth was separated from the shadow of the tattoo. In\n",
      "contrast, DeMSI does not provide refined annotations for\n",
      "tattoos, leading to multiple background pixels being incor-\n",
      "rectly annotated as part of a tattoo, as shown in Table 15.\n",
      "The second factor relates to tattoo classes with insufficient\n",
      "semantic samples, such as dragons, letters, and spider webs,\n",
      "which were excluded in TSSD2023 and are ignored by our\n",
      "approach, as seen by the text tattoo in Table 15. Due to\n",
      "these differences in annotation design between DeMSI and\n",
      "TSSD2023 datasets, performance degradation occurs when\n",
      "directly applying our approach.\n",
      "To alleviate the performance degradation caused by an-\n",
      "notation differences between the datasets, we fine-tuned the\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a91ea9d2-58f2-4ff0-b416-63a02117a702\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 122\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 784\n",
      "--------------------------------------------------------------------------------\n",
      "To alleviate the performance degradation caused by an-\n",
      "notation differences between the datasets, we fine-tuned the\n",
      "VOLUME 4, 2016 15\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "TABLE 13. Evaluation of the effect of data\n",
      "augmentation components. The results were\n",
      "obtained using proposed LLM F CL, SegFormer,\n",
      "and proposed G-OPenIPCS.\n",
      "No Aug. Geometric Image\n",
      "Adjustments Dropout CSA\n",
      "(Ours) mIoU IoU\n",
      "UUC\n",
      "✓ 0.4556±.186 0.1547\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8edf5865-a12c-4e65-97bc-874f089bf1ad\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 123\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 793\n",
      "--------------------------------------------------------------------------------\n",
      "and proposed G-OPenIPCS.\n",
      "No Aug. Geometric Image\n",
      "Adjustments Dropout CSA\n",
      "(Ours) mIoU IoU\n",
      "UUC\n",
      "✓ 0.4556±.186 0.1547\n",
      "✓ 0.4320±.182 0.1797\n",
      "✓ 0.4814±.203 0.1884\n",
      "✓ 0.4959±.189 0.1538\n",
      "✓ 0.4576±.203 0.1770\n",
      "✓ ✓ ✓ 0.4733±.196 0.2303\n",
      "✓ ✓ ✓ ✓ 0.4900±.201 0.2753\n",
      "TABLE 14. Evaluation of the performance on the DeMSI dataset in the\n",
      "closed-set tattoo segmentation scenario. The results were obtained using the\n",
      "proposed LLM F CL and SegFormer.\n",
      "Fine-tuned IoU\n",
      "Tattoo FPR FNR\n",
      "0.3878 0.0234 0.5784\n",
      "✓ 0.8232 0.0278 0.0945\n",
      "SegFormer network’s last layer using the DeMSI dataset.\n",
      "For the fit, was used 60% of the images for training and 40%\n",
      "for testing as proposed by [13]. After this adjustment, our\n",
      "approach proved effective for performing closed-set tattoo\n",
      "segmentation, as evidenced by the significantly reduced\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 4fafbed4-8cba-4da4-97eb-67498fd7f977\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 124\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 761\n",
      "--------------------------------------------------------------------------------\n",
      "approach proved effective for performing closed-set tattoo\n",
      "segmentation, as evidenced by the significantly reduced\n",
      "FNR values in Table 14. Visually, the improvement in\n",
      "segmentation quality before and after fine-tuning can be\n",
      "observed in Table 15.\n",
      "TABLE 15. Visual result samples of closed-set tattoo segmentation on the\n",
      "DeMSI dataset. The tattoo and background pixels are depicted as black and\n",
      "white, respectively. The results were obtained using the proposed LLM F CL\n",
      "and SegFormer.\n",
      "Image\n",
      "Mask\n",
      "Fine-Tuned\n",
      "No\n",
      "Fine-Tuned\n",
      "VI. CONCLUSIONS\n",
      "This paper built a novo tattoo semantic segmentation dataset\n",
      "called TSSD2023, introducing an unexplored and challeng-\n",
      "ing problem in semantic tattoo recognition. This dataset can\n",
      "serve as a valuable basis for future research.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 7824ac6f-3d53-423e-8339-1410615504c6\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 125\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 794\n",
      "--------------------------------------------------------------------------------\n",
      "ing problem in semantic tattoo recognition. This dataset can\n",
      "serve as a valuable basis for future research.\n",
      "Furthermore, the paper has presented the proposed Large-\n",
      "Margin Focal Loss ( LLM F CL) to enhance tattoo seman-\n",
      "tic segmentation outcomes in both closed and open-set\n",
      "scenarios. In the closed-set scenario, LLM F CL performed\n",
      "competitively and outperformed other evaluated loss func-\n",
      "tions in terms of Macro F1, demonstrating its suitability\n",
      "for closed-set semantic segmentation. An in-depth analysis\n",
      "at the class level revealed that superior performance is\n",
      "generally observed in classes depicted by high semantic\n",
      "dissimilarity and minimal overlap with other tattoos within\n",
      "an image. Conversely, the most challenging classes rarely\n",
      "appear in isolation, often serving as complementary com-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 61aad28e-9bfb-427e-a30c-bf2c217c9b24\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 126\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 791\n",
      "--------------------------------------------------------------------------------\n",
      "an image. Conversely, the most challenging classes rarely\n",
      "appear in isolation, often serving as complementary com-\n",
      "ponents to other tattoos. In the open-set scenario, this paper\n",
      "proposes a generalized approach for the OpenIPCS method\n",
      "named G-OpenIPCS that facilitates the integration of this\n",
      "open-set classifier with more modern segmentation network\n",
      "architectures, such as transform-based networks. Using G-\n",
      "OpenIPCS, we compared the performance of different loss\n",
      "functions with the proposed LLM F CL, which showed higher\n",
      "scores regarding AUROC, Macro F1, and overall mIoU.\n",
      "When considering only the UUCs, LLM F CL performed\n",
      "competitively in AUROC while outperforming F1-Score\n",
      "and IoU. This highlights its effectiveness in the domain of\n",
      "open-set tattoo semantic segmentation. A more thorough\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ec959999-918f-4e81-a138-9082dda60577\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 127\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 775\n",
      "--------------------------------------------------------------------------------\n",
      "and IoU. This highlights its effectiveness in the domain of\n",
      "open-set tattoo semantic segmentation. A more thorough\n",
      "examination revealed that classes bearing high similarity\n",
      "to other classes, overlapped, and with limited samples, it\n",
      "represented the significant challenges for open-set tattoo\n",
      "semantic segmentation.\n",
      "It is worth noting that when a semantic tattoo class has\n",
      "high variability, segmentation errors can occur due to false\n",
      "outliers. To address this issue, we propose a new data aug-\n",
      "mentation technique named Class Semantic Augmentation\n",
      "(CSA) that increases the available semantic information of\n",
      "classes for better model generalization. Nevertheless, there\n",
      "are opportunities for further advancements in this research\n",
      "area.\n",
      "When comparing our approach, which combines\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: cc18c714-da7b-4180-a568-c385d1d76fc3\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 128\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 758\n",
      "--------------------------------------------------------------------------------\n",
      "are opportunities for further advancements in this research\n",
      "area.\n",
      "When comparing our approach, which combines\n",
      "LLM F CL, SegFormer, and G-OpenIPCS, with other state-\n",
      "of-the-art methods for open-set tattoo semantic segmenta-\n",
      "tion, our approach consistently attains the highest overall\n",
      "results. It surpasses the performance of other state-of-the-art\n",
      "methods, underscoring its potential for addressing challeng-\n",
      "ing scenarios in open-set semantic tattoo segmentation.\n",
      "Furthermore, our approach applicability extends beyond\n",
      "the domain of tattoo segmentation, and it holds the potential\n",
      "to enhance open-set semantic segmentation in various other\n",
      "application domains. Finally, it is essential to note that while\n",
      "our approach demonstrates promise, there remains room\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 378044bb-91de-4872-bc9d-f1009fc41ee7\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 129\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 721\n",
      "--------------------------------------------------------------------------------\n",
      "application domains. Finally, it is essential to note that while\n",
      "our approach demonstrates promise, there remains room\n",
      "for further improvements in specific areas. This includes\n",
      "improving the handling of class imbalance, class overlap,\n",
      "high intra-class semantic variability, and, in some cases,\n",
      "high inter-class similarity, detailed as follows.\n",
      "VII. CHALLENGES IN OPEN SET TATTOO SEMANTIC\n",
      "SEGMENTATION\n",
      "Although the approach discussed here is promising and\n",
      "with results superior to state-of-the-art methods for open-\n",
      "set semantic segmentation, in the context of tattoos, we can\n",
      "16 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 62069299-4a81-48ca-b545-7f29840a1b02\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 130\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 756\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "observe numerous difficulties and open points for this line\n",
      "of research. Below, we detail some of those that we consider\n",
      "most relevant in the proposed context:\n",
      "A. SEMANTIC SHIFT\n",
      "Semantic shift is caused by classes that influence model\n",
      "predictions due to their semantic similarity to other classes,\n",
      "causing segmentation errors [33], [37]. This semantic shift\n",
      "can be caused in three different ways in the tattoo dataset.\n",
      "The first is the presence of invisible objects in the back-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 1a5258b1-ec4e-4917-9b3f-5a9a9e76c66a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 131\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 796\n",
      "--------------------------------------------------------------------------------\n",
      "The first is the presence of invisible objects in the back-\n",
      "ground class that are similar to known and unknown classes.\n",
      "The background class is a large grouping of objects rel-\n",
      "atively irrelevant to the application. However, tattoos are\n",
      "semantically very similar to illustrations projected onto\n",
      "different surfaces. For example, a skull on a tattoo artist’s\n",
      "chair can be easily identified as a tattoo, generating a\n",
      "misclassification. Furthermore, due to the great difficulty in\n",
      "annotating a large volume of tattoos for semantic segmen-\n",
      "tation, some tattoos are “ignored” in the labeling process\n",
      "and noted as belonging to the background class, which can\n",
      "cause misclassifications.\n",
      "The second comprises objects of known classes are simi-\n",
      "lar to each other. Some semantic classes are similar to others\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 0fa9d7e8-d26a-48c5-9b35-785d69947e64\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 132\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 769\n",
      "--------------------------------------------------------------------------------\n",
      "The second comprises objects of known classes are simi-\n",
      "lar to each other. Some semantic classes are similar to others\n",
      "within the set of known classes. For example, the ‘wolf’,\n",
      "‘dog’, and ‘fox’ classes belong to the same animal family,\n",
      "thus presenting similar characteristics that even humans may\n",
      "find difficult to distinguish. This high similarity makes the\n",
      "process of recognition by the segmentation model difficult,\n",
      "which can, in some cases, generate classification errors.\n",
      "Furthermore, this semantic similarity can extend to small\n",
      "parts of the tattoos, which can cause small segmentation\n",
      "errors. Finally, the third includes objects of unknown classes\n",
      "similar to known classes. This challenge is similar to the\n",
      "previous one but more demanding. The segmentation model\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 84df796c-fa0e-4653-85c9-75281533bb42\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 133\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 771\n",
      "--------------------------------------------------------------------------------\n",
      "similar to known classes. This challenge is similar to the\n",
      "previous one but more demanding. The segmentation model\n",
      "must be trained to produce robust decision boundaries to\n",
      "separate known data from each other and separate it from\n",
      "unknown data. While in the previous problem, it is only\n",
      "necessary to distinguish among known classes.\n",
      "A possible way to alleviate these factors’ influence is to\n",
      "make a descriptive note of the tattoo using natural language.\n",
      "This can help identify tattoos more effectively, avoiding\n",
      "reducing the artistic feature of a tattoo to a set of labels.\n",
      "B. INTRA-CLASS VARIABILITY\n",
      "An open challange is also related to objects from known\n",
      "classes with different characteristics from the group of\n",
      "objects from the same semantic class. Because tattoos are a\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f4904faa-321a-4f2d-b30a-6b1ccf116931\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 134\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 792\n",
      "--------------------------------------------------------------------------------\n",
      "classes with different characteristics from the group of\n",
      "objects from the same semantic class. Because tattoos are a\n",
      "form of artistic expression based on drawings, the number of\n",
      "designs that can be thought of to create a tattoo is practically\n",
      "unlimited. This infinite universe of possibilities makes each\n",
      "tattoo unique, with the variability of tattoos present in the\n",
      "real world being practically immeasurable. When defining a\n",
      "semantic class for a set of objects, we intuitively state that\n",
      "these objects are similar, which is generally true. However,\n",
      "concerning tattoos, it is possible that despite belonging to\n",
      "the same semantic class, practically none of the patterns are\n",
      "identically replicated between tattoos. In some cases, the\n",
      "patterns are so different between objects of the known class\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 67a9b180-79a4-4801-ba05-829752ca86a9\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 135\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 798\n",
      "--------------------------------------------------------------------------------\n",
      "identically replicated between tattoos. In some cases, the\n",
      "patterns are so different between objects of the known class\n",
      "that the segmentation model segments the known tattoo as\n",
      "belonging to the unknown class.\n",
      "C. OBJECTS OF KNOWN AND UNKNOWN CLASSES\n",
      "SEGMENTED WITH LOW PRECISION\n",
      "This challenge incorporates several subproblems encoun-\n",
      "tered in closed-set segmentation and also in open-set seg-\n",
      "mentation. In segmenting closed sets, the models suffer\n",
      "from the subproblem of overlap between objects, mak-\n",
      "ing it difficult to separate the boundaries between objects\n",
      "accurately. Another common subproblem is small objects\n",
      "present in images, generally classified as objects from other\n",
      "classes, such as the background class. The open-set semantic\n",
      "segmentation task inherits all these subproblems. However,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 9c079932-6c08-4026-bf04-a7c1a955ef52\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 136\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 764\n",
      "--------------------------------------------------------------------------------\n",
      "classes, such as the background class. The open-set semantic\n",
      "segmentation task inherits all these subproblems. However,\n",
      "in open-set segmentation, we still have the challenge of ac-\n",
      "curately classifying regions of completely unknown objects\n",
      "while dealing with the subproblems derived from closed-set\n",
      "segmentation.\n",
      "D. DATA AUGMENTATION FOR SEMANTIC\n",
      "SEGMENTATION\n",
      "The data augmentation proposal presented here may be\n",
      "promising for incorporating more data into tattoo datasets,\n",
      "the annotation of which is costly and time-consuming. A\n",
      "possible path is to combine the approach presented in [13]\n",
      "with the data augmentation ideas proposed in this paper in\n",
      "order to create greater variability of classes and examples,\n",
      "including data in an open set, to enable the training of\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: d0c8c761-b7f8-47a2-a7e2-340123e15cd1\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 137\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 791\n",
      "--------------------------------------------------------------------------------\n",
      "order to create greater variability of classes and examples,\n",
      "including data in an open set, to enable the training of\n",
      "more complex models and, consequently, increase final\n",
      "performance.\n",
      "E. COMPUTATIONAL COMPLEXITY\n",
      "We understand that a detailed analysis of computational\n",
      "complexity must be conducted for certain applications,\n",
      "mainly involving embedded systems. This analysis must\n",
      "include the training and deployment of the model according\n",
      "to the target device. Specific architectures for this type of\n",
      "application can also be evaluated, as demonstrated in [71].\n",
      "REFERENCES\n",
      "[1] S. T. Acton and A. Rossi, “Matching and retrieval of tattoo images: Active\n",
      "contour CBIR and glocal image features,” in Proceedings of the IEEE\n",
      "Southwest Symposium on Image Analysis and Interpretation, 2008, pp.\n",
      "21–24.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ab67d4f7-47e3-4ac0-9682-cac8c7f65904\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 138\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 762\n",
      "--------------------------------------------------------------------------------\n",
      "Southwest Symposium on Image Analysis and Interpretation, 2008, pp.\n",
      "21–24.\n",
      "[2] T. Harbert, “FBI wants better automated image analysis for tattoos\n",
      "[news],” IEEE Spectrum, vol. 52, no. 9, pp. 13–16, 2015.\n",
      "[3] J.-E. Lee, R. Jin, and A. K. Jain, “Rank-based distance metric learning: An\n",
      "application to image retrieval,” in Proceedings of the IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, 2008, pp. 1–8.\n",
      "[4] S. Fang, J. Coverdale, P. Nguyen, and M. Gordon, “Tattoo recognition in\n",
      "screening for victims of human trafficking,” Journal of Nervous & Mental\n",
      "Disease, vol. 206, no. 10, pp. 824–827, 2018.\n",
      "[5] F. Bacchini and L. Lorusso, “A tattoo is not a face. ethical aspects of tattoo-\n",
      "based biometrics,” Journal of Information, Communication and Ethics in\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: e18b2553-bd6f-437e-a614-78facdb22e53\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 139\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 745\n",
      "--------------------------------------------------------------------------------\n",
      "based biometrics,” Journal of Information, Communication and Ethics in\n",
      "Society, vol. 16, no. 2, pp. 110–122, 2017.\n",
      "[6] R. T. Da Silva and H. S. Lopes, “A transfer learning approach for the\n",
      "tattoo classification problem,” in 2022 IEEE Latin American Conference\n",
      "on Computational Intelligence (LA-CCI). IEEE, 2022, pp. 1–6.\n",
      "VOLUME 4, 2016 17\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8d692570-022b-46a9-987c-287f64ad7f34\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 140\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 791\n",
      "--------------------------------------------------------------------------------\n",
      "[7] Y . Mo, Y . Wu, X. Yang, F. Liu, and Y . Liao, “Review the state-of-\n",
      "the-art technologies of semantic segmentation based on deep learning,”\n",
      "Neurocomputing, vol. 493, pp. 626–646, 2022.\n",
      "[8] C. Jiawang and Z. Yuan, “Tattoo recognition based on triplet GAN,” in\n",
      "Proceedings pf 37th Chinese Control Conference. IEEE, 2018, pp. 9595–\n",
      "9597.\n",
      "[9] T. Hrka ´c, K. Brki ´c, and Z. Kalafati ´c, “Tattoo detection for soft biometric\n",
      "de-identification based on convolutional neural networks,” in Proceedings\n",
      "of the OAGM & ARW Joint Workshop. Verlag der Technischen Univer-\n",
      "sität Graz, 2016.\n",
      "[10] J. Dong, X. Qu, and H. Li, “Color tattoo segmentation based on skin\n",
      "color space and k-mean clustering,” in Proceedings of the IEEE 4th\n",
      "International Conference on Information, Cybernetics and Computational\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 2b8b35a8-3f4e-4a91-8315-223c24b09a25\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 141\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 774\n",
      "--------------------------------------------------------------------------------\n",
      "International Conference on Information, Cybernetics and Computational\n",
      "Social Systems, 2017, pp. 53–56.\n",
      "[11] M. Nicolás-Díaz, A. Morales-González, and H. Méndez-Vázquez,\n",
      "“Weighted average pooling of deep features for tattoo identification,”\n",
      "Multimedia Tools and Applications, vol. 81, no. 18, pp. 25 853–25 875,\n",
      "2022.\n",
      "[12] M. Ngan and P. Grother, “Tattoo recognition technology - challenge (tatt-\n",
      "c): an open tattoo database for developing tattoo recognition research,” in\n",
      "Proceedings of the IEEE International Conference on Identity, Security\n",
      "and Behavior Analysis, 2015, pp. 1–6.\n",
      "[13] L. J. Gonzalez-Soler, C. Rathgeb, and D. Fischer, “Semi-synthetic data\n",
      "generation for tattoo segmentation,” in 2023 11th International Workshop\n",
      "on Biometrics and Forensics, 2023, pp. 1–6.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f1b6bd38-2bf3-47f2-8243-41e39373c6fe\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 142\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 781\n",
      "--------------------------------------------------------------------------------\n",
      "generation for tattoo segmentation,” in 2023 11th International Workshop\n",
      "on Biometrics and Forensics, 2023, pp. 1–6.\n",
      "[14] C. Geng, S.-j. Huang, and S. Chen, “Recent advances in open set recog-\n",
      "nition: A survey,” IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, vol. 43, no. 10, pp. 3614–3631, 2020.\n",
      "[15] C. C. Da Silva, K. Nogueira, H. N. Oliveira, and J. A. dos Santos, “Towards\n",
      "open-set semantic segmentation of aerial images,” in Proceedings of the\n",
      "IEEE Latin American GRSS & ISPRS Remote Sensing Conference.\n",
      "Santiago, Chile: IEEE, 2020, pp. 16–21.\n",
      "[16] I. Nunes, M. B. Pereira, H. Oliveira, J. A. dos Santos, and M. Poggi,\n",
      "“Conditional reconstruction for open-set semantic segmentation,” in Pro-\n",
      "ceedings of the IEEE International Conference on Image Processing.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 3ccb4f19-5795-46d9-b5a8-40cac9fb87a4\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 143\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 779\n",
      "--------------------------------------------------------------------------------\n",
      "ceedings of the IEEE International Conference on Image Processing.\n",
      "Bordeaux, France: IEEE, 2022, pp. 946–950.\n",
      "[17] M. Gutoski, A. E. Lazzaretti, and H. S. Lopes, “Deep metric learning\n",
      "for open-set human action recognition in videos,” Neural Computing and\n",
      "Applications, vol. 33, pp. 1207–1220, 2021.\n",
      "[18] D. Miller, N. Sunderhauf, M. Milford, and F. Dayoub, “Class anchor\n",
      "clustering: A loss for distance-based open set recognition,” in Proceedings\n",
      "of the IEEE/CVF Winter Conference on Applications of Computer Vision.\n",
      "Waikoloa, USA: IEEE, 2021, pp. 3570–3578.\n",
      "[19] W. Liu, Y . Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for\n",
      "convolutional neural networks,” arXiv preprint arXiv:1612.02295, 2016.\n",
      "[20] T. Kobayashi, “Large margin in softmax cross-entropy loss,” in Proceed-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 263a5c0e-7e42-45e4-bfbd-3c4984bba29f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 144\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 730\n",
      "--------------------------------------------------------------------------------\n",
      "[20] T. Kobayashi, “Large margin in softmax cross-entropy loss,” in Proceed-\n",
      "ings of the British Machine Vision Conference (BMVC), 2019.\n",
      "[21] X. Li, D. Chang, T. Tian, and J. Cao, “Large-margin regularized softmax\n",
      "cross-entropy loss,” IEEE access, vol. 7, pp. 19 572–19 578, 2019.\n",
      "[22] Q. Xu, S. Ghosh, X. Xu, Y . Huang, and A. W. K. Kong, “Tattoo detection\n",
      "based on CNN and remarks on the NIST database,” in Proceedings of the\n",
      "IEEE International Conference on Biometrics, 2016, pp. 1–7.\n",
      "[23] H. Han, J. Li, A. K. Jain, S. Shan, and X. Chen, “Tattoo image search at\n",
      "scale: Joint detection and compact representation learning,” IEEE Trans-\n",
      "actions on Pattern Analysis and Machine Intelligence, vol. 41, no. 10, pp.\n",
      "2333–2348, 2019.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: f0190a0c-dc6f-4484-99e6-af62392c1897\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 145\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "actions on Pattern Analysis and Machine Intelligence, vol. 41, no. 10, pp.\n",
      "2333–2348, 2019.\n",
      "[24] M. Nicolás-Díaz, A. Morales-González, and H. Méndez-Vázquez, “Deep\n",
      "generic features for tattoo identification,” in Proceedings of the 24th\n",
      "Iberoamerican Congress on Progress in Pattern Recognition, Image Anal-\n",
      "ysis, Computer Vision, and Applications. Berlin, Heidelberg: Springer-\n",
      "Verlag, 2019, p. 272–282.\n",
      "[25] A. Jain, Y . Chen, and U. Park, “Scars, marks & tattoos (SMT): Physical\n",
      "attributes for person identification,” Michigan State University, East Lans-\n",
      "ing, USA, Tech. Rep. CSE 07-22, 2007.\n",
      "[26] A. K. Jain, J.-E. Lee, and R. Jin, “Tattoo-ID: Automatic tattoo image\n",
      "retrieval for suspect and victim identification,” in Advances in Multimedia\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a3d087ed-a2a0-4bc6-bae1-ed1759f4b1bf\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 146\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 752\n",
      "--------------------------------------------------------------------------------\n",
      "retrieval for suspect and victim identification,” in Advances in Multimedia\n",
      "Information Processing. Heidelberg: Springer, 2007, pp. 256–265.\n",
      "[27] J. D. Allen, N. Zhao, J. Yuan, and X. Liu, “Unsupervised tattoo seg-\n",
      "mentation combining bottom-up and top-down cues,” in Proceedings of\n",
      "SPIE Mobile Multimedia/Image Processing, Security, and Applications\n",
      "Conference, S. S. Agaian, S. A. Jassim, and Y . Du, Eds., 2011, pp. 1–7.\n",
      "[28] B. Heflin, W. Scheirer, and T. E. Boult, “Detecting and classifying scars,\n",
      "marks, and tattoos found in the wild,” in Proceedings of the IEEE Fifth In-\n",
      "ternational Conference on Biometrics: Theory, Applications and Systems\n",
      "(BTAS), 2012, pp. 31–38.\n",
      "[29] P. Duangphasuk and W. Kurutach, “Tattoo skin detection and segmentation\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 65335580-60fe-4353-a8dd-1723e4eaa332\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 147\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 753\n",
      "--------------------------------------------------------------------------------\n",
      "(BTAS), 2012, pp. 31–38.\n",
      "[29] P. Duangphasuk and W. Kurutach, “Tattoo skin detection and segmentation\n",
      "using image negative method,” in Proceedings of the IEEE 13th Inter-\n",
      "national Symposium on Communications and Information Technologies\n",
      "(ISCIT), 2013, pp. 354–359.\n",
      "[30] J. Kim, A. Parra, H. Li, and E. J. Delp, “Efficient graph-cut tattoo\n",
      "segmentation,” in Visual Information Processing and Communication VI,\n",
      "A. Said, O. G. Guleryuz, and R. L. Stevenson, Eds. SPIE, 2015, pp. 1–8.\n",
      "[31] T. Hrka ´c, K. B. S. Ribari ´c, and D. Mar ˇceti´c, “Deep learning architectures\n",
      "for tattoo detection and de-identification,” in Proceedings of the IEEE\n",
      "First International Workshop on Sensing, Processing and Learning for\n",
      "Intelligent Machines (SPLINE), 2016, pp. 1–5.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: adac0c8c-6139-4a9b-9211-ac032be89de0\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 148\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 790\n",
      "--------------------------------------------------------------------------------\n",
      "First International Workshop on Sensing, Processing and Learning for\n",
      "Intelligent Machines (SPLINE), 2016, pp. 1–5.\n",
      "[32] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\n",
      "for semantic segmentation,” in Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition, 2015, pp. 3431–3440.\n",
      "[33] A. Brilhador, A. E. Lazzaretti, and H. S. Lopes, “A comparative study\n",
      "for open set semantic segmentation methods,” in Anais do 15 Congresso\n",
      "Brasileiro de Inteligência Computacional. Joinville, Brazil: SBIC, 2021,\n",
      "pp. 1–8.\n",
      "[34] I. Nunes, C. Laranjeira, H. Oliveira, and J. A. dos Santos, “A systematic\n",
      "review on open-set segmentation,” Computers & Graphics, vol. 115, pp.\n",
      "296–308, 2023.\n",
      "[35] Y . Tian, D. Su, S. Lauria, and X. Liu, “Recent advances on loss functions\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 6d8e153c-3f96-459f-91b8-f794ef0a281a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 149\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 732\n",
      "--------------------------------------------------------------------------------\n",
      "296–308, 2023.\n",
      "[35] Y . Tian, D. Su, S. Lauria, and X. Liu, “Recent advances on loss functions\n",
      "in deep learning for computer vision,” Neurocomputing, vol. 497, pp. 129–\n",
      "158, 2022.\n",
      "[36] Y . Liu, Y . Tang, L. Zhang, L. Liu, M. Song, K. Gong, Y . Peng, J. Hou, and\n",
      "T. Jiang, “Hyperspectral open set classification with unknown classes re-\n",
      "jection towards deep networks,” International Journal of Remote Sensing,\n",
      "vol. 41, no. 16, pp. 6355–6383, 2020.\n",
      "[37] Z. Cui, W. Longshi, and R. Wang, “Open set semantic segmentation\n",
      "with statistical test and adaptive threshold,” in Proceedings of the IEEE\n",
      "International Conference on Multimedia and Expo, 2020, pp. 1–6.\n",
      "[38] H. Oliveira, C. Silva, G. L. Machado, K. Nogueira, and J. A. dos Santos,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 480dbc18-403e-4a69-bf39-b718db5e0560\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 150\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 761\n",
      "--------------------------------------------------------------------------------\n",
      "[38] H. Oliveira, C. Silva, G. L. Machado, K. Nogueira, and J. A. dos Santos,\n",
      "“Fully convolutional open set segmentation,” Machine Learning, pp. 1–52,\n",
      "2021.\n",
      "[39] D. Hendrycks, S. Basart, M. Mazeika, A. Zou, J. Kwon, M. Mostajabi,\n",
      "J. Steinhardt, and D. Song, “Scaling out-of-distribution detection for\n",
      "real-world settings,” in Proceedings of the 39th International Conference\n",
      "on Machine Learning, ser. Proceedings of Machine Learning Research,\n",
      "K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,\n",
      "Eds., vol. 162. PMLR, 17–23 Jul 2022, pp. 8759–8773.\n",
      "[40] M. Grci ´c, P. Bevandi ´c, and S. Šegvi ´c, “Densehybrid: Hybrid anomaly\n",
      "detection for dense open-set recognition,” in European Conference on\n",
      "Computer Vision. Springer, 2022, pp. 500–517.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 32776032-630b-44f5-9a37-b229edb53999\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 151\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 746\n",
      "--------------------------------------------------------------------------------\n",
      "detection for dense open-set recognition,” in European Conference on\n",
      "Computer Vision. Springer, 2022, pp. 500–517.\n",
      "[41] X. Guo, J. Liu, T. Liu, and Y . Yuan, “Handling open-set noise and\n",
      "novel target recognition in domain adaptive semantic segmentation,” IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 8,\n",
      "pp. 9846–9861, 2023.\n",
      "[42] J. Cen, P. Yun, J. Cai, M. Y . Wang, and M. Liu, “Deep metric learning\n",
      "for open world semantic segmentation,” in Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision, 2021, pp. 15 333–15 342.\n",
      "[43] J. Hong, W. Li, J. Han, J. Zheng, P. Fang, M. Harandi, and L. Petersson,\n",
      "“Goss: Towards generalized open-set semantic segmentation,” The Visual\n",
      "Computer, pp. 1–14, 2023.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 06bcd700-ce2b-4e56-b92c-41cf5ec389eb\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 152\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 739\n",
      "--------------------------------------------------------------------------------\n",
      "“Goss: Towards generalized open-set semantic segmentation,” The Visual\n",
      "Computer, pp. 1–14, 2023.\n",
      "[44] H. Zhang and H. Ding, “Prototypical matching and open set rejection\n",
      "for zero-shot semantic segmentation,” in Procceding of the IEEE/CVF\n",
      "International Conference on Computer Vision, 2021, pp. 6974–6983.\n",
      "[45] B. Yu, T. Liu, M. Gong, C. Ding, and D. Tao, “Correcting the triplet\n",
      "selection bias for triplet loss,” in Proceedings of the European Conference\n",
      "on Computer Vision (ECCV), 2018, pp. 71–87.\n",
      "[46] S. N. Rai, F. Cermelli, B. Caputo, and C. Masone, “Mask2anomaly:\n",
      "Mask transformer for universal open-set segmentation,” arXiv preprint\n",
      "arXiv:2309.04573, 2023.\n",
      "[47] M. J. Wilber, E. Rudd, B. Heflin, Y .-M. Lui, and T. E. Boult, “Exemplar\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 0dad354b-ecba-46a9-920e-e8b588224882\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 153\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 760\n",
      "--------------------------------------------------------------------------------\n",
      "arXiv:2309.04573, 2023.\n",
      "[47] M. J. Wilber, E. Rudd, B. Heflin, Y .-M. Lui, and T. E. Boult, “Exemplar\n",
      "codes for facial attributes and tattoo recognition,” in Proceedings of the\n",
      "IEEE Winter Conference on Applications of Computer Vision, 2014, pp.\n",
      "205–212.\n",
      "[48] W. He, J. Wang, L. Wang, R. Pan, and W. Gao, “A semantic segmentation\n",
      "algorithm for fashion images based on modified mask rcnn,” Multimedia\n",
      "Tools and Applications, vol. 82, no. 18, pp. 28 427–28 444, 2023.\n",
      "[49] W. Wan, Y . Zhong, T. Li, and J. Chen, “Rethinking feature distribution\n",
      "for loss functions in image classification,” in Proceedings of the IEEE\n",
      "18 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: fffe8264-1070-4ae6-8492-26f7ce5bc579\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 154\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 738\n",
      "--------------------------------------------------------------------------------\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "conference on computer vision and pattern recognition, 2018, pp. 9117–\n",
      "9126.\n",
      "[50] K. Crammer and Y . Singer, “On the algorithmic implementation of multi-\n",
      "class kernel-based vector machines,” Journal of machine learning research,\n",
      "vol. 2, no. Dec, pp. 265–292, 2001.\n",
      "[51] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense\n",
      "object detection,” in Proceedings of the IEEE international conference on\n",
      "computer vision, 2017, pp. 2980–2988.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 5ade6915-aec1-4653-a8b7-6214ff9d3a31\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 155\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 786\n",
      "--------------------------------------------------------------------------------\n",
      "object detection,” in Proceedings of the IEEE international conference on\n",
      "computer vision, 2017, pp. 2980–2988.\n",
      "[52] H.-M. Yang, X.-Y . Zhang, F. Yin, Q. Yang, and C.-L. Liu, “Convolutional\n",
      "prototype network for open set recognition,” IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence, vol. 44, no. 5, pp. 2358–2370, 2022.\n",
      "[53] X. Sun, Z. Yang, C. Zhang, K.-V . Ling, and G. Peng, “Conditional\n",
      "gaussian distribution learning for open set recognition,” in Proceedingsof\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n",
      "2020, pp. 13 480–13 489.\n",
      "[54] R. Shwartz-Ziv and N. Tishby, “Opening the black box of deep neural\n",
      "networks via information,” ArXiv preprint, vol. 1703.00810, 2017.\n",
      "[55] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway networks,”\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8ba7ae40-3b69-4a0f-aaeb-1b8074d2b813\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 156\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 790\n",
      "--------------------------------------------------------------------------------\n",
      "[55] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway networks,”\n",
      "ArXiv preprint, vol. 1505.00387, 2015.\n",
      "[56] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\n",
      "connected convolutional networks,” in Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition. Honolulu, USA:\n",
      "IEEE, 2017, pp. 4700–4708.\n",
      "[57] M. E. Tipping and C. M. Bishop, “Mixtures of probabilistic principal\n",
      "component analyzers,” Neural Computation, vol. 11, no. 2, pp. 443–482,\n",
      "1999.\n",
      "[58] T. DeVries and G. W. Taylor, “Improved regularization of convolutional\n",
      "neural networks with cutout,” arXiv preprint arXiv:1708.04552, 2017.\n",
      "[59] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\n",
      "A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “Im-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 72e0c9d3-755a-4399-8089-1f951f10751f\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 157\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 750\n",
      "--------------------------------------------------------------------------------\n",
      "A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “Im-\n",
      "ageNet Large Scale Visual Recognition Challenge,” International Journal\n",
      "of Computer Vision, vol. 115, no. 3, pp. 211–252, 2015.\n",
      "[60] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y . Yoo, “Cutmix:\n",
      "Regularization strategy to train strong classifiers with localizable features,”\n",
      "in Proceedings of the IEEE/CVF international conference on computer\n",
      "vision, 2019, pp. 6023–6032.\n",
      "[61] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\n",
      "T. Xiang, P. H. Torr et al., “Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers,” in Proceedings of\n",
      "the IEEE/CVF conference on computer vision and pattern recognition,\n",
      "2021, pp. 6881–6890.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: b9dcce3e-ef27-49ed-bbe3-236d88d8ec29\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 158\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 744\n",
      "--------------------------------------------------------------------------------\n",
      "the IEEE/CVF conference on computer vision and pattern recognition,\n",
      "2021, pp. 6881–6890.\n",
      "[62] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin\n",
      "transformer: Hierarchical vision transformer using shifted windows,” in\n",
      "Proceedings of the IEEE/CVF international conference on computer vi-\n",
      "sion, 2021, pp. 10 012–10 022.\n",
      "[63] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Seg-\n",
      "former: Simple and efficient design for semantic segmentation with trans-\n",
      "formers,” Advances in Neural Information Processing Systems, vol. 34,\n",
      "pp. 12 077–12 090, 2021.\n",
      "[64] T. Dozat, “Incorporating Nesterov Momentum into Adam,” in Proceedings\n",
      "of the 4th International Conference on Learning Representations, 2016, pp.\n",
      "1–4.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: b6620e9c-3eea-41fb-96c7-2125e0952ffd\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 159\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 737\n",
      "--------------------------------------------------------------------------------\n",
      "of the 4th International Conference on Learning Representations, 2016, pp.\n",
      "1–4.\n",
      "[65] H.-M. Yang, X.-Y . Zhang, F. Yin, and C.-L. Liu, “Robust classification\n",
      "with convolutional prototype learning,” in Proceedings of the IEEE Con-\n",
      "ference on Computer Vision and Pattern Recognition, 2018, pp. 3474–\n",
      "3482.\n",
      "[66] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning imbalanced\n",
      "datasets with label-distribution-aware margin loss,” Advances in neural\n",
      "information processing systems, vol. 32, 2019.\n",
      "[67] M. Yeung, L. Rundo, Y . Nan, E. Sala, C.-B. Schönlieb, and G. Yang,\n",
      "“Calibrating the dice loss to handle neural network overconfidence for\n",
      "biomedical image segmentation,” Journal of Digital Imaging, vol. 36,\n",
      "no. 2, pp. 739–752, 2023.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 167eab2d-0edb-4359-afd0-445fa1fb835a\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 160\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 747\n",
      "--------------------------------------------------------------------------------\n",
      "biomedical image segmentation,” Journal of Digital Imaging, vol. 36,\n",
      "no. 2, pp. 739–752, 2023.\n",
      "[68] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E. Boult, “Toward\n",
      "open set recognition,” IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, vol. 35, no. 7, pp. 1757–1772, 2013.\n",
      "[69] A. Bendale and T. E. Boult, “Towards open set deep networks,” in Proceed-\n",
      "ings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "2016, pp. 1563–1572.\n",
      "[70] Z. Wang, J. Li, Z. Tan, X. Liu, and M. Li, “Swin-upernet: A semantic\n",
      "segmentation model for mangroves and spartina alterniflora loisel based\n",
      "on upernet,” Electronics, vol. 12, no. 5, p. 1111, 2023.\n",
      "[71] B. Olimov, J. Kim, and A. Paul, “Ref-net: Robust, efficient, and fast\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: a4c10e69-528c-4a44-8863-204536847b4e\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 161\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 790\n",
      "--------------------------------------------------------------------------------\n",
      "[71] B. Olimov, J. Kim, and A. Paul, “Ref-net: Robust, efficient, and fast\n",
      "network for semantic segmentation applications using devices with limited\n",
      "computational resources,” IEEE Access, vol. 9, pp. 15 084–15 098, 2021.\n",
      "ANDERSON BRILHADOR received the MSc\n",
      "degree in Computer Science from the Federal\n",
      "University of Technology - Paraná (UTFPR),\n",
      "Brazil, in 2015. He is pursuing a Ph.D. in Elec-\n",
      "trical Engineering and Industrial Informatics with\n",
      "UTFPR. He is currently a professor in the com-\n",
      "puter science program at UTFPR. His research\n",
      "interests include computer vision, machine learn-\n",
      "ing, deep learning, and data mining.\n",
      "RODRIGO TCHALSKI DA SIL VA received\n",
      "MSc degree in Industrial Computing from the\n",
      "Federal University of Technology - Paraná\n",
      "(UTFPR), Brazil, in 2022, and he is currently\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 0813ca06-74f4-40c2-9962-106913059368\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 162\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 763\n",
      "--------------------------------------------------------------------------------\n",
      "Federal University of Technology - Paraná\n",
      "(UTFPR), Brazil, in 2022, and he is currently\n",
      "pursuing a Ph.D. degree in Industrial Computing\n",
      "at the UTFPR. His research interests include com-\n",
      "puter vision, tattoo recognition, machine learning,\n",
      "and complex networks.\n",
      "CARLOS ROBERTO MODINEZ-JUNIOR is\n",
      "currently pursuing an undergraduate degree in\n",
      "Electronic Engineering at Federal University of\n",
      "Technology - Paraná (UTFPR), Brazil. He has\n",
      "been working with image processing and his\n",
      "research interests include semantic segmentation.\n",
      "GABRIEL DE ALMEIDA SP ADAFORAis cur-\n",
      "rently pursuing an undergraduate degree in Com-\n",
      "puter Engineering at the Federal University of\n",
      "Technology - Paraná (UTFPR), Brazil. His re-\n",
      "search interests include image processing and\n",
      "neural networks.\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: 8f795dae-8eb5-458d-99c4-958b38f514ca\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 163\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 781\n",
      "--------------------------------------------------------------------------------\n",
      "Technology - Paraná (UTFPR), Brazil. His re-\n",
      "search interests include image processing and\n",
      "neural networks.\n",
      "HEITOR SIL VÉRIO LOPES received the BSc\n",
      "and MSc degree in Electrical Engineering from\n",
      "Federal University of Technology - Paraná\n",
      "(UTFPR), in 1984 and 1990, respectively, and\n",
      "his PhD from the Federal University of Santa\n",
      "Catarina in 1996. Currently, he is a tenured full\n",
      "Professor with the Department of Eletronics and\n",
      "the Graduate Program on Electrical Engineer-\n",
      "ing and Applied Computer Science (CPGEI) at\n",
      "UTFPR, Curitiba. His major research interests are\n",
      "in the fields of computer vision, deep learning, evolutionary computation,\n",
      "and data mining.\n",
      "ANDRÉ EUGÊNIO LAZZARETTI (MEMBER,\n",
      "IEEE) received his BSc, MSc, and DSc degrees\n",
      "in Electrical Engineering from the Federal Uni-\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Doc ID: ddb85d92-d87c-4d66-812a-39506eef25b1\n",
      "Source: documento.pdf\n",
      "File type: None\n",
      "Chunk ID: 164\n",
      "Chunking: None\n",
      "Tamanho (caracteres): 782\n",
      "--------------------------------------------------------------------------------\n",
      "IEEE) received his BSc, MSc, and DSc degrees\n",
      "in Electrical Engineering from the Federal Uni-\n",
      "versity of Technology - Paraná in 2007, 2010\n",
      "and 2015. He is currently a professor with the\n",
      "Department of Electronics at the Federal Uni-\n",
      "versity of Technology - Paraná. His research\n",
      "interests include machine learning, deep learning,\n",
      "and digital signal processing.\n",
      "VOLUME 4, 2016 19\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBoas práticas ao inspecionar chunks:\\n\\n✔ Sempre verifique os chunks ANTES de culpar o modelo\\n✔ Confira se não há chunks vazios ou duplicados\\n✔ Verifique tamanhos (<= 1000 chars)\\n✔ Confirme metadados (source, chunk_id)\\n\\n90% dos problemas de RAG vêm de ingestão mal validada.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INSPECIONAR CHUNKS ARMAZENADOS NO VECTORSTORE FAISS\n",
    "# ============================================================\n",
    "\n",
    "# Objetivo:\n",
    "# - Verificar quais chunks foram realmente salvos no FAISS\n",
    "# - Inspecionar texto + metadados\n",
    "# - Fazer debug de ingestão (etapa crítica em RAG)\n",
    "\n",
    "# Premissas:\n",
    "# - vectorstore já foi criado OU carregado do disco\n",
    "# - embeddings compatíveis já estão configurados\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. ACESSAR DOCUMENTOS INTERNOS DO VECTORSTORE\n",
    "# ------------------------------------------------------------\n",
    "# O FAISS guarda os Documents em um docstore interno\n",
    "\n",
    "docstore = vectorstore.docstore\n",
    "doc_ids = list(docstore._dict.keys())\n",
    "\n",
    "print(f\"Total de chunks no vectorstore: {len(doc_ids)}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. VISUALIZAR ALGUNS CHUNKS (RECOMENDADO)\n",
    "# ------------------------------------------------------------\n",
    "# Evita poluir o terminal com texto demais\n",
    "\n",
    "for doc_id in doc_ids[:]:  # mostra apenas os 5 primeiros\n",
    "    doc = docstore._dict[doc_id]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Doc ID: {doc_id}\")\n",
    "    print(f\"Source: {doc.metadata.get('source')}\")\n",
    "    print(f\"File type: {doc.metadata.get('file_type')}\")\n",
    "    print(f\"Chunk ID: {doc.metadata.get('chunk_id')}\")\n",
    "    print(f\"Chunking: {doc.metadata.get('chunking')}\")\n",
    "    print(f\"Tamanho (caracteres): {len(doc.page_content)}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content[:])  # limita o texto exibido 600\n",
    "    print(\"...\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. INSPEÇÃO COMPLETA (USAR COM CUIDADO)\n",
    "# ------------------------------------------------------------\n",
    "# Se você realmente quiser ver TODOS os chunks:\n",
    "\n",
    "# for doc_id in doc_ids:\n",
    "#     doc = docstore._dict[doc_id]\n",
    "#     print(doc.page_content)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OBSERVAÇÃO PROFISSIONAL IMPORTANTE\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Boas práticas ao inspecionar chunks:\n",
    "\n",
    "✔ Sempre verifique os chunks ANTES de culpar o modelo\n",
    "✔ Confira se não há chunks vazios ou duplicados\n",
    "✔ Verifique tamanhos (<= 1000 chars)\n",
    "✔ Confirme metadados (source, chunk_id)\n",
    "\n",
    "90% dos problemas de RAG vêm de ingestão mal validada.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de2215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af303f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fa0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd7fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHUNK 0 | 128 caracteres\n",
      "--------------------------------------------------------------------------------\n",
      "RAG (Retrieval-Augmented Generation) é uma técnica que combina\n",
      "modelos de linguagem com mecanismos de recuperação de informação.\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1 | 105 caracteres\n",
      "--------------------------------------------------------------------------------\n",
      "Ela é amplamente usada para reduzir alucinações e melhorar\n",
      "respostas baseadas em conhecimento específico.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nQuando usar LLM-based chunking?\\n\\n✔ Textos longos e complexos\\n✔ Documentos jurídicos\\n✔ Manuais técnicos\\n✔ PDFs mal estruturados\\n\\nQuando NÃO usar?\\n\\n✘ Grandes volumes (milhares de documentos)\\n✘ Quando custo/latência importa mais que qualidade\\n\\nPadrão profissional:\\n- Chunking heurístico (RecursiveCharacterTextSplitter) para 80%\\n- LLM-based chunking apenas para documentos críticos\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chunking baseado em LLM (LLM-based Chunking)\n",
    "===========================================\n",
    "\n",
    "Objetivo:\n",
    "- Usar um LLM local (Ollama) para dividir texto em chunks SEMÂNTICOS\n",
    "- Garantir no máximo 1000 caracteres por chunk\n",
    "- Ideal para textos técnicos, jurídicos e científicos\n",
    "\n",
    "Observação importante:\n",
    "- Isso é MAIS caro computacionalmente\n",
    "- Porém gera chunks de MUITO maior qualidade semântica\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. IMPORTS\n",
    "# ============================================================\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. LLM LOCAL (OLLAMA)\n",
    "# ============================================================\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0.0,   # chunking deve ser determinístico\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. PROMPT DE CHUNKING SEMÂNTICO\n",
    "# ============================================================\n",
    "\n",
    "chunking_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Você é um especialista em organização de textos técnicos.\n",
    "\n",
    "Tarefa:\n",
    "- Divida o texto abaixo em CHUNKS SEMÂNTICOS.\n",
    "- Cada chunk deve conter NO MÁXIMO 1000 caracteres.\n",
    "- Nunca corte uma ideia no meio.\n",
    "- Preserve a ordem original.\n",
    "- Retorne os chunks separados pelo delimitador <CHUNK>.\n",
    "\n",
    "Texto:\n",
    "{text}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. PIPELINE DE CHUNKING COM LLM\n",
    "# ============================================================\n",
    "\n",
    "chunking_chain = (\n",
    "    chunking_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. FUNÇÃO DE CHUNKING LLM-BASED\n",
    "# ============================================================\n",
    "\n",
    "def llm_based_chunking(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Recebe um texto grande e retorna uma lista de chunks\n",
    "    semanticamente coerentes com até 1000 caracteres.\n",
    "    \"\"\"\n",
    "    response = chunking_chain.invoke({\"text\": text})\n",
    "\n",
    "    # Divide usando o delimitador definido no prompt\n",
    "    chunks = [\n",
    "        chunk.strip()\n",
    "        for chunk in response.split(\"<CHUNK>\")\n",
    "        if chunk.strip()\n",
    "    ]\n",
    "\n",
    "    # Garantia defensiva (fallback)\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > 1000:\n",
    "            raise ValueError(\n",
    "                \"Chunk maior que 1000 caracteres detectado. \"\n",
    "                \"Revise o prompt ou modelo.\"\n",
    "            )\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. EXEMPLO DE USO\n",
    "# ============================================================\n",
    "\n",
    "texto_exemplo = \"\"\"\n",
    "RAG (Retrieval-Augmented Generation) é uma técnica que combina\n",
    "modelos de linguagem com mecanismos de recuperação de informação.\n",
    "Ela é amplamente usada para reduzir alucinações e melhorar\n",
    "respostas baseadas em conhecimento específico.\n",
    "\"\"\"\n",
    "\n",
    "chunks = llm_based_chunking(texto_exemplo)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"CHUNK {i} | {len(chunk)} caracteres\")\n",
    "    print(\"-\" * 80)\n",
    "    print(chunk)\n",
    "    print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. OBSERVAÇÃO PROFISSIONAL\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Quando usar LLM-based chunking?\n",
    "\n",
    "✔ Textos longos e complexos\n",
    "✔ Documentos jurídicos\n",
    "✔ Manuais técnicos\n",
    "✔ PDFs mal estruturados\n",
    "\n",
    "Quando NÃO usar?\n",
    "\n",
    "✘ Grandes volumes (milhares de documentos)\n",
    "✘ Quando custo/latência importa mais que qualidade\n",
    "\n",
    "Padrão profissional:\n",
    "- Chunking heurístico (RecursiveCharacterTextSplitter) para 80%\n",
    "- LLM-based chunking apenas para documentos críticos\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74c42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a0228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
