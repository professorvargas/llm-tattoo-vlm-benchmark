{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a4556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2d6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Extraindo PDF com PyMuPDF4LLM...\n",
      "âœ… 19 chunks extraÃ­dos\n",
      "\n",
      "ðŸ§  Carregando modelo de embeddings...\n",
      "ðŸ§  Gerando embeddings dos chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa1b1c843db4cd9af4a7707cd23e6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Recuperando contexto relevante...\n",
      "\n",
      "â³ Consultando o modelo LLM local (Ollama)...\n",
      "\n",
      "ðŸ§  Resposta:\n",
      "\n",
      "The authors of this work are not explicitly named in the provided text. It only refers to \"the authors\" throughout the document.\n",
      "\n",
      "ðŸ“š Chunks recuperados:\n",
      "\n",
      "--- Chunk 1 | Similaridade: 0.273 ---\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**KKC Tattoos**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**UUC Tattoos**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|Animals|Insects|Plants|Architectural|Uncategorized|\n",
      "|---|---|---|---|---|\n",
      "|**Crocodile**|**Bee**|**Glass**|**Bridge**|**Iceberg**|\n",
      "|**Bear**|**Beetle**|**Beetle**|**Castle**|**Joker**|\n",
      "|**Cow/Bull**|**Ladybug**|**Ladybug**|**Ladybug**|**Robot**|\n",
      "|**Chicken**|**Chicken**|**Chicken**|**Chicken**|**Ship**|\n",
      "|**Elephant**|**Elephant**|**Elephant**|**Elephant**|**Ying-Yang**|\n",
      "|**Hippopotamus**|**Hippopotamus**|**Hippopotamus**|**Hippopotamus**|**Hippopotamus**|\n",
      "|**Horse**|**Horse**|**Horse**|**Horse**|**Hors\n",
      "\n",
      "--- Chunk 2 | Similaridade: 0.157 ---\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**Algorithm 1:** Class semantic augmentation.\n",
      "\n",
      "\n",
      "**Input:** Image ( _X_ )\n",
      "**Input:** Ground truth ( _Y_ )\n",
      "**Input:** List of transforms ( _t_ )\n",
      "**Input:** List of probability of choice ( _p_ )\n",
      "**Input:** List of ignore classes ( _ic_ )\n",
      "**Output:** Transformed image ( _X_ [Ë†] )\n",
      "1: _seg_ _ _ids_ = _unique_ ( _Y_ )\n",
      "2: _seg_ _ _ids_ = _remove_ _ _ic_ ( _seg_ _ _ids, ic_ )\n",
      "3: _X_ [Ë†] = _copy_ ( _X_ )\n",
      "4: **Foreach:** _i âˆˆ_ _seg_ _ _ids_ **do**\n",
      "5: _y_ = _new_ _ _array_ _ _zeros_ ( _Y.shape_ )\n",
      "6: _y_ [ _Y_ == _i_ ] = 1\n",
      "7: _T_ = _random_ _ _choice_ ( _t, p_ )\n",
      "8: _X_ Ë† = \n",
      "\n",
      "--- Chunk 3 | Similaridade: 0.150 ---\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 13.** Evaluation of the effect of data\n",
      "augmentation components. The results were\n",
      "obtained using proposed _LLMF CL_, SegFormer,\n",
      "and proposed G-OPenIPCS.\n",
      "\n",
      "\n",
      "\n",
      "**No Aug.** **Geometric** **AdjustmentsImage** **Dropout** **(Ours)CSA** **mIoU** _UUC_ **IoU**\n",
      "âœ“ 0.4556 _Â±_ .186 0.1547\n",
      "âœ“ 0.4320 _Â±_ .182 0.1797\n",
      "âœ“ 0.4814 _Â±_ .203 0.1884\n",
      "âœ“ **0.4959** _Â±_ .189 0.1538\n",
      "âœ“ 0.4576 _Â±_ .203 0.1770\n",
      "âœ“ âœ“ âœ“ 0.4733 _Â±_ .196 0.2303\n",
      "âœ“ âœ“ âœ“ âœ“ 0.4900 _Â±_ .201 **0.2753**\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 14.** Evaluation of the performance on the DeMSI dataset in the\n",
      "closed-set tattoo segmentation scenario.\n",
      "\n",
      "--- Chunk 4 | Similaridade: 0.150 ---\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 7.** Comparison of the closed-set segmentation results\n",
      "achieved per each loss function. Bold values indicate the best\n",
      "overall results, including all loss functions. The results were\n",
      "obtained using SegFormer.\n",
      "\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **AUROC** **Macro F1** **mIoU**\n",
      "\n",
      " - _LCE_ .7966 _Â±_ .122 .6179 _Â±_ .224 .4805 _Â±_ .216\n",
      "\n",
      "[51] _LF CL_ .8024 _Â±_ .111 .6433 _Â±_ .213 .5053 _Â±_ .209\n",
      "\n",
      "[65] _LDCE_ .8030 _Â±_ .118 .6108 _Â±_ .219 .4724 _Â±_ .218\n",
      "\n",
      "[66] _LLDAM_ **.8295** _Â±_ .105 .6497 _Â±_ .181 .5057 _Â±_ .191\n",
      "\n",
      "[20] _LLMCE_ .8110 _Â±_ .112 .6457 _Â±_ .217 **.5097** _Â±_ .216\n",
      "\n",
      "\n",
      "--- Chunk 5 | Similaridade: 0.132 ---\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 12.** Comparison\n",
      "between the proposed\n",
      "approach and state-of-the-art\n",
      "open-set semantic\n",
      "segmentation techniques.\n",
      "Bold values indicate the best\n",
      "overall results, including all\n",
      "methods.\n",
      "\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **Network** **Open SetClassifer** **AUROC** **Macro F1** **mIoU** _UUC_ **IoU**\n",
      "\n",
      "Baseline _LCE_ SegFormer SoftMax-T .7855 _Â±_ .126 .5782 _Â±_ .229 .4398 _Â±_ .216 .1350\n",
      "Baseline _LCE_ SegFormer OpenMax .7785 _Â±_ .123 .5781 _Â±_ .227 .4389 _Â±_ .212 .1611\n",
      "Baseline _LCE_ Swin+UPerNet SoftMax-T .7234 _Â±_ .095 .4852 _Â±_ .198 .3428 _Â±_ .188 .1145\n",
      "Baseline _LCE_ Sw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PDF RAG QA â€” Jupyter Notebook (single cell version)\n",
    "\n",
    "Este script implementa um pipeline completo de RAG (Retrieval-Augmented Generation)\n",
    "para PDFs cientÃ­ficos usando:\n",
    "\n",
    "- PyMuPDF4LLM para extraÃ§Ã£o e chunking por pÃ¡gina\n",
    "- SentenceTransformers para embeddings semÃ¢nticos\n",
    "- Similaridade por cosseno (via dot product normalizado)\n",
    "- Ollama como LLM local (ex: gemma3:4b)\n",
    "\n",
    "Projetado para rodar em uma ÃšNICA cÃ©lula de Jupyter Notebook.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "\n",
    "import pymupdf4llm\n",
    "import requests\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ConfiguraÃ§Ã£o do Sistema\n",
    "# =========================\n",
    "\n",
    "# Endpoint local do Ollama\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Modelo LLM local\n",
    "MODEL_NAME = \"gemma3:4b\"\n",
    "\n",
    "# Modelo de embeddings (rÃ¡pido e robusto para artigos cientÃ­ficos)\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Quantidade de chunks recuperados no RAG\n",
    "TOP_K_MATCHES = 5\n",
    "\n",
    "# Caminho do PDF (ajuste conforme necessÃ¡rio)\n",
    "PDF_PATH = \"datasets/tattoo.pdf\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FunÃ§Ãµes Auxiliares\n",
    "# =========================\n",
    "\n",
    "def get_embedder():\n",
    "    \"\"\"\n",
    "    Carrega o modelo de embeddings.\n",
    "    Executar apenas uma vez no notebook.\n",
    "    \"\"\"\n",
    "    return SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "\n",
    "def embed_texts(texts, embedder):\n",
    "    \"\"\"\n",
    "    Gera embeddings normalizados (norma L2 = 1),\n",
    "    permitindo o uso de produto interno como similaridade cosseno.\n",
    "    \"\"\"\n",
    "    embeddings = embedder.encode(\n",
    "        texts,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def chunk_markdown(md_chunks):\n",
    "    \"\"\"\n",
    "    Normaliza a saÃ­da do PyMuPDF4LLM para uma lista de strings.\n",
    "    Cada elemento corresponde a um chunk textual.\n",
    "    \"\"\"\n",
    "    if isinstance(md_chunks, list):\n",
    "        return [\n",
    "            chunk if isinstance(chunk, str)\n",
    "            else chunk.get(\"text\", str(chunk))\n",
    "            for chunk in md_chunks\n",
    "        ]\n",
    "    return [md_chunks]\n",
    "\n",
    "\n",
    "def retrieve(query, chunks, chunk_embeddings, embedder, k=TOP_K_MATCHES):\n",
    "    \"\"\"\n",
    "    Recupera os k chunks mais similares Ã  query usando similaridade cosseno.\n",
    "    \"\"\"\n",
    "    # Embedding da query\n",
    "    query_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "\n",
    "    # Similaridade cosseno (dot product)\n",
    "    similarities = np.dot(chunk_embeddings, query_vec.T).squeeze()\n",
    "\n",
    "    # Ãndices dos melhores chunks\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "    return [(chunks[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "\n",
    "def call_ollama(prompt):\n",
    "    \"\"\"\n",
    "    Envia o prompt para o Ollama e retorna a resposta do modelo.\n",
    "    Streaming desativado para simplicidade no notebook.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        OLLAMA_BASE_URL,\n",
    "        json=payload,\n",
    "        timeout=120\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"response\", \"\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pipeline Principal\n",
    "# =========================\n",
    "\n",
    "# 1. ExtraÃ§Ã£o e chunking do PDF\n",
    "print(\"ðŸ“„ Extraindo PDF com PyMuPDF4LLM...\")\n",
    "md_chunks = pymupdf4llm.to_markdown(PDF_PATH, page_chunks=True)\n",
    "chunks = chunk_markdown(md_chunks)\n",
    "print(f\"âœ… {len(chunks)} chunks extraÃ­dos\")\n",
    "\n",
    "# 2. GeraÃ§Ã£o de embeddings (feito apenas uma vez)\n",
    "print(\"\\nðŸ§  Carregando modelo de embeddings...\")\n",
    "embedder = get_embedder()\n",
    "\n",
    "print(\"ðŸ§  Gerando embeddings dos chunks...\")\n",
    "chunk_embeddings = embed_texts(chunks, embedder)\n",
    "\n",
    "# 3. Defina a pergunta (edite esta variÃ¡vel e reexecute a cÃ©lula)\n",
    "query = \"What are the authors of this work?\"\n",
    "\n",
    "# 4. RecuperaÃ§Ã£o semÃ¢ntica\n",
    "print(\"\\nðŸ”Ž Recuperando contexto relevante...\")\n",
    "top_chunks = retrieve(query, chunks, chunk_embeddings, embedder)\n",
    "\n",
    "# 5. ConstruÃ§Ã£o do contexto para o LLM\n",
    "context = \"\\n\\n\".join(chunk for chunk, _ in top_chunks)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the following question using ONLY the context below.\n",
    "If the answer is not in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 6. Chamada ao LLM\n",
    "print(\"\\nâ³ Consultando o modelo LLM local (Ollama)...\\n\")\n",
    "answer = call_ollama(prompt)\n",
    "\n",
    "# 7. ExibiÃ§Ã£o da resposta\n",
    "print(\"ðŸ§  Resposta:\\n\")\n",
    "print(answer)\n",
    "\n",
    "# 8. InspeÃ§Ã£o dos chunks recuperados (debug / avaliaÃ§Ã£o)\n",
    "print(\"\\nðŸ“š Chunks recuperados:\\n\")\n",
    "\n",
    "for i, (chunk, score) in enumerate(top_chunks, 1):\n",
    "    print(f\"--- Chunk {i} | Similaridade: {score:.3f} ---\")\n",
    "    print(chunk[:800])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476e06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n",
      "\n",
      "\n",
      "_Digital Object Identifier 10.1109/ACCESS.2017.DOI_\n",
      "\n",
      "# **Open-Set Tattoo Semantic Segmentation**\n",
      "\n",
      "\n",
      "1Federal University of Technology - ParanÃ¡, Av. Sete de Setembro, 3165, Curitiba, 80230-901, ParanÃ¡, Brazil.\n",
      "\n",
      "\n",
      "Corresponding author: Anderson Brilhador (e-mail: andersonbrilhador@gmail.com).\n",
      "\n",
      "\n",
      "**ABSTRACT** Tattoos can serve as an essential source of biometric information for public security, aiding\n",
      "in identifying suspects and victims. In order to automate tattoo classification, tasks like classification\n",
      "require more detailed image content analysis, such as semantic segmentation. However, a dataset with\n",
      "appropriate semantic segmentation annotations is currently lacking. Also, there are countless ways to\n",
      "categorize tattoo classes, and many are not directly categorizable, either because they belong to a specific\n",
      "artistic trait or characterize an object with previously undefined semantics. An effective way to overcome\n",
      "these limitations is to build recognition systems based on open-set assumptions. Nevertheless, state-ofthe-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to\n",
      "the significant class imbalance (predominant background). To the best of our knowledge, this paper is\n",
      "the first to explore semantic segmentation in closed and open-set scenarios for tattoos. In this sense, this\n",
      "paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set\n",
      "classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible\n",
      "test set, enabling comparisons and future research in this area. The proposed approach outperforms other\n",
      "methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU\n",
      "for the unknown class, demonstrating the feasibility of this approach for automatic tattoo analysis. The\n",
      "paper also highlights key limitations and open research areas in this challenging field. Dataset and codes\n",
      "[are available at https://github.com/Brilhador/tssd2023.](https://github.com/Brilhador/tssd2023)\n",
      "\n",
      "\n",
      "**INDEX TERMS** open-world, open-set, semantic segmentation, large-margin learning, tattoo classification.\n",
      "\n",
      "\n",
      "\n",
      "**I. INTRODUCTION**\n",
      "Tattoos are forms of human expression and are also considered an art. In their almost unique features, tattoos\n",
      "go beyond artistic expressions and can serve as essential\n",
      "sources of biometric information. Consequently, it can be\n",
      "useful in identifying their bearers, mainly for public security\n",
      "\n",
      "[1], [2] because tattoos can be used to identify not only\n",
      "suspects but also victims [3], [4]. In addition, the subject\n",
      "has raised studies on ethical and social issues that may\n",
      "encompass the topic [5].\n",
      "Compared to other biometrics, tattoos bring a series of\n",
      "characteristics that make them very difficult to recognize.\n",
      "Other biometrics usually have well-defined standards, robust\n",
      "techniques, well-established methods for their treatment and\n",
      "recognition, standardized data capture and storage, and other\n",
      "factors that help their reliability and robustness. However,\n",
      "tattoos still need to have such characteristics and requirements. Apart from the issues related to processing and\n",
      "using general biometrics, tattoo recognition has a singular\n",
      "\n",
      "\n",
      "\n",
      "complexity because it can be divided into several subproblems, each equally significant [6].\n",
      "First, an image can be submitted to detect, locate, and\n",
      "segment (outline or instance) the tattoo contained therein.\n",
      "Subsequently, the image can be classified, de-identified,\n",
      "or re-identified (image-to-image, sketch-to-image, partial,\n",
      "or similar). After preprocessing an image, the best result\n",
      "could be a well-segmented tattoo without any pollution or\n",
      "background. Then, for all classification, de-identification, or\n",
      "re-identification tasks, these images contain only the most\n",
      "essential information to store and, later, process [6].\n",
      "Nonetheless, tasks that consider the meaning of the\n",
      "content of images, such as classification, may require a\n",
      "more detailed separation of objects in a tattoo image, called\n",
      "semantic segmentation [7]. At this point, a segmented tattoo\n",
      "could identify and detach each object in the tattoo, after\n",
      "which each could be analyzed separately. For instance,\n",
      "security and biometric recognition systems could benefit\n",
      "from tattoo semantic split at the pixel-level for fine-grained\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 1\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "feature extraction by semantic classes, reduce false positives by precisely delineating the boundaries of biometrics\n",
      "features, and allow selective anonymization of regions of an\n",
      "image. Moreover, tattoo biometrics systems may require the\n",
      "identification of multiple semantic classes in multiple areas\n",
      "of the image, which can be performed accurately by tattoo\n",
      "semantic segmentations.\n",
      "Although this topic has been widely explored in studies\n",
      "related to images and videos in many different areas, semantic segmentation is still underexplored in the context\n",
      "of tattoos. Several works only focus on tattoo classification\n",
      "and detection [6], [8], and only a few researches deal with\n",
      "segmentation [9], [10] without identifying the semantics of\n",
      "the components that compose the tattoos.\n",
      "One of the reasons that make semantic segmentation difficult is related to the complexity that tattooing can have. As\n",
      "mentioned, tattoos are expressions of art, and their features\n",
      "can be as varied as possible and imaginable. In this way, objects can be positioned very closely, mixed, overlapped, and\n",
      "distorted, and abstract images can also be present, among\n",
      "many other hindering factors [11]. Additionally, the lack\n",
      "of public and comprehensive datasets makes it even more\n",
      "challenging to develop efficient methods for segmentation.\n",
      "Some recent works propose public datasets, such as [12]\n",
      "and [13]. However, in the case of [13], using semi-synthetic\n",
      "images without employing the semantics associated with\n",
      "each class makes it difficult to generalize the proposed â€“\n",
      "characteristic also observed in [12].\n",
      "Still, in this context, it is essential to emphasize the\n",
      "complexity and semantic variability of tattoo classes. Tattoo\n",
      "categories can vary significantly, and some are not easily categorized due to their association with specific artistic styles\n",
      "or objects with undefined semantics. The tattoo recognition\n",
      "scenario, especially from a public safety perspective, is also\n",
      "somewhat challenging, especially given the circumstances\n",
      "in which the information is obtained and analyzed. It is not\n",
      "uncommon for tattoo information to be obtained partially,\n",
      "and, therefore, semantic segmentation has great relevance\n",
      "in the identification process, as in the following scenarios:\n",
      "(i) semantic segmentation can be used to create databases\n",
      "with automatic textual annotations, as it is common for a\n",
      "witness or victim to remember or have visual contact with\n",
      "only parts of a tattoo of a wanted person, and, in this way,\n",
      "from the description, it would be possible to identify tattoos\n",
      "with those parts visualized; (ii) semantic segmentation is\n",
      "important in the pre-processing of images in preparation\n",
      "for information recognition processes, such as for partial\n",
      "re-identification of tattoos, wherein an automatic process,\n",
      "the segmented parts can be recovered separately in cases of\n",
      "partial image collections [6].\n",
      "An effective way to overcome these limitations is to\n",
      "build recognition systems based on dynamic and open-set\n",
      "perception. These systems are designed to handle objects\n",
      "from unknown classes commonly encountered in real-world\n",
      "applications. Open-set recognition has extensively studied\n",
      "the ability to recognize new classes [14]. Open-set semantic\n",
      "\n",
      "\n",
      "\n",
      "segmentation, in turn, is an approach that incorporates openset perception into semantic segmentation. The main difference with closed-set semantic segmentation is that openset semantic segmentation must correctly classify samples\n",
      "belonging to known classes while rejecting those belonging\n",
      "to unknown classes. In the context of this work, open-set\n",
      "semantic segmentation can be an ally in improving databases\n",
      "and models for identifying and classifying tattoo objects,\n",
      "allowing the improvement of annotations and descriptions\n",
      "of complex tattoos. Therefore, semantic segmentation must\n",
      "also be seen as a middle process, not just as an end process\n",
      "in the tattoo recognition roadmap.\n",
      "Studies have explored the use of open-set semantic segmentation in different applications [15], [16]. These studies\n",
      "focus on adapting or building open-set classifiers to make\n",
      "closed-set semantic segmentation models capable of recognizing unknown classes. While the outcomes of these studies\n",
      "are promising, the performance of these approaches is limited due to the low representation of the obtained features,\n",
      "resulting in an â€œirregularâ€ logit space with low discrimination among the classes. Recent research [17], [18] has\n",
      "demonstrated that incorporating metric learning techniques\n",
      "can enhance open-set recognition. Metric learning aids in\n",
      "obtaining more discriminative features and building a logit\n",
      "space that tightly clusters known classes while maintaining\n",
      "a considerable distance from unknown classes. However, it\n",
      "is essential to acknowledge that applying metric learning\n",
      "in the context of semantic segmentation can be impractical.\n",
      "This is primarily due to the exponential complexity of the\n",
      "task, as calculating pairwise distances among logit vectors\n",
      "of pixels becomes computationally expensive.\n",
      "Recent studies have investigated the potential of largemargin learning to acquire more discriminative features, yielding improved outcomes in image classification\n",
      "tasks [19]â€“[21]. Hence, strategies established on largemargin learning present promising and viable alternatives\n",
      "for building a well-defined logit space that enhances the\n",
      "separation among decision boundaries of semantic classes.\n",
      "Motivated by these results, our study explores the discriminative capabilities of large-margin learning to produce\n",
      "more distinctive features for tattoo semantic segmentation.\n",
      "This approach effectively increases the spatial separation\n",
      "among decision boundaries to different semantic classes,\n",
      "forcing the build of ideal logit space as illustrated in\n",
      "Figure 1b. Furthermore, this expanded separation among\n",
      "decision boundaries will set the stage for accommodating\n",
      "unknown classes in the future, enhancing the effectiveness\n",
      "of tattoo semantic segmentation within an open-set scenario.\n",
      "Visual comparisons depicting the differences in logit space\n",
      "resulting from the presence of unknown classes can be\n",
      "observed in Figure 1.\n",
      "Given the limitations presented so far and the fact that,\n",
      "to the extent of our knowledge, open-set classification has\n",
      "not been used in the context of tattoo recognition and is\n",
      "an open research gap, this work aims to propose a new\n",
      "large-margin-based loss function adapted to the context of\n",
      "\n",
      "\n",
      "\n",
      "2 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "segmentation in an open-set context, whose high complexity\n",
      "will serve as a benchmark for comparing methods in this\n",
      "area. The main contributions of this paper are then summarized as follows:\n",
      "\n",
      "_â€¢_ Test set publicly available for the dataset, allowing\n",
      "comparisons and future work in the open-set and\n",
      "closed-set scenarios;\n",
      "\n",
      "_â€¢_ Novel class semantic augmentation method to expand\n",
      "the tattoo samples;\n",
      "\n",
      "_â€¢_ Novel large-margin loss function for open-set tattoo\n",
      "semantic segmentation to build more discriminative\n",
      "features and handle the class imbalance;\n",
      "\n",
      "_â€¢_ A generalized open-set classifier approach based on\n",
      "open principal component scoring with incremental\n",
      "learning called G-OpenIPCS;\n",
      "\n",
      "_â€¢_ Detailed and in-depth comparison with different stateof-the-art loss functions and open-set semantic segmentation methods;\n",
      "\n",
      "_â€¢_ Statement of the main challenges for the open-set tattoo\n",
      "semantic segmentation.\n",
      "This paper is organized as follows. Section II presents\n",
      "related works, mainly including related datasets, tattoo\n",
      "segmentation, and open-set semantic segmentation. Our\n",
      "proposed methods, particularly the novel tattoo semantic\n",
      "segmentation dataset, novel tattoo semantic augmentation\n",
      "method, and the novel loss function and open set classifier,\n",
      "are detailed in Section III. The experiment setup is discussed\n",
      "in Section IV, with results, discussions, and comparisons\n",
      "with state-of-the-art approaches presented in Section V.\n",
      "\n",
      "\n",
      "\n",
      "art in the context of tattoo segmentation, we divided the\n",
      "related works into three parts where this paper presents\n",
      "main contributions: datasets, tattoo segmentation, and open\n",
      "set semantic segmentation. The following subsections detail\n",
      "each of these works, pointing out the innovative aspects of\n",
      "this work on each front.\n",
      "\n",
      "\n",
      "_**A. AVAILABLE DATASETS**_\n",
      "\n",
      "Regarding the datasets, we chose to organize in the Table 1\n",
      "a summary of the main characteristics of the most common\n",
      "datasets in tattoo detection, classification, and segmentation\n",
      "problems. These criteria include the number of samples,\n",
      "public availability, and type of annotation (classification,\n",
      "detection of objects with bounding box â€“ BB, or segmentation). Regarding the annotation focused on segmentation,\n",
      "we also included whether it contains semantic segmentation.\n",
      "\n",
      "\n",
      "**TABLE 1.** Tattoo datasets.\n",
      "\n",
      "\n",
      "**Ref.** **Total** **Public?** **Annot.** **Semantic?**\n",
      "\n",
      "[12] 7,526 No BB     \n",
      "[9] 890 Yes Seg. No\n",
      "\n",
      "[22] 5,740 Yes Class.     \n",
      "[23] 5,000 Yes BB     \n",
      "[24] 210 Yes Class.     \n",
      "[13] 5,500 Yes Seg. No\n",
      "Ours 2,106 315 Seg. Yes\n",
      "\n",
      "\n",
      "In the case of [12], despite the relatively large number of\n",
      "samples and an annotation focused on tattoo detection, the\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 3\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "dataset is private, which makes comparisons, establishing\n",
      "benchmarks, and analyzing new methods for the dataset\n",
      "difficult. On the other hand, the dataset proposed in [23]\n",
      "is publicly available and annotated only with BB, i.e., it\n",
      "is not possible to use it in the context of segmentation.\n",
      "The datasets proposed in [24] and [22], in turn, are focused\n",
      "exclusively on classification and provide only one label for\n",
      "each image or image patch, restricting their use to multiclass classification problems, without the location of the\n",
      "tattoo in the image.\n",
      "\n",
      "[9] were the first to address tattoo segmentation. The\n",
      "proposed method aimed to de-identify soft biometric identifiers (tattoos) by discriminating tattoo and non-tattoo image\n",
      "patches with a deep neural network. In this sense, the\n",
      "proposed dataset presents a pixel-level annotation of the\n",
      "presence or absence of a tattoo in the image. However,\n",
      "the authors did not individualize the tattoo classes in the\n",
      "proposed annotation. The same is observed in the dataset\n",
      "presented in [13]. Furthermore, the authors presented a\n",
      "proposal using semi-synthetic images. This characteristic\n",
      "can sometimes lead to an image far from a real tattoo,\n",
      "compromising the segmentation approach.\n",
      "Our dataset, in turn, presents some original and innovative\n",
      "features that can complement currently available datasets:\n",
      "(i) Inclusion of a semantic segmentation annotation; (ii)\n",
      "Several classes of tattoos in the same image, increasing\n",
      "complexity; (iii) Quite varied sizes of tattoos and classes\n",
      "of tattoos in the same image; and (iv) Tattoos in different\n",
      "regions of the body, maintaining the variability that exists in\n",
      "real situations. As will be detailed later, only the test set is\n",
      "made publicly available since most of the images available\n",
      "in these scenarios contain public use restrictions. However,\n",
      "we believe that as it is the first dataset with semantic\n",
      "annotation in segmentation, it will allow the comparison and\n",
      "evaluation of different approaches to this problem, which is\n",
      "significantly challenging.\n",
      "\n",
      "\n",
      "_**B. TATTOO SEGMENTATION**_\n",
      "Tattoo segmentation methods were presented in many studies, but their results were suppressed, maybe because segmentation was not the main focus. Furthermore, as many\n",
      "of them were carried out a long time ago, the research\n",
      "did not address deep learning methods, for example, and\n",
      "the methods cannot be directly compared with the approach\n",
      "adopted here.\n",
      "In such cases, authors have performed their researches\n",
      "using methods based on: (i) Content-Base Image Retrieval\n",
      "(CBIR) and Edge Direction Coherence Vector (EDCV) [25];\n",
      "(ii) 3 _Ã—_ 3 Sobel filter [26], Active Contour CBIR (ACCBIR),\n",
      "and Vector Field Convolution (VFC) [1]; (iii) a complex\n",
      "system combining bottom-up and top-down priorities that\n",
      "transfer tattoo segmentation to detection split-merge skin\n",
      "detection, followed by figure-ground tattoo segmentation\n",
      "\n",
      "[27]; (iv) LoG (Laplacian of Gaussian) and Sobel kernel\n",
      "filters called quasi-connected components (QCC), using the\n",
      "GrabCut algorithm to produce the final segmented tattoo\n",
      "\n",
      "\n",
      "\n",
      "image [28]; (v) a negative image method with HSV (hue,\n",
      "saturation, and value, or lighting) model [29]; (vi) identification of pixels of skin in regions close to the tattoos and a\n",
      "graph-cut model based on skin color and a visual bump map\n",
      "\n",
      "[30], and (vii) a _k_ -means cluster used in LAB color space to\n",
      "detect the skin area with a morphology processing used to\n",
      "smooth the clear graphic of the tattoo image segment [10].\n",
      "The main limitation of these hand-crafted-based methods is\n",
      "that the feature extractor may have adequate performance\n",
      "for some classes and datasets but significantly lower performance for others, with compromised generalization. This\n",
      "is accentuated for datasets with greater variability in tattoo\n",
      "images.\n",
      "As far as our research has reached, only two studies\n",
      "have used deep learning methods for the problem of tattoo\n",
      "segmentation.\n",
      "\n",
      "\n",
      "**TABLE 2.** Tattoo segmentation models.\n",
      "\n",
      "\n",
      "**Ref.** **Network** **Semantic?** **Open-Set?**\n",
      "\n",
      "[9] ConvNet No No\n",
      "\n",
      "[31] AlexNet+VGG No No\n",
      "\n",
      "[13] ViT-based No No\n",
      "Ours SegFormer Yes Yes\n",
      "\n",
      "\n",
      "Based on the study on CNNs, [9] used the structure\n",
      "of a ConvNet network to train small pieces of images to\n",
      "learn to identify which ones have or do not have pieces of\n",
      "tattoos. After training the network, a sliding window was\n",
      "passed through the image to be tested, and each segment of\n",
      "the sliding window was tested as a piece with a tattoo or\n",
      "not, marking the positive pieces. Parts with possible tattoos\n",
      "would be segmented piece by piece at the end of the slide.\n",
      "\n",
      "[31] proposed a continuation of work presented in\n",
      "\n",
      "[9], this time testing three different networks for tattoo\n",
      "segmentation: (i) an architecture consisting only of multiple\n",
      "fully connected layers, without convolutional layers; (ii)\n",
      "an architecture inspired by the AlexNet network; and (iii)\n",
      "an architecture inspired by the VGGNet network. On the\n",
      "other hand, state-of-the-art segmentation models based on\n",
      "Vision Transformer (ViT) were evaluated in [13]; however,\n",
      "the main idea of that work was the unsupervised tattoo\n",
      "generator that allowed the creation of many semi-synthetic\n",
      "images with tattooed subjects. Hence, as shown in Table 2,\n",
      "related approaches used still needed to follow a semantic\n",
      "segmentation methodology and did not use an open-set\n",
      "semantic segmentation view, which is the focus of our\n",
      "current study and detailed as follows.\n",
      "\n",
      "\n",
      "_**C. OPEN-SET SEMANTIC SEGMENTATION**_\n",
      "The success of the fully convolutional network (FCN) in\n",
      "closed-set semantic segmentation [32] has led to the successful implementation of various neural network models\n",
      "for closed-set semantic segmentation on different applications [7]. However, these methods are unsuitable for openset scenarios, which are common in real-world computer\n",
      "vision. This is because the closed-set perception fails when\n",
      "\n",
      "\n",
      "\n",
      "4 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "unknown classes from training are found in the test phase\n",
      "\n",
      "[33]. Due to that, several proposals have been developed\n",
      "for the open-set context in different applications, mainly autonomous driving, remote sensing, and data collection [34].\n",
      "For open-set semantic segmentation, the loss function\n",
      "selection to guide the optimization process is an important\n",
      "aspect of the achieved results [35]. In general, studies limit\n",
      "themselves to using the successful and widely employed\n",
      "cross-entropy loss (CE) [15], [16], [33], [36]â€“[40]. This\n",
      "loss function measures the disparity between the predicted\n",
      "values and the ground truth, guiding the modelâ€™s learning\n",
      "process based on labeled data. Hence, the studies focus on\n",
      "adapting or building open-set classifiers to make closedset semantic segmentation models capable of recognizing\n",
      "unknown classes, even in label noise scenarios [41].\n",
      "In this sense, metric learning has recently gained significant attention in addressing the open-set problem. Metric\n",
      "learning is an approach based on learning a distance metric that reinforces similarity between objects in the latent\n",
      "space. It has been studied in various fields, such as image\n",
      "classification [14], semantic segmentation [42], [43], and\n",
      "zero-shot segmentation [44]. According to [42], the metric\n",
      "learning strategy aims to direct the feature extraction process\n",
      "to obtain a well-controlled latent space, maximizing interclass spacing and minimizing intra-class spacing based on\n",
      "a distance metric. Thus, the unknown samples are repelled\n",
      "into open space, as can be seen in Figure 4.\n",
      "Similarly, the large-margin-based loss (LM) functions\n",
      "\n",
      "[19]â€“[21] maximize the margins between classes by imposing a regularization on the logit vectors of pixels to induce\n",
      "an increased separation between the boundary regions of\n",
      "the semantic classes. This strategy improves the generalization of models and provides open space between classes\n",
      "that can be valuable in the context of open-set semantic\n",
      "segmentation, allowing unknown samples to be projected\n",
      "into these open spaces. Furthermore, the large-margin loss\n",
      "adopts a more efficient training strategy than other metric\n",
      "learning approaches, such as those involving cubic costs for\n",
      "computing pairwise distances between the logit vectors of\n",
      "pixels [45].\n",
      "Recent studies [40], [46] have made improved open-set\n",
      "semantic segmentation for autonomous driving applications\n",
      "by employing negative auxiliary data. Unlike these studies, our approach uses no auxiliary data to enhance the\n",
      "performance of open-set semantic segmentation. Table 3\n",
      "summarizes the approaches used for open-set semantic\n",
      "segmentation.\n",
      "In the context of tattoo semantic segmentation, as stated\n",
      "in the previous Section, to the extent of our knowledge,\n",
      "no works focus on the open set contextâ€”the most closely\n",
      "related works are [23] and [47]. [23] built a tattoo search\n",
      "approach that can learn tattoo detection and compact representation jointly in a single CNN via multi-task learning\n",
      "is presented. However, the compactness proposed by the\n",
      "authors is more focused on the compressive yet discriminative feature learning for large-scale visual search and\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 3.** Open-set semantic segmentation approaches.\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **Context** **Aux. data?**\n",
      "\n",
      "[15], [16], [36], [38], [39] CE Remote Sensing No\n",
      "\n",
      "[33], [37] CE General No\n",
      "\n",
      "[33] CE Synthetic Data No\n",
      "\n",
      "[40], [46] CE Aut. Driving Yes\n",
      "\n",
      "[42] Metric Aut. Driving No\n",
      "\n",
      "[44] Metric General No\n",
      "\n",
      "[43] Metric General No\n",
      "Ours LM Tattoo No\n",
      "\n",
      "\n",
      "instance retrieval applications, i.e., the efficiency of the\n",
      "search procedure. Open-set classification is not presented,\n",
      "and discussions of the proposed multi-task procedure are\n",
      "not encouraged for other applications. While [47] presented\n",
      "a classification method based on the Extreme Value Theory\n",
      "for tattoo classification. However, the focus of the proposed\n",
      "approach was the mid-level representations as a tool to\n",
      "adjust the trade-off between accuracy and efficiency. Hence,\n",
      "the results are mainly dedicated to real-world computer\n",
      "vision systems, where high accuracy is maintained even\n",
      "on commodity hardware with a low computational budget.\n",
      "Details regarding open-set are also not addressed.\n",
      "To the best of our knowledge, this is the first work to\n",
      "explore tattoo semantic segmentation in closed and open-set\n",
      "scenarios, establishing benchmarks for both conditions using\n",
      "our publicly available test set. Our approach introduces the\n",
      "large-margin loss function as an efficient learning strategy to\n",
      "build a well-defined logit space and handle class imbalance,\n",
      "using contemporary network architecture based on transformers and presenting the generalist approach to integrating\n",
      "a robust open-set classifier for semantic segmentation tasks.\n",
      "\n",
      "\n",
      "**III. PROPOSED METHODS**\n",
      "This section outlines the proposed methods. Firstly, in\n",
      "Section III-A, a novel-built TSSD2023 dataset is presented.\n",
      "Then, Section III-B introduces a novel tattoo semantic\n",
      "augmentation to expand tattoo samples of the TSSD2023\n",
      "dataset. Subsequently, in Section III-C, a novel large-margin\n",
      "loss function is proposed to handle class imbalance and\n",
      "enhance the discriminative of the classes in the TSSD2023\n",
      "dataset. Lastly, in Section III-D, a generalized approach for\n",
      "OpenIPCS is proposed for open-set semantic segmentation.\n",
      "\n",
      "\n",
      "_**A. TATTOO SEMANTIC SEGMENTATION DATASET**_\n",
      "Identifying the various things that make up a tattoo can be\n",
      "defined as a semantic segmentation task. This process allows\n",
      "machines to comprehend the meaning behind a tattoo better.\n",
      "Its usefulness is particularly evident in security systems,\n",
      "where it assists in identifying and monitoring individuals through surveillance systems, for instance, locating a\n",
      "person based on a brief tattoo description. However, as\n",
      "previously presented, current tattoo datasets are limited to\n",
      "image classification [22], [24], object detection [12], [23], or\n",
      "tattoo segmentation [9], [13], which only separate the tattoo\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 5\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "from the background and fail to provide a comprehensive\n",
      "understanding of the significance of the tattoos.\n",
      "In this manner, we created the Tattoo Semantic Segmentation Dataset (TSSD2023), respecting the copyrights of\n",
      "image owners to the greatest extent possible. To build this\n",
      "dataset, web scraping was conducted on Flickr [1] . Numerous\n",
      "terms related to tattoos were used as search queries on\n",
      "the platform. Then, a visual inspection was performed by\n",
      "humans to confirm whether the images obtained contained\n",
      "tattoos and whether their content was suitable for sharing.\n",
      "Finally, the licenses users attribute to these images when\n",
      "sharing them on the platform were considered to define the\n",
      "training, validation, and test sets.\n",
      "As a result, the test dataset exclusively comprises images\n",
      "for which sharing permissions had been granted. In contrast,\n",
      "the training and validation datasets consist only of images\n",
      "for which sharing was unauthorized. This division strategy\n",
      "was adopted due to the limited availability of images with\n",
      "public sharing licenses. Thus, the test sets will be publicly\n",
      "available for comparison and development of future work.\n",
      "However, the training and validation sets will be kept private\n",
      "to ensure that none of the authorsâ€™ copyrights are infringed.\n",
      "Figure 2 shows 33 classes [2] for Known Known Class\n",
      "(KKC) tattoos and 23 classes for Unknown Unknown Class\n",
      "(UUC) tattoos selected for annotation in TSSD2023 [3] . The\n",
      "motivation for choosing this split for KKC and UUC classes\n",
      "was mainly based on the number of images available for\n",
      "each semantic class. Classes with reduced representation in\n",
      "the universe of available tattoos and, consequently, public\n",
      "samples were selected exclusively to compose the UUCs due\n",
      "to the impossibility of successfully training the segmentation\n",
      "models on these classes. Furthermore, the proposed division\n",
      "guarantees that the test semantic classes defined as UUC\n",
      "are not found on the training and validation sets. Hence,\n",
      "we consider the proposed division sufficient to evaluate the\n",
      "open-set methods proposed in this work. Once the data were\n",
      "divided, UUCs were chosen to form part of the test set for\n",
      "open-set evaluation. This approach enabled the representation of similar and dissimilar semantic classes compared\n",
      "to KKCs, allowing for evaluation in straightforward and\n",
      "complex scenarios.\n",
      "Each KKC class was meticulously labeled with unique\n",
      "identifiers to enable the model to distinguish each object\n",
      "semantically. In contrast, all UUC tattoos were assigned the\n",
      "same â€œunknownâ€ class label. Notably, all KKC classes are\n",
      "represented in the training and validation sets and the test\n",
      "\n",
      "\n",
      "1A photo and video hosting platform established in 2004. Available\n",
      "[in: https://flickr.com/](https://flickr.com/)\n",
      "\n",
      "2The dataset also includes annotations for the â€˜stem/branchâ€™ and â€˜ropeâ€™\n",
      "classes, which were omitted from the analysis due to the limited number\n",
      "of annotations.\n",
      "3The closed-set approach assumes that the training and testing pixels\n",
      "belong to the same label space (defined as KKCs), meaning that the train\n",
      "and test sets contain the same classes. However, this assumption does not\n",
      "hold in real-world scenarios, especially in earth observation applications.\n",
      "During the prediction phase, the model may face pixels from classes not\n",
      "seen during the training phase (UUCs). We direct the reader to [14] for a\n",
      "deeper reading about these definitions.\n",
      "\n",
      "\n",
      "\n",
      "set. However, UUC tattoos are exclusively found in the test\n",
      "open-set. A human inspection process was also conducted\n",
      "to ensure these open-set recognition conditions [14].\n",
      "\n",
      "\n",
      "Table 4 presents four possible tattoo composition situations found in TSSD2023: (i) Single tattoo: only one\n",
      "tattoo class is present in the image; (ii) Multiple tattoos:\n",
      "multiple tattoo classes can be found in a single image;\n",
      "(iii) Multiple tattoos (with overlap): similar to the previous\n",
      "scenario, but the tattoos are overlapped with each other.\n",
      "Due to this overlap, this scenario is more challenging\n",
      "than the previous one [33]; and (iv) Tattoos (unlabeled)\n",
      "as background: Tattoos (unlabeled) as background: Specific\n",
      "tattoo classes were categorized as background classes due\n",
      "to two primary reasons. First, this decision was necessary\n",
      "because, at times, it was impossible to determine their specific semantic tattoo class. Second, due to a limited number\n",
      "of available samples, assigning individual semantic labels\n",
      "to these classes was not feasible. These situations provide a\n",
      "comprehensive representation of tattoo compositions within\n",
      "the dataset.\n",
      "\n",
      "\n",
      "**TABLE 4.** Samples of the images and annotations of the TSSD2023 dataset.\n",
      "\n",
      "\n",
      "Single\n",
      "tattoo\n",
      "\n",
      "\n",
      "Multiple\n",
      "tattoos\n",
      "\n",
      "\n",
      "Multiple\n",
      "tattoos\n",
      "(with overlap)\n",
      "\n",
      "\n",
      "Tattoos\n",
      "(unlabeled)\n",
      "as background\n",
      "\n",
      "\n",
      "In conclusion, TSSD2023 contains 2,106 tattoo images\n",
      "without specific image resolution standards that have been\n",
      "annotated at the pixel level. These images are divided into\n",
      "the following subsets: 1,404 for training, 387 for validation,\n",
      "and 254 for the closed-set test. Additionally, 61 images with\n",
      "UUC tattoos are included to form the test open-set with\n",
      "315 images. Figure 3 illustrates the distribution of pixel\n",
      "percentages for each class within TSSD2023, demonstrating\n",
      "that the dataset is notably imbalanced, particularly in the\n",
      "background class, which accounts for an average of _â‰ˆ_\n",
      "80% of pixels across the subsets. It is also important to\n",
      "highlight that the unknown class represents approximately\n",
      "3% of the pixels present in the test open-set. Samples\n",
      "of images from TSSD2023 can be observed in Table 4,\n",
      "showcasing tattoos presented in diverse scenarios, sizes,\n",
      "styles, positions, and combinations. These variations aim to\n",
      "maintain the variability of real-world situations.\n",
      "\n",
      "\n",
      "\n",
      "6 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**KKC Tattoos**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**UUC Tattoos**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|Animals|Insects|Plants|Architectural|Uncategorized|\n",
      "|---|---|---|---|---|\n",
      "|**Crocodile**|**Bee**|**Glass**|**Bridge**|**Iceberg**|\n",
      "|**Bear**|**Beetle**|**Beetle**|**Castle**|**Joker**|\n",
      "|**Cow/Bull**|**Ladybug**|**Ladybug**|**Ladybug**|**Robot**|\n",
      "|**Chicken**|**Chicken**|**Chicken**|**Chicken**|**Ship**|\n",
      "|**Elephant**|**Elephant**|**Elephant**|**Elephant**|**Ying-Yang**|\n",
      "|**Hippopotamus**|**Hippopotamus**|**Hippopotamus**|**Hippopotamus**|**Hippopotamus**|\n",
      "|**Horse**|**Horse**|**Horse**|**Horse**|**Horse**|\n",
      "|**Monkey**|**Monkey**|**Monkey**|**Monkey**|**Monkey**|\n",
      "|**Pinguim**|**Pinguim**|**Pinguim**|**Pinguim**|**Pinguim**|\n",
      "|**Pork**|**Pork**|**Pork**|**Pork**|**Pork**|\n",
      "|**Sheep**|**Sheep**|**Sheep**|**Sheep**|**Sheep**|\n",
      "|**Turtle**|**Turtle**|**Turtle**|**Turtle**|**Turtle**|\n",
      "\n",
      "\n",
      "|Objects|Animals|Insects|Plants|Elements|Weapons|Uncategorized|\n",
      "|---|---|---|---|---|---|---|\n",
      "|**Anchor**|**Bird**|**Butterfly**|**Flower**|**Fire**|**Gun**|**Background**|\n",
      "|**Crown**|**Cat**|**Scorpion**|**Leaf**|**Water**|**Knife/Sword**|**Heart**|\n",
      "|**Diamond**|**Dog**|**Spider**|**Spider**|**Spider**|**Shield**|**Mermaid**|\n",
      "|**Key**|**Eagle**|**Eagle**|**Eagle**|**Eagle**|**Eagle**|**Skull**|\n",
      "|**Ribbon**|**Fish**|**Fish**|**Fish**|**Fish**|**Fish**|**Star**|\n",
      "||**Fox**|**Fox**|**Fox**|**Fox**|**Fox**|**Fox**|\n",
      "||**Lion**|**Lion**|**Lion**|**Lion**|**Lion**|**Lion**|\n",
      "||**Octopus**|**Octopus**|**Octopus**|**Octopus**|**Octopus**|**Octopus**|\n",
      "||**Owl**|**Owl**|**Owl**|**Owl**|**Owl**|**Owl**|\n",
      "||**Shark**|**Shark**|**Shark**|**Shark**|**Shark**|**Shark**|\n",
      "||**Snake**|**Snake**|**Snake**|**Snake**|**Snake**|**Snake**|\n",
      "||**Tiger**|**Tiger**|**Tiger**|**Tiger**|**Tiger**|**Tiger**|\n",
      "||**Wolf**|**Wolf**|**Wolf**|**Wolf**|**Wolf**|**Wolf**|\n",
      "\n",
      "\n",
      "\n",
      "**FIGURE 2.** TSSD2023 Classes. KKCs tattoos correspond to the 33 labeled classes, while the UUCs tattoos are represented by the 23 classes defined as\n",
      "â€œunknownâ€ (black label). These classes can be viewed within a conceptual taxonomy, facilitating an understanding of the domain coverage provided by the dataset.\n",
      "\n",
      "\n",
      "**Test open-set** **Test closed-set** **Validation Set** **Training Set**\n",
      "\n",
      "\n",
      "5,00%\n",
      "\n",
      "\n",
      "4,00%\n",
      "\n",
      "\n",
      "3,00%\n",
      "\n",
      "\n",
      "2,00%\n",
      "\n",
      "\n",
      "1,00%\n",
      "\n",
      "\n",
      "0,00%\n",
      "\n",
      "\n",
      "**FIGURE 3.** The distribution of pixels per class and set. These values are organized based on their quantities in the training set. The values of background classes\n",
      "were suppressed due to discrepancies with the other classes. The reference values for the background are 79.8% in training, 79.4% validation images, 81.4% test\n",
      "closed-set images, and 81.7% test open-set images.\n",
      "\n",
      "\n",
      "\n",
      "_**B. CLASS SEMANTIC AUGMENTATION**_\n",
      "\n",
      "Motivated by the limited number of samples within\n",
      "TSSD2023 compared to the extensive diversity of real-world\n",
      "tattoos, this work introduces a novel data augmentation\n",
      "named class semantic augmentation (CSA) to increase the\n",
      "variety of tattoos contained in TSSD2023. This method applies pixel-level transformations to distinct classes, enabling\n",
      "unique augmentation for different classes within an image,\n",
      "as illustrated in Table 5. In this instance, the pixels of\n",
      "the heart class go through different transformations of colorations, gray styles, and color tones, trying to approximate\n",
      "the infinite possibilities of representing tattoos in the real\n",
      "world from limited semantic data.\n",
      "\n",
      "\n",
      "**TABLE 5.** Samples of the data augmentations on only the heart class, except\n",
      "the mix column that applies the data augmentations on all semantic classes.\n",
      "\n",
      "\n",
      "RGB Random\n",
      "Original To gray Mix\n",
      "shift tune\n",
      "\n",
      "\n",
      "Algorithm 1 details the proposed implementation of tattoo\n",
      "semantic augmentation. The algorithm takes five parameters:\n",
      "the input image _X_, the ground truth _Y_, a list of pixel\n",
      "\n",
      "\n",
      "level transformations to increase the variability of semantic\n",
      "classes _t_, a list containing the probabilities of executing\n",
      "each transformation _p_, and a list of semantic class indices\n",
      "_ic_ that remain unchanged during data augmentation. It is\n",
      "important to note that the parameters _p_ and _ic_ are optional.\n",
      "The algorithm first identifies the indices of semantic classes\n",
      "within the image. Subsequently, it iterates through each\n",
      "index, applying one of the transformations specified in _t_ .\n",
      "As shown in Table 5, a heart can be represented in various\n",
      "ways while other classes remain unchanged. It demonstrates\n",
      "that this semantic augmentation allows specific adjustments\n",
      "for each class in the dataset, making it useful in various\n",
      "application domains, such as autonomous driving [42] and\n",
      "fashion images [48]. In addition, different pixel-level transformations can be selected for more suitable application\n",
      "contexts. For instance, the color of a flower may vary. At the\n",
      "same time, the leaf and stem could be confined to modifying\n",
      "the original color tones, preserving the real-world patterns.\n",
      "On the other hand, all semantic classes in an image can be\n",
      "changed without restrictions, as observed in the mix column\n",
      "of Table 5 and as employed in this study. It is worth noting\n",
      "that this augmentation technique is limited to pixel-level\n",
      "transformations.\n",
      "\n",
      "\n",
      "_**C. PROPOSED LOSS FUNCTION**_\n",
      "\n",
      "Classical convolutional neural networks (CNNs) based semantic segmentation networks [32] can be divided into two\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 7\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**Algorithm 1:** Class semantic augmentation.\n",
      "\n",
      "\n",
      "**Input:** Image ( _X_ )\n",
      "**Input:** Ground truth ( _Y_ )\n",
      "**Input:** List of transforms ( _t_ )\n",
      "**Input:** List of probability of choice ( _p_ )\n",
      "**Input:** List of ignore classes ( _ic_ )\n",
      "**Output:** Transformed image ( _X_ [Ë†] )\n",
      "1: _seg_ _ _ids_ = _unique_ ( _Y_ )\n",
      "2: _seg_ _ _ids_ = _remove_ _ _ic_ ( _seg_ _ _ids, ic_ )\n",
      "3: _X_ [Ë†] = _copy_ ( _X_ )\n",
      "4: **Foreach:** _i âˆˆ_ _seg_ _ _ids_ **do**\n",
      "5: _y_ = _new_ _ _array_ _ _zeros_ ( _Y.shape_ )\n",
      "6: _y_ [ _Y_ == _i_ ] = 1\n",
      "7: _T_ = _random_ _ _choice_ ( _t, p_ )\n",
      "8: _X_ Ë† = _apply_ _ _transform_ ( _T,_ _X, y_ [Ë†] )\n",
      "9: **end foreach**\n",
      "10: **return** _X_ Ë†\n",
      "\n",
      "\n",
      "parts: a feature extractor that includes several convolution\n",
      "layers followed by max-pooling and activation function, and\n",
      "the linear classifier _**f**_ = _**W**_ _[âŠ¤]_ _**x**_ + _**b**_ in the last fully-connected\n",
      "layer applied on the feature vector _**x**_ of the penultimate layer\n",
      "for obtaining the logit vector _**f**_ _âˆˆ_ R _[C]_ of each pixel of the\n",
      "input image, in which _C_ represents the number of classes.\n",
      "To solve the overfitting problem and produce more discriminative logit vectors _**f**_ in training, classifier margin has been\n",
      "exploited [19], [21], [49]. Following [50], the classification\n",
      "margin is the difference between the predicted score _fcâˆ—_\n",
      "and target score _fy_, where _y_ indicates the ground truth\n",
      "class labels and _c_ _[âˆ—]_ = argmax _c_ = _yfc_ . Based on the margin\n",
      "_fy âˆ’_ _fcâˆ—_, the traditional classifier margin loss function can\n",
      "be expressed as follows:\n",
      "\n",
      "\n",
      "       -        _Lmargin_ = _L_ max _,_ (1)\n",
      "_c_ = _y_ _[f][c][ âˆ’]_ _[f][y]_ [ +] _[ Ï]_\n",
      "\n",
      "\n",
      "in which _Ï_ is a boundary control parameter, usually _Ï â‰¥_ 0.\n",
      "Thus, increasing the value of _Ï_ results in a larger classification distance between labels.\n",
      "According to [20], the cross-entropy loss ( _LCE_ ) can\n",
      "partially encourage the development of a large-margin classifier within the CNNs. Based on this analysis, a symmetric Kullback-Leibler (KL) divergence term _LLM_ was\n",
      "introduced as a regularization component for _LCE_, inducing\n",
      "a more large-margin classifier in the original _LCE_ . The\n",
      "combination was termed the large-margin cross-entropy loss\n",
      "( _LCE_ + _LM_ ), and it is the formula is presented as follows:\n",
      "\n",
      "\n",
      "_LCE_ + _LM_ = _LCE_ + _LLM_ _,_ (2)\n",
      "\n",
      "\n",
      "in which _LCE_ is defined as:\n",
      "\n",
      "\n",
      "_LCE_ = _âˆ’_ log _py_ ( _**f**_ ) = _âˆ’_ log ~~ï¿½~~ _C_ exp( _fy_ ) _,_ (3)\n",
      "_c_ =1 [exp][(] _[f][c]_ [)]\n",
      "\n",
      "\n",
      "\n",
      "and _LLM_ is expressed as:\n",
      "\n",
      "\n",
      "\n",
      "where _CUUC_ denotes the UUC, and _Î»out_ the cutoff threshold to determinate the UUC pixels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_LLM_ = _[Î»]_\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_c_ = _y_\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "exp( _fc_ ) 1\n",
      "\n",
      "~~ï¿½~~ _c_ _[â€²]_ = _y_ [exp][(] _[f][c][â€²]_ [)] _[ âˆ’]_ _C âˆ’_ 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(4)\n",
      "\n",
      "\n",
      "\n",
      "_Ã—_ log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "exp( _fc_ )\n",
      "\n",
      "~~ï¿½~~\n",
      "\n",
      "_c_ _[â€²]_ = _y_ [exp][(] _[f][c][â€²]_ [)]\n",
      "\n",
      "\n",
      "\n",
      "_,_\n",
      "\n",
      "\n",
      "\n",
      "where _Î»_ is a regularization parameter. Increasing the value\n",
      "of the _Î»_ enlarges the space between classes, thereby more\n",
      "resistance faced by the learning objectives. The detailed\n",
      "derivation has been explored in [19].\n",
      "Considering the class imbalance between the background\n",
      "and the foreground classes in the tattoo semantic segmentation dataset, we modify the _LCE_ + _LM_ replacing the _LCE_\n",
      "by the focal loss _LF CL_ . This choice is based on the fact\n",
      "that _LF CL_ is a variant of the _LCE_ that preserves the\n",
      "discriminative capacity of the original loss while dealing\n",
      "with the imbalance among the classes [51]. Thus, our\n",
      "proposed large-margin focal loss ( _LF CL_ + _LM_ ) is described\n",
      "by:\n",
      "_LF CL_ + _LM_ (ours) = _LF CL_ + _LLM_ _,_ (5)\n",
      "\n",
      "\n",
      "_LF CL_ is defined as:\n",
      "\n",
      "_LF CL_ = _Î±_ (1 _âˆ’_ _py_ ( _**f**_ )) _[Î³]_ _Ã—_ log _py_ ( _**f**_ ) _._ (6)\n",
      "\n",
      "\n",
      "The _LF CL_ applies a modulating term to the _LCE_ to focus\n",
      "learning on hard examples and down-weight the numerous\n",
      "easy examples, where _Î±_ control the class weights and _Î³_\n",
      "reduce the loss contribution from easy examples. Thus, we\n",
      "obtain a loss that deals with imbalances between classes\n",
      "while increasing the classification margin to improve the\n",
      "discriminability of the trained model.\n",
      "\n",
      "\n",
      "_**D. OPEN-SET CLASSIFIER FOR SEMANTIC**_\n",
      "_**SEGMENTATION**_\n",
      "In semantic segmentation networks, the logit vector _**f**_ _âˆˆ_\n",
      "R _[C]_ are commonly normalized using the softmax function\n",
      "into a probability distribution for each class _y âˆˆ{_ 1 _, ..., C}_\n",
      "to perform the final classification of each pixel.Therefore,\n",
      "the final classification of each pixel is defined as _Y_ [Ë†] _f_ _[close]_ =\n",
      "argmax _py_ ( _**f**_ ) _._\n",
      "This learnable classifier cannot recognize UUC, making it\n",
      "unsuitable for open-set recognition as it assigns all features\n",
      "to KKCs [42]. Thus, open-set classifiers must be developed\n",
      "to classify KKCs while recognizing UUCs accurately. Previous studies have investigated this development in semantic\n",
      "segmentation task [16], [38], [42], [52]. Based on those\n",
      "studies, one can present the general open set classifier as\n",
      "follows:\n",
      "\n",
      "\n",
      "\n",
      "_Y_ Ë† _f_ _[open]_ =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_CUUC_ max( _py_ ( _**f**_ )) â©½ _Î»out,_\n",
      "(7)\n",
      "_Y_ Ë† _f_ _[close]_ max( _py_ ( _**f**_ )) _> Î»out._\n",
      "\n",
      "\n",
      "\n",
      "8 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "A recent study [38] presented an open-set classifier\n",
      "method based on principal components analysis (PCA) on\n",
      "the internal logic vectors of CNNs to provide an open-set\n",
      "semantic segmentation, named Open Principal Component\n",
      "Scoring with Incremental Learning (OpenIPCS). Its training\n",
      "approach is efficient, which is crucial due to the exponential\n",
      "nature of pixel-level classification compared to image classification. Furthermore, the study notes that OpenIPCS outperforms open-set semantic segmentation strategies based\n",
      "on probability maps by a significant margin.\n",
      "OpenIPCS was inspired by the Conditional Gaussian\n",
      "Distribution Learning (CGDL) proposed in [53], which is\n",
      "a Variational Autoencoder (VAE) model for conditional\n",
      "Gaussian distribution estimation, capable of learning conditional distributions of KKC and rejecting UUC examples.\n",
      "In distinction, the OpenIPCS replaced the VAE with PCA\n",
      "and uses multiple internal activation layers to adjust the\n",
      "generative model with validation samples only.\n",
      "\n",
      "[54] showed that logit vectors get closer to the label\n",
      "space as CNN layers deepen. Thus, in addition to the last\n",
      "layer _f_, OpenIPCS considers the enabling aspects of the\n",
      "previous layers _f, f_ [1] _, . . ., f_ _[L]_, in which _L_ denotes the total\n",
      "number of the CNNs layers. This approach combines lowlevel and high-level semantic information, thus enhancing\n",
      "the discrimination capability of the model.\n",
      "Then, for each pixel, a corresponding feature vector\n",
      "_f_ Ë† _train_ is built by concatenating the networkâ€™s internal logit\n",
      "vectors _f, f_ [1] _, . . ., f_ _[L]_ . Such a concatenation produces highdimensional and redundant features due to the hundreds or\n",
      "thousands of activation channels present in the CNN and\n",
      "the FCN layers [55], [56].\n",
      "As described by [57], PCA can serve a dual purpose.\n",
      "Apart from its primary role in reducing dimensionality, it\n",
      "can also act as a probability density estimator with Gaussian\n",
      "priors. These features allow OpenIPCs to use PCA as a\n",
      "generative model ( _G_ ) for UUC recognition while solving\n",
      "the high-dimensionality problem of feature vectors.\n",
      "This _G_ is incrementally adjusted using only the validation\n",
      "images. This process consists of adjusting the PCA with a\n",
      "batch of samples from the validation set. The classification\n",
      "step using the _G_ consists of projecting the feature vector\n",
      "_f_ Ë† _test_ of the test images in the latent space obtained by\n",
      "adjusting the PCA on the validation images and performing\n",
      "the inverse process to obtaining the _f_ [Ë†] _test_ _[G]_ [. The difference]\n",
      "between the original feature vector _f_ [Ë†] _test_ and the _f_ [Ë†] _test_ _[G]_ [is]\n",
      "calculated. Consequently, pixels of KKCs have low difference values, while pixels belonging to the UUCs have high\n",
      "difference values. Thus, in accordance with [38], open-set\n",
      "recognition from OpenIPCS can be achieved as follows:\n",
      "\n",
      "\n",
      "\n",
      "As outlined in [54], the deeper layers are closer to the\n",
      "label space. Based on this analysis, this work proposes\n",
      "a generalized version of OpenIPCS (G-OpenIPCS) that\n",
      "focuses only on the last and penultimate layers of CNNs.\n",
      "These layers contain more substantial semantic information,\n",
      "which proves highly valuable for training OpenIPCS. Consequently, it becomes possible to disregard the other network\n",
      "layers within the CNNs. This aspect simplifies our approach\n",
      "and is easy to incorporate into different network architectures, unlike the original OpenIPCS building exclusively on\n",
      "the FCN decoder.\n",
      "\n",
      "\n",
      "**IV. EXPERIMENTS SETUP**\n",
      "Due to the open-set recognition process being still dependent on models built in a closed-set [33], [38], our experiments are divided into three parts. First, in Section IV-A,\n",
      "we compare our proposed loss function, described in Section III-C, with other significant loss functions widely used\n",
      "in semantic segmentation or recently introduced to improve\n",
      "discriminability in deep neural networks. Subsequently, in\n",
      "Section IV-B, we employ the proposed G-OpenIPCS approach detailed in Section III-D to evaluate the performance\n",
      "of models trained with the loss functions from the previous\n",
      "experiment in an open-set tattoo semantic segmentation\n",
      "scenario. Finally, we compare the performance of our openset semantic segmentation approach with other state-ofthe-art open-set semantic segmentation methods. Figure 4\n",
      "provides an overview of the proposed method. Each stage\n",
      "of Figure 4 is detailed as follows.\n",
      "\n",
      "\n",
      "_**A. CLOSED-SET SEMANTIC SEGMENTATION**_\n",
      "**Datasets** . A total of 2,045 images from TSSD2023 were\n",
      "used to train and evaluate the closed-set models. These\n",
      "images follow the following division: 1,404 for training, 387\n",
      "for validation, and 254 for closed set testing, as described\n",
      "in Section III-A.\n",
      "**Pre-processing and Data augmentations** . This step involves a sequence of transformations to train and improve\n",
      "the semantic segmentation network, motivated by the limited\n",
      "dataset of training images available in TSSD2023 around\n",
      "the high diversity of tattoos that virtually have no limits in\n",
      "the real world. This process involves the following steps: a)\n",
      "Implement a resizing operation, which can be either random\n",
      "or center crop, or resize to get an image with a resolution of\n",
      "224 _Ã—_ 224; b) Next, there is a 50% probability of applying\n",
      "one of the geometric transformations to the image, including\n",
      "horizontal and vertical flipping, shifting, scaling, or rotation.\n",
      "These transformations have a scale and shift limit of 0.2, and\n",
      "rotation is limited to a maximum of positive or negative\n",
      "45 degrees; c) Executing the tattoo semantic augmentation\n",
      "method proposed by this work, as described in more detail\n",
      "below. This augmentation involves RGB shifting, conversion\n",
      "to grayscale, and the application of random tone curve\n",
      "transformations. Each of these transformations has a 25%\n",
      "probability of being applied to each class within an image;\n",
      "d) Subsequently, there is a 50% chance of applying certain\n",
      "\n",
      "\n",
      "\n",
      "_Y_ Ë† _f_ _[openipcs]_ =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "- _CUUC abs_ ( _f_ [Ë†] _test âˆ’_ _f_ [Ë†] _test_ _[G]_ [)] _[ > Î»]_ _out_ _[,]_\n",
      "(8)\n",
      "_Y_ Ë† _f_ _[close]_ _abs_ ( _f_ [Ë†] _test âˆ’_ _f_ [Ë†] _test_ _[G]_ [)][ â©½] _[Î»]_ _out_ _[.]_\n",
      "\n",
      "\n",
      "\n",
      "The _Î»out_ do not represent equal statistical entities with\n",
      "the Equation 7. Thus, as in [38], the _Î»out_ value was defined\n",
      "from preset values of True Positive Rate (TPR).\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 9\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "lightweight multilayer perceptron (MLP) decoders. The\n",
      "model was pre-trained on the ImageNet dataset [59] and\n",
      "was acquired through the Hugging Face library [5] . During\n",
      "closed-set training, a batch size of 16 images on an RTX\n",
      "3090 with 500 epochs was utilized, employing a patience\n",
      "factor of 10 epochs. The Nadam optimizer [64] was used,\n",
      "with an initial learning rate set at 0 _._ 0001 and a weight decay\n",
      "coefficient of 0 _._ 00001.\n",
      "**Baseline loss functions** . In order to investigate the performance of large-margin focal loss ( _LLMF CL_ ) in both closedset and open-set semantic segmentation, we conducted a\n",
      "comparative study with five other loss functions: i) crossentropy loss ( _LCE_ ) â€“ it was selected because it is widely\n",
      "used and a standard choice for semantic segmentation; ii)\n",
      "focal loss ( _LF CL_ ) [51] â€“ this is a commonly used variation\n",
      "\n",
      "\n",
      "[4https://albumentations.ai/](https://albumentations.ai/)\n",
      "[5https://huggingface.co/](https://huggingface.co/)\n",
      "\n",
      "\n",
      "\n",
      "because it has recently been proposed to improve the results\n",
      "of closed-set classifiers by increasing the space between the\n",
      "decision boundaries of each class. Moreover, we utilized\n",
      "the _LLMCE_ as a foundation to develop our proposed loss\n",
      "function for the open-set tattoo semantic segmentation on a\n",
      "scenario that presents various challenges, such as high intraclass variability, class imbalance, and small tattoos, while\n",
      "enhanced discriminative capabilities in closed-set and openset semantic segmentation. Table 6 presents the loss function\n",
      "parameters used for model training.\n",
      "\n",
      "\n",
      "**Metrics** . In order to assess the results of closed-set semantic segmentation, followed by open-set segmentation,\n",
      "we employed the following metrics. Firstly, we used the\n",
      "Area Under the ROC Curve (AUROC) because it has been\n",
      "recently utilized to assess open-set semantic segmentation\n",
      "\n",
      "[38], [42]. This metric aids in measuring the modelâ€™s ability\n",
      "to distinguish between classes, particularly with respect\n",
      "\n",
      "\n",
      "\n",
      "10 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 6.** Parameters\n",
      "per each loss function\n",
      "utilized for the model\n",
      "training.\n",
      "\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **Parameters**\n",
      "\n",
      " - _LCE_  \n",
      "[51] _LF CL_ _Î±_ : 1.0, _Î³_ : 2.0\n",
      "\n",
      "[65] _LDCE_ _T_ : 3\n",
      "\n",
      "[66] _LLDAM_ _s_ : 30, _Î»max_ : 0.5\n",
      "\n",
      "[20] _LLMCE_ _Î»_ : 0.3\n",
      "\n",
      "[42] _LDCE_ + _V L_ _T_ : 3, _Î»vl_ : 0.01\n",
      "\n",
      "[67] _LDSC_ ++ _Î³_ : 2.0\n",
      "Ours _LLMF CL_ _Î»_ : 0.3, _Î±_ : 1.0, _Î³_ : 2.0\n",
      "\n",
      "\n",
      "\n",
      "to UUC. Secondly, the macro-averaged F1-Score (Macro\n",
      "F1), recommended in previous works for open-class classification [68], [69]. It helps evaluate the modelâ€™s precision and recall of the segmented pixels. Lastly, the Mean\n",
      "Intersection over Union (mIoU) is a standard metric for\n",
      "semantic segmentation evaluation that provides an overview\n",
      "of the modelâ€™s performance across all classes, including both\n",
      "KKCs and UUC.\n",
      "\n",
      "\n",
      "_**B. OPEN-SET SEMANTIC SEGMENTATION**_\n",
      "**Datasets** . G-OpenIPCS was trained using just the 387\n",
      "validation images. The test open-set with 315 images was\n",
      "used to evaluate the proposed method, as detailed in Section III-A.\n",
      "**Implementation details** . For each model trained using the\n",
      "evaluated loss functions in closed-set semantic segmentation, a G-OpenIPCS model was trained to perform semantic\n",
      "segmentation in an open-set scenario. As depicted in Figure\n",
      "4, our approach utilizes the last two connected layers of\n",
      "the segmentation network to construct feature vectors for\n",
      "each pixel. In the case of SegFormer, this feature vector\n",
      "is equal to the number of KKCs plus the logit vector of\n",
      "the inner layer, with a length of 768. Subsequently, we\n",
      "defined 64 principal components for the adjustment in GOpenIPCS. It is worth noting that the feature vector size and\n",
      "the number of principal components may vary depending\n",
      "on the semantic segmentation model and the application\n",
      "domain. Once G-OpenIPCS is trained, it becomes possible\n",
      "to recognize UUCs by considering the discrepancy between\n",
      "the raw feature vector and the projected feature vector, as\n",
      "outlined in Equation 8.\n",
      "**Baseline** . Initially, a comparative analysis of the openset semantic segmentation results was conducted using GOpenIPCS trained for each evaluated loss function. This\n",
      "step serves to validate the performance of the proposed\n",
      "loss function in the open-set scenario. Subsequently, our\n",
      "approach was compared with other significant open-set\n",
      "segmentation methods proposed in the state-of-the-art literature. These methods include OpenIPCS [38], Anomalous Probability Map (APM) [42], Maximum Unnormalized\n",
      "Logit (MaxLogit) [39], and Maximum Softmax Probability\n",
      "(MSP) [43]. For these methods, training followed the same\n",
      "configurations as described in Section IV-A, except that\n",
      "only geometric transformations were applied. All networks\n",
      "were pre-trained on the ImageNet dataset [59]. Specifically,\n",
      "for DRN50+PSPNet [42] and RN101+PSPNet [39], images\n",
      "were resized to dimensions of 250, 300, 350, 400, and 450,\n",
      "\n",
      "\n",
      "\n",
      "with a down-sampling factor of 8 for DRN50. Additionally,\n",
      "we implemented SoftMax-Thresholding (SoftMax-T) [38]\n",
      "and OpenMax [36] as the baselines for our implementation.\n",
      "As this is the first study on tattoo semantic segmentation\n",
      "and without other tattoo semantic datasets, we only compare\n",
      "our approach with state-of-the-art methods that do not use\n",
      "auxiliary data to supply their open-set recognition models.\n",
      "Finally, to demonstrate the generalization of our approach\n",
      "with baselines, we replicated the experiments, replacing the\n",
      "SegFormer with the Swin+UperNet model [70].\n",
      "**Cutoff thresholds** . All the methods compared in Section V-C require a cutoff threshold to distinguish between\n",
      "KKC and UUC pixels. To realistically undertake an open-set\n",
      "recognition task, these limits are defined empirically based\n",
      "on the available KKCs during the validation data following\n",
      "the conditions of the original papers. No information about\n",
      "the UUCs is used to select the cutoff thresholds for the openset classifiers. For the proposed G-OpenIPCS, the preset\n",
      "values of the cutoff thresholds ( _Î»out_ ) are determined based\n",
      "on TPR quantiles, as outlined in [38].\n",
      "**Metrics** . To evaluate the semantic segmentation results in an\n",
      "open set, we kept the AUROC, Macro F1, and mIoU used\n",
      "in the experiment in a closed set, described in Section IV-A.\n",
      "Additionally, we highlight the results involving UUCs.\n",
      "\n",
      "\n",
      "**V. RESULTS AND DISCUSSIONS**\n",
      "\n",
      "This section presents the results and discussions obtained\n",
      "from our experiments. The purpose is to validate the impact of the proposed Large-Margin Focal Loss on closed\n",
      "and open-set tattoo semantic segmentation, in addition\n",
      "to demonstrating the effectiveness of the proposed GOpenIPCS for open-set semantic segmentation. Initially, in\n",
      "Section V-A, we present the results of the loss functions\n",
      "evaluated in this work in a closed set scenario. Next,\n",
      "in Section V-B, we evaluate the performance of models\n",
      "trained using the proposed G-OpenIPCS method. Finally,\n",
      "in Section V-C, we compare our proposed approach with\n",
      "other state-of-the-art techniques.\n",
      "\n",
      "\n",
      "_**A. CLOSED-SET SEMANTIC SEGMENTATION**_\n",
      "\n",
      "This section focuses on evaluating the performance impact of the proposed _LLMF CL_ in a closed-set semantic\n",
      "segmentation scenario. To demonstrate this, several loss\n",
      "functions from the literature were compared, as described\n",
      "in Section IV-A. Table 7 presents the overall results for all\n",
      "evaluated loss functions in terms of AUROC, Macro F1, and\n",
      "mIoU. When evaluating the performance of loss functions,\n",
      "_LLMF CL_ outperforms all other metrics in terms of Macro\n",
      "F1 and maintains consistent results across AUROC and\n",
      "mIoU. This makes _LLMF CL_ a suitable choice for closedset semantic segmentation. However, _LLDAM_ and _LLMCE_\n",
      "exhibit an outperform over _LLMF CL_ in terms of AUROC\n",
      "and mIoU, respectively. Additionally, _LF CL_ stood out by\n",
      "producing results that closely resemble those of _LLMF CL_\n",
      "and _LLMF CL_, despite not incorporating a large margin in\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 11\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 7.** Comparison of the closed-set segmentation results\n",
      "achieved per each loss function. Bold values indicate the best\n",
      "overall results, including all loss functions. The results were\n",
      "obtained using SegFormer.\n",
      "\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **AUROC** **Macro F1** **mIoU**\n",
      "\n",
      " - _LCE_ .7966 _Â±_ .122 .6179 _Â±_ .224 .4805 _Â±_ .216\n",
      "\n",
      "[51] _LF CL_ .8024 _Â±_ .111 .6433 _Â±_ .213 .5053 _Â±_ .209\n",
      "\n",
      "[65] _LDCE_ .8030 _Â±_ .118 .6108 _Â±_ .219 .4724 _Â±_ .218\n",
      "\n",
      "[66] _LLDAM_ **.8295** _Â±_ .105 .6497 _Â±_ .181 .5057 _Â±_ .191\n",
      "\n",
      "[20] _LLMCE_ .8110 _Â±_ .112 .6457 _Â±_ .217 **.5097** _Â±_ .216\n",
      "\n",
      "[42] _LDCE_ + _V L_ .7843 _Â±_ .120 .5933 _Â±_ .217 .4519 _Â±_ .205\n",
      "\n",
      "[67] _LDSC_ ++ .8108 _Â±_ .108 .6303 _Â±_ .200 .4874 _Â±_ .196\n",
      "Ours _LLMF CL_ .8142 _Â±_ .102 **.6514** _Â±_ .189 .5090 _Â±_ .194\n",
      "\n",
      "\n",
      "\n",
      "its formulation. This emphasizes its capability to address\n",
      "class imbalance encountered in TSSD2023.\n",
      "On the other hand, _LCE_, _LDSC_ ++, and particularly\n",
      "_LDCE_ and _LDCE_ + _V L_ exhibit lower effectiveness compared\n",
      "to the other loss functions. Among these less favorable\n",
      "outcomes, _LDCE_ + _V L_ yielded the poorest results. This could\n",
      "indicate that the significant variability present in TSSD2023\n",
      "posed challenges for the metric learning process. Thus,\n",
      "functions based on large-margin, such as proposed _LLMF CL_\n",
      "and _LLMCE_, were demonstrated to be more effective for\n",
      "closed-set semantic tattoo segmentations.\n",
      "Table 8 presents the closed-set segmentation results categorized by class obtained from proposed _LLMF CL_ . It\n",
      "emphasizes the top 5 best and top 5 worst results, excluding\n",
      "the background class. The top-5 best have high IoU values, indicating that the modelâ€™s segmentation performance\n",
      "for these categories is particularly accurate. Additionally,\n",
      "they exhibit relatively high AUROC and Macro F1 scores.\n",
      "Notably, the classes within the top 5 best results do not\n",
      "necessarily constitute the majority of pixel quantity in\n",
      "TSSD2023, as illustrated in Figure 3. This suggests that the\n",
      "excellent performance comes from other aspects, such as the\n",
      "semantic dissimilarity of this set of tattoos to other classes,\n",
      "and mainly due to the recurrence of these tattoos being tattooed individually, without overlapping with other semantic\n",
      "classes, as can be briefly observed in Table 9. In contrast,\n",
      "the top 5 worst classes can be categorized as complementary\n",
      "tattoos, meaning they are rarely encountered in isolation but\n",
      "are typically accompanied by other predominantly dominant\n",
      "tattoos in terms of pixel quantity. For instance, a crown is\n",
      "almost always found with another element, such as a human\n",
      "or animal face. Leaves often complement flowers and stems,\n",
      "and water is nearly always associated with aquatic creatures\n",
      "like fishes, sharks, and octopuses.\n",
      "In outline, _LLMF CL_ showed considerable impact in the\n",
      "closed-set semantic segmentation, presenting the best results\n",
      "overall. However, aspects can still be considered to improve\n",
      "performance in closed-set segmentation. In addition to dealing with class imbalance, it is still necessary to deal with the\n",
      "overlap between semantic classes, improving the accuracy of\n",
      "segmentations, especially in complementary tattoo classes.\n",
      "This will be detailed in Section VII.\n",
      "\n",
      "\n",
      "_**B. OPEN-SET SEMANTIC SEGMENTATION**_\n",
      "\n",
      "This section evaluates the impact on the performance of\n",
      "the proposed _LLMF CL_ in an open-set scenario. Table 10\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 8.** Closed-set segmentation results separated by classes obtained\n",
      "from the SegFormer trained using the proposed _LLMF CL_ . The results were\n",
      "sorted in descending order of mIoU _â†‘_ values. The top-5 best classes are\n",
      "highlighted in green, while the top-5 worst classes are highlighted in red .\n",
      "\n",
      "\n",
      "**Class** **AUROC** **Macro F1** **mIoU** _â†‘_\n",
      "background 0.8886 0.9669 0.9359\n",
      "\n",
      "butterfly 0.8729 0.8164 0.6897\n",
      "lion 0.8714 0.8106 0.6816\n",
      "star 0.8787 0.8028 0.6706\n",
      "dog 0.8863 0.7868 0.6485\n",
      "scorpion 0.9149 0.7586 0.6110\n",
      "flower 0.8212 0.7412 0.5888\n",
      "fox 0.8302 0.7083 0.5483\n",
      "fish 0.8278 0.7042 0.5435\n",
      "shark 0.8604 0.6978 0.5359\n",
      "gun 0.8507 0.6914 0.5284\n",
      "cat 0.7772 0.6878 0.5241\n",
      "anchor 0.8192 0.6565 0.4886\n",
      "bird 0.7641 0.6490 0.4804\n",
      "diamond 0.8530 0.6280 0.4577\n",
      "mermaid 0.9150 0.6256 0.4551\n",
      "eagle 0.8788 0.6180 0.4472\n",
      "spide 0.8606 0.5998 0.4283\n",
      "heart 0.7696 0.5809 0.4093\n",
      "ribbon 0.7572 0.5496 0.3789\n",
      "wolf 0.8008 0.5403 0.3702\n",
      "skull 0.7562 0.5393 0.3693\n",
      "shield 0.7086 0.5375 0.3675\n",
      "\n",
      "all 0.8142 0.6514 0.5090\n",
      "\n",
      "\n",
      "compares the open-set segmentation results obtained using\n",
      "various loss functions, all evaluated using the proposed GOpenIPCS method. Among the loss functions, _LLMF CL_\n",
      "achieves the highest AUROC score of 0.8013, Macro F1\n",
      "with 0.6318, and also outperforms the mIoU score with\n",
      "0.4900. These results indicate that _LLMF CL_ is the most\n",
      "effective loss function in terms of overall performance. Considering only UUC, although _LCE_ has the best performance\n",
      "in terms of AUROC (UUC). _LLMF CL_ remains competitive\n",
      "in this metric while providing superior performance in terms\n",
      "of the F1-Score (UUC) and mIoU (UUC).\n",
      "\n",
      "Table 11 presents an evaluation of class-level performance\n",
      "from open-set semantic segmentation results from proposed\n",
      "_LLMF CL_ . The top-5 best practically remained the same as\n",
      "\n",
      "\n",
      "\n",
      "12 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "**TABLE 9.** Visual result samples of open-set semantic segmentation for each loss function. UUCs are depicted as black pixels. The respective colors for other\n",
      "semantic classes can be found in Figure 2. The UUC has been highlighted in yellow, while the top-5 best classes are highlighted in green, and the top-5 worst\n",
      "\n",
      "\n",
      "classes are highlighted in red . The results were obtained using SegFormer and proposed G-OpenIPCS.\n",
      "\n",
      "\n",
      "|Class|Image Mask LCE LF CL LDCE LLDAM LLMCE LDCE+V L LDSC++ L LMF CL|\n",
      "|---|---|\n",
      "|Top-5 Best<br>tiger<br>key<br>octopus<br>owl<br>star||\n",
      "|Top-5 Worst<br>wolf<br>leaf<br>water<br>knife<br>fre||\n",
      "|UUCs||\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 10.** Comparison of the\n",
      "open-set segmentation results\n",
      "achieved per each loss function. All\n",
      "results are obtained on SegFormer\n",
      "and the proposed G-OpenIPCS\n",
      "method. Bold values indicate the best\n",
      "overall results, including all loss\n",
      "functions.\n",
      "\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **AUROC** **AUROC** _UUC_ **Macro F1** **F1 Score** _UUC_ **mIoU** _UUC_ **IoU**\n",
      "\n",
      " - _LCE_ .7737 _Â±_ .122 **.8204** .5892 _Â±_ .234 .3759 .4525 _Â±_ .220 .2314\n",
      "\n",
      "[51] _LF CL_ .7865 _Â±_ .112 .7212 .5919 _Â±_ .216 .2886 .4505 _Â±_ .207 .1686\n",
      "\n",
      "[65] _LDCE_ .7827 _Â±_ .119 .7333 .5770 _Â±_ .215 .3782 .4357 _Â±_ .209 .2332\n",
      "\n",
      "[66] _LLDAM_ **.8077** _Â±_ .118 .4977 .5851 _Â±_ .202 .0428 .4399 _Â±_ .193 .0219\n",
      "\n",
      "[20] _LLMCE_ .7975 _Â±_ .111 .6916 .6070 _Â±_ .209 .4052 .4650 _Â±_ .204 .2541\n",
      "\n",
      "[42] _LDCE_ + _V L_ .7693 _Â±_ .116 .7345 .5681 _Â±_ .214 .3903 .4254 _Â±_ .201 .2425\n",
      "\n",
      "[67] _LDSC_ ++ .7939 _Â±_ .110 .6353 .5822 _Â±_ .203 .2605 .4375 _Â±_ .197 .1497\n",
      "Ours _LLMF CL_ .8013 _Â±_ .104 .7827 **.6318** _Â±_ .201 **.4318** **.4900** _Â±_ .201 **.2753**\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 13\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "the closed-set results, which is natural given the dependence\n",
      "of the closed-set model of the proposed approach on openset recognition. However, the â€˜starâ€™ class was an exception,\n",
      "replacing the â€˜snakeâ€™ class among the top 5 in terms of\n",
      "IoU. This is probably due to the low similarity of the\n",
      "â€˜starâ€™ class to the other semantic classes in TSSD2023,\n",
      "which practically maintained the performance obtained in\n",
      "the closed set. Classes that show more significant similarity\n",
      "to other classes, regardless of whether a KKC or a UUC,\n",
      "become more challenging for open-set segmentation, as\n",
      "they generate considerable uncertainty for the segmentation\n",
      "model.\n",
      "This case of declining performance owing to class similarity is also evident among the top 5 worst classes.\n",
      "Classes such as â€˜leafâ€™, â€˜waterâ€™, â€˜knifeâ€™, and â€˜fireâ€™ remained\n",
      "in this category, underscoring their significant reliance on\n",
      "the closed-set model. However, there was an exception\n",
      "in the â€˜wolfâ€™ class, which, despite its poor performance\n",
      "in the closed set, experienced a deterioration after openset segmentation. This decline is attributed to its semantic\n",
      "similarity with other KKC animals, such as â€˜dogâ€™ and â€˜foxâ€™,\n",
      "and its resemblance to UUCs, particularly in bear tattoos.\n",
      "This provides misclassifications, as depicted in Figure 2.\n",
      "Furthermore, KKCs can face challenges due to the high\n",
      "semantic variability exhibited within tattoos of the same\n",
      "class, arising from the infinite possibilities for representing an object through tattoos. For illustration, consider\n",
      "the representation of a â€˜catâ€™ in various styles, such as\n",
      "cartoonish, realistic, stick-figure, geometric, and so on. It\n",
      "becomes exceedingly challenging to group all these diverse\n",
      "representations to ensure no shape is mistakenly isolated as\n",
      "an outlier and segmented as part of the unknown-unknown\n",
      "class (UUC). We endeavored to construct a robust data\n",
      "augmentation pipeline to address this limitation, as we\n",
      "believe it offers a potential solution (detailed in Section VII).\n",
      "Regarding the UUC, the results obtained using _LLMF CL_\n",
      "remain somewhat limited but show great promise. Notably,\n",
      "the UUC exhibited superior results in terms of IoU and F1Score when compared to the top 5 worst classes. However,\n",
      "in terms of AUROC, 12 KKCs are surpassed.\n",
      "In conclusion, utilizing the proposed _LLMF CL_ in conjunction with the G-OpenIPCS achieves the best overall results in open-set segmentation tasks. Its robust segmentation\n",
      "performance, capable of effectively distinguishing between\n",
      "UUCs and KKCs, makes it a promising choice for challenging scenarios with high semantic variability between classes.\n",
      "However, it is essential to note that there are still tough\n",
      "situations and areas where further improvements are needed\n",
      "in the context of open-set segmentation for TSSD2023.\n",
      "\n",
      "\n",
      "_**C. COMPARISON WITH OTHERS OPEN-SET SEMANTIC**_\n",
      "_**SEGMENTATION METHODS**_\n",
      "Table 12 comprehensively compares different state-of-theart open-set semantic segmentation methods, including the\n",
      "proposed approach, and two more baselines: SoftMax-T and\n",
      "OpenMax. The proposed approach using _LLMF CL_ and G\n",
      "\n",
      "\n",
      "**TABLE 11.** Open-set segmentation results are separated by classes obtained\n",
      "from the SegFormer trained using the proposed _LLMF CL_ . All results are\n",
      "obtained on the proposed G-OpenIPCS method. The results were sorted in\n",
      "descending order of mIoU _â†‘_ values. The UUC has been highlighted in yellow,\n",
      "\n",
      "\n",
      "while the top-5 best classes are highlighted in green, and the top-5 worst\n",
      "\n",
      "\n",
      "classes are highlighted in red .\n",
      "\n",
      "\n",
      "**Class** **AUROC** **Macro F1** **mIoU** _â†‘_\n",
      "background 0.9025 0.9643 0.9310\n",
      "\n",
      "snake 0.9177 0.8080 0.6779\n",
      "butterfly 0.8714 0.8064 0.6756\n",
      "dog 0.8584 0.7690 0.6247\n",
      "scorpion 0.8940 0.7424 0.5903\n",
      "diamond 0.8420 0.7195 0.5619\n",
      "lion 0.8449 0.7192 0.5615\n",
      "fox 0.8279 0.7124 0.5533\n",
      "flower 0.7971 0.7043 0.5435\n",
      "fish 0.8221 0.6991 0.5374\n",
      "gun 0.8430 0.6972 0.5351\n",
      "cat 0.7758 0.6965 0.5344\n",
      "shark 0.8604 0.6798 0.5150\n",
      "mermaid 0.9158 0.6754 0.5099\n",
      "bird 0.7631 0.6544 0.4863\n",
      "anchor 0.8075 0.6431 0.4740\n",
      "eagle 0.8780 0.5941 0.4226\n",
      "skull 0.7502 0.5733 0.4019\n",
      "shield 0.7063 0.5429 0.3725\n",
      "spide 0.8202 0.5420 0.3718\n",
      "ribbon 0.7594 0.5402 0.3700\n",
      "crown 0.7121 0.4761 0.3124\n",
      "heart 0.6827 0.4573 0.2964\n",
      "\n",
      "all 0.8013 0.6318 0.4900\n",
      "\n",
      "\n",
      "OpenIPCS stands out as the best-performing method. It\n",
      "achieves the highest AUROC, Macro F1, mIoU, and IoU\n",
      "(UUC) values, indicating its superiority in open-set tattoo\n",
      "semantic segmentation. The scores obtained for AUROC\n",
      "of 0.8013, Macro F1 of 0.6318, mIoU of 0.4900, and\n",
      "IoU (UUC) of 0.2753 are significant compared to other\n",
      "approaches in the literature. However, the performance\n",
      "difference is less to the baselines, except for the values\n",
      "obtained from IoU to UUC. This indicates that the proposed\n",
      "approach produces more accurate and visually coherent\n",
      "segmentation results.\n",
      "\n",
      "In comparison to the original OpenIPCS, it is essential to\n",
      "emphasize the strengths that make our approach superior.\n",
      "The original OpenIPCS relies on an FCN decoder that\n",
      "combines multiple layers from various levels of the network\n",
      "to construct the feature vector. In contrast, our G-OpenIPCS\n",
      "approach follows a more straightforward and intuitive path,\n",
      "considering that the critical features for class discrimination\n",
      "are primarily situated in the latter layers of the segmentation network. This approach leads to improved results and\n",
      "avoids the potential scrambling of high-level and low-level\n",
      "\n",
      "\n",
      "\n",
      "14 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 12.** Comparison\n",
      "between the proposed\n",
      "approach and state-of-the-art\n",
      "open-set semantic\n",
      "segmentation techniques.\n",
      "Bold values indicate the best\n",
      "overall results, including all\n",
      "methods.\n",
      "\n",
      "\n",
      "\n",
      "**Ref.** **Loss** **Network** **Open SetClassifer** **AUROC** **Macro F1** **mIoU** _UUC_ **IoU**\n",
      "\n",
      "Baseline _LCE_ SegFormer SoftMax-T .7855 _Â±_ .126 .5782 _Â±_ .229 .4398 _Â±_ .216 .1350\n",
      "Baseline _LCE_ SegFormer OpenMax .7785 _Â±_ .123 .5781 _Â±_ .227 .4389 _Â±_ .212 .1611\n",
      "Baseline _LCE_ Swin+UPerNet SoftMax-T .7234 _Â±_ .095 .4852 _Â±_ .198 .3428 _Â±_ .188 .1145\n",
      "Baseline _LCE_ Swin+UPerNet OpenMax .7134 _Â±_ .095 .4736 _Â±_ .197 .3325 _Â±_ .181 .1258\n",
      "\n",
      "[38] _LCE_ DN121+FCN OpenIPCS .6523 _Â±_ .098 .3596 _Â±_ .217 .2422 _Â±_ .183 .0632\n",
      "\n",
      "[38] _LCE_ WRN50+FCN OpenIPCS .6695 _Â±_ .098 .3983 _Â±_ .214 .2722 _Â±_ .185 .0470\n",
      "\n",
      "[42] _LDCE_ + _V L_ DRN50+PSPNet APM .7123 _Â±_ .139 .4290 _Â±_ .284 .3152 _Â±_ .244 .1657\n",
      "\n",
      "[39] _LCE_ RN101+PSPNet MaxLogit .7358 _Â±_ .139 .5173 _Â±_ .293 .4031 _Â±_ .268 .1439\n",
      "\n",
      "[43] _LCE_ + _P C_ RN50+DeepLabV3+ MaxLogit .7048 _Â±_ .105 .4471 _Â±_ .211 .3126 _Â±_ .191 .1327\n",
      "\n",
      "[43] _LCE_ + _P C_ RN50+DeepLabV3+ MSP .7401 _Â±_ .097 .4694 _Â±_ .207 .3157 _Â±_ .190 .1354\n",
      "Ours _LLMF CL_ SegFormer G-OpenIPCS **.8013** _Â±_ .104 **.6318** _Â±_ .201 **.4900** _Â±_ .201 **.2753**\n",
      "Ours _LLMF CL_ Swin+UPerNet [*] G-OpenIPCS .7921 _Â±_ .116 .5927 _Â±_ .211 .4518 _Â±_ .213 .1981\n",
      "\n",
      "- Evaluation of the generalization of the proposed method using Swin+UPerNet [62], [70].\n",
      "\n",
      "\n",
      "\n",
      "information that can occur when using multiple layers, as\n",
      "in the original OpenIPCS.\n",
      "\n",
      "Due to this more streamlined design choice, our approach\n",
      "integrates with other modern segmentation architectures,\n",
      "such as transform-based structures. For instance, we employed the SegFormer and Swin+UperNet networks, outperforming FCN-based models in open-set tattoo semantic segmentation. Notably, the combination using the SegFormer\n",
      "outperforms other methods by a significant margin. This\n",
      "underscores that our design choice directly enhances performance, especially considering the high reliance on features\n",
      "in open-set segmentation models [42].\n",
      "\n",
      "Moreover, it is worth noting that the original OpenIPCS\n",
      "is built upon _LCE_ . While this loss function consistently\n",
      "delivers results in closed-set semantic segmentation, the\n",
      "challenge of open-set segmentation demands segmentation\n",
      "models with a heightened discriminative capacity, which is\n",
      "precisely what our proposed _LLMF CL_ aims to enhance.\n",
      "\n",
      "\n",
      "_**D. ABLATION STUDIES OF THE DATA AUGMENTATION**_\n",
      "_**COMPONENTS**_\n",
      "\n",
      "\n",
      "We performed an ablation study to examine the impact\n",
      "of various data augmentation techniques on our approach\n",
      "to open-set recognition for tattoo semantic segmentation.\n",
      "These techniques were divided into four categories, as\n",
      "explained in Section IV-A: geometric, image adjustments,\n",
      "dropout, and our proposed CSA method. We also included\n",
      "a no-augmentation experiment as a baseline for comparison.\n",
      "\n",
      "The results in Table 13 indicate that our approach performs better when all data augmentation methods are combined. Among these methods, image adjustments consistently showed performance gains in mIoU and IoU for UUC.\n",
      "The other approaches, including geometric and proposed\n",
      "CSA, improved the values of IoU for UUC, while the\n",
      "dropout method improved the mIoU. Notably, the proposed\n",
      "CSA significantly improved performance when combined\n",
      "with the other three methods. In summary, our approach is\n",
      "appropriate for effective open-set tattoo semantic segmentation.\n",
      "\n",
      "\n",
      "\n",
      "_**E. EVALUATION OF THE PERFORMANCE IN THE**_\n",
      "_**CLOSED-SET TATTOO SEGMENTATION SCENARIO**_\n",
      "\n",
      "Semantic segmentation focuses on recognizing specific\n",
      "classes of tattoos, such as cats, dogs, stars, and others. Tattoo\n",
      "segmentation only aims to isolate the tattoo region from\n",
      "the background. This distinction is beneficial in situations\n",
      "that do not require detailed semantic information about\n",
      "tattoos. However, it may be interesting that our approach\n",
      "also behaves adequately in closed-set tattoo segmentation\n",
      "scenarios. Thus, we evaluated the performance of our proposed approach in a closed tattoo segmentation scenario\n",
      "using the DeMSI dataset [9], where tattoos were manually\n",
      "annotated without any semantic differentiation. In addition\n",
      "to measuring the IoU for tattoos, we also evaluate the\n",
      "False Positive Rate (FPR), which indicates the proportion of\n",
      "background pixels incorrectly classified as tattoos, and the\n",
      "False Negative Rate (FNR), which means the proportion of\n",
      "tattoo pixels incorrectly classified as background.\n",
      "\n",
      "The quantitative results are presented in Table 14. We\n",
      "observed a degradation in the performance of the proposed\n",
      "approach when directly applying the model trained on\n",
      "TSSD2023 and converting the multiclass output to a binary\n",
      "output. This degradation is primarily reflected in the FNR\n",
      "values, indicating that many tattoo pixels were misclassified\n",
      "as background. We attribute this behavior to the labeling\n",
      "design used in TSSD2023. The primary factor is the detailed\n",
      "annotations in TSSD2023, designed to accurately classify\n",
      "the pixel classes, as seen in Table 4. For example, the sharkâ€™s\n",
      "mouth was separated from the shadow of the tattoo. In\n",
      "contrast, DeMSI does not provide refined annotations for\n",
      "tattoos, leading to multiple background pixels being incorrectly annotated as part of a tattoo, as shown in Table 15.\n",
      "The second factor relates to tattoo classes with insufficient\n",
      "semantic samples, such as dragons, letters, and spider webs,\n",
      "which were excluded in TSSD2023 and are ignored by our\n",
      "approach, as seen by the text tattoo in Table 15. Due to\n",
      "these differences in annotation design between DeMSI and\n",
      "TSSD2023 datasets, performance degradation occurs when\n",
      "directly applying our approach.\n",
      "\n",
      "To alleviate the performance degradation caused by annotation differences between the datasets, we fine-tuned the\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 15\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 13.** Evaluation of the effect of data\n",
      "augmentation components. The results were\n",
      "obtained using proposed _LLMF CL_, SegFormer,\n",
      "and proposed G-OPenIPCS.\n",
      "\n",
      "\n",
      "\n",
      "**No Aug.** **Geometric** **AdjustmentsImage** **Dropout** **(Ours)CSA** **mIoU** _UUC_ **IoU**\n",
      "âœ“ 0.4556 _Â±_ .186 0.1547\n",
      "âœ“ 0.4320 _Â±_ .182 0.1797\n",
      "âœ“ 0.4814 _Â±_ .203 0.1884\n",
      "âœ“ **0.4959** _Â±_ .189 0.1538\n",
      "âœ“ 0.4576 _Â±_ .203 0.1770\n",
      "âœ“ âœ“ âœ“ 0.4733 _Â±_ .196 0.2303\n",
      "âœ“ âœ“ âœ“ âœ“ 0.4900 _Â±_ .201 **0.2753**\n",
      "\n",
      "\n",
      "\n",
      "**TABLE 14.** Evaluation of the performance on the DeMSI dataset in the\n",
      "closed-set tattoo segmentation scenario. The results were obtained using the\n",
      "proposed _LLMF CL_ and SegFormer.\n",
      "\n",
      "\n",
      "**Fine-tuned** _Tattoo_ **IoU** **FPR** **FNR**\n",
      "0.3878 0.0234 0.5784\n",
      "âœ“ 0.8232 0.0278 0.0945\n",
      "\n",
      "\n",
      "SegFormer networkâ€™s last layer using the DeMSI dataset.\n",
      "For the fit, was used 60% of the images for training and 40%\n",
      "for testing as proposed by [13]. After this adjustment, our\n",
      "approach proved effective for performing closed-set tattoo\n",
      "segmentation, as evidenced by the significantly reduced\n",
      "FNR values in Table 14. Visually, the improvement in\n",
      "segmentation quality before and after fine-tuning can be\n",
      "observed in Table 15.\n",
      "\n",
      "\n",
      "**TABLE 15.** Visual result samples of closed-set tattoo segmentation on the\n",
      "DeMSI dataset. The tattoo and background pixels are depicted as black and\n",
      "white, respectively. The results were obtained using the proposed _LLMF CL_\n",
      "and SegFormer.\n",
      "\n",
      "|Image<br>Mask<br>Fine-Tuned<br>No<br>Fine-Tuned|Col2|Col3|Col4|Col5|Col6|Col7|\n",
      "|---|---|---|---|---|---|---|\n",
      "|Image<br>Mask<br>Fine-Tuned<br>No<br>Fine-Tuned|||||||\n",
      "|Image<br>Mask<br>Fine-Tuned<br>No<br>Fine-Tuned|||||||\n",
      "|Image<br>Mask<br>Fine-Tuned<br>No<br>Fine-Tuned|||||||\n",
      "\n",
      "\n",
      "\n",
      "**VI. CONCLUSIONS**\n",
      "This paper built a novo tattoo semantic segmentation dataset\n",
      "called TSSD2023, introducing an unexplored and challenging problem in semantic tattoo recognition. This dataset can\n",
      "serve as a valuable basis for future research.\n",
      "Furthermore, the paper has presented the proposed LargeMargin Focal Loss ( _LLMF CL_ ) to enhance tattoo semantic segmentation outcomes in both closed and open-set\n",
      "scenarios. In the closed-set scenario, _LLMF CL_ performed\n",
      "competitively and outperformed other evaluated loss functions in terms of Macro F1, demonstrating its suitability\n",
      "for closed-set semantic segmentation. An in-depth analysis\n",
      "at the class level revealed that superior performance is\n",
      "generally observed in classes depicted by high semantic\n",
      "dissimilarity and minimal overlap with other tattoos within\n",
      "an image. Conversely, the most challenging classes rarely\n",
      "\n",
      "\n",
      "\n",
      "appear in isolation, often serving as complementary components to other tattoos. In the open-set scenario, this paper\n",
      "proposes a generalized approach for the OpenIPCS method\n",
      "named G-OpenIPCS that facilitates the integration of this\n",
      "open-set classifier with more modern segmentation network\n",
      "architectures, such as transform-based networks. Using GOpenIPCS, we compared the performance of different loss\n",
      "functions with the proposed _LLMF CL_, which showed higher\n",
      "scores regarding AUROC, Macro F1, and overall mIoU.\n",
      "When considering only the UUCs, _LLMF CL_ performed\n",
      "competitively in AUROC while outperforming F1-Score\n",
      "and IoU. This highlights its effectiveness in the domain of\n",
      "open-set tattoo semantic segmentation. A more thorough\n",
      "examination revealed that classes bearing high similarity\n",
      "to other classes, overlapped, and with limited samples, it\n",
      "represented the significant challenges for open-set tattoo\n",
      "semantic segmentation.\n",
      "It is worth noting that when a semantic tattoo class has\n",
      "high variability, segmentation errors can occur due to false\n",
      "outliers. To address this issue, we propose a new data augmentation technique named Class Semantic Augmentation\n",
      "(CSA) that increases the available semantic information of\n",
      "classes for better model generalization. Nevertheless, there\n",
      "are opportunities for further advancements in this research\n",
      "area.\n",
      "When comparing our approach, which combines\n",
      "_LLMF CL_, SegFormer, and G-OpenIPCS, with other stateof-the-art methods for open-set tattoo semantic segmentation, our approach consistently attains the highest overall\n",
      "results. It surpasses the performance of other state-of-the-art\n",
      "methods, underscoring its potential for addressing challenging scenarios in open-set semantic tattoo segmentation.\n",
      "Furthermore, our approach applicability extends beyond\n",
      "the domain of tattoo segmentation, and it holds the potential\n",
      "to enhance open-set semantic segmentation in various other\n",
      "application domains. Finally, it is essential to note that while\n",
      "our approach demonstrates promise, there remains room\n",
      "for further improvements in specific areas. This includes\n",
      "improving the handling of class imbalance, class overlap,\n",
      "high intra-class semantic variability, and, in some cases,\n",
      "high inter-class similarity, detailed as follows.\n",
      "\n",
      "\n",
      "**VII. CHALLENGES IN OPEN SET TATTOO SEMANTIC**\n",
      "**SEGMENTATION**\n",
      "Although the approach discussed here is promising and\n",
      "with results superior to state-of-the-art methods for openset semantic segmentation, in the context of tattoos, we can\n",
      "\n",
      "\n",
      "\n",
      "16 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "observe numerous difficulties and open points for this line\n",
      "of research. Below, we detail some of those that we consider\n",
      "most relevant in the proposed context:\n",
      "\n",
      "\n",
      "_**A. SEMANTIC SHIFT**_\n",
      "Semantic shift is caused by classes that influence model\n",
      "predictions due to their semantic similarity to other classes,\n",
      "causing segmentation errors [33], [37]. This semantic shift\n",
      "can be caused in three different ways in the tattoo dataset.\n",
      "The first is the presence of invisible objects in the background class that are similar to known and unknown classes.\n",
      "The background class is a large grouping of objects relatively irrelevant to the application. However, tattoos are\n",
      "semantically very similar to illustrations projected onto\n",
      "different surfaces. For example, a skull on a tattoo artistâ€™s\n",
      "chair can be easily identified as a tattoo, generating a\n",
      "misclassification. Furthermore, due to the great difficulty in\n",
      "annotating a large volume of tattoos for semantic segmentation, some tattoos are â€œignoredâ€ in the labeling process\n",
      "and noted as belonging to the background class, which can\n",
      "cause misclassifications.\n",
      "The second comprises objects of known classes are similar to each other. Some semantic classes are similar to others\n",
      "within the set of known classes. For example, the â€˜wolfâ€™,\n",
      "â€˜dogâ€™, and â€˜foxâ€™ classes belong to the same animal family,\n",
      "thus presenting similar characteristics that even humans may\n",
      "find difficult to distinguish. This high similarity makes the\n",
      "process of recognition by the segmentation model difficult,\n",
      "which can, in some cases, generate classification errors.\n",
      "Furthermore, this semantic similarity can extend to small\n",
      "parts of the tattoos, which can cause small segmentation\n",
      "errors. Finally, the third includes objects of unknown classes\n",
      "similar to known classes. This challenge is similar to the\n",
      "previous one but more demanding. The segmentation model\n",
      "must be trained to produce robust decision boundaries to\n",
      "separate known data from each other and separate it from\n",
      "unknown data. While in the previous problem, it is only\n",
      "necessary to distinguish among known classes.\n",
      "A possible way to alleviate these factorsâ€™ influence is to\n",
      "make a descriptive note of the tattoo using natural language.\n",
      "This can help identify tattoos more effectively, avoiding\n",
      "reducing the artistic feature of a tattoo to a set of labels.\n",
      "\n",
      "\n",
      "_**B. INTRA-CLASS VARIABILITY**_\n",
      "An open challange is also related to objects from known\n",
      "classes with different characteristics from the group of\n",
      "objects from the same semantic class. Because tattoos are a\n",
      "form of artistic expression based on drawings, the number of\n",
      "designs that can be thought of to create a tattoo is practically\n",
      "unlimited. This infinite universe of possibilities makes each\n",
      "tattoo unique, with the variability of tattoos present in the\n",
      "real world being practically immeasurable. When defining a\n",
      "semantic class for a set of objects, we intuitively state that\n",
      "these objects are similar, which is generally true. However,\n",
      "concerning tattoos, it is possible that despite belonging to\n",
      "the same semantic class, practically none of the patterns are\n",
      "\n",
      "\n",
      "\n",
      "identically replicated between tattoos. In some cases, the\n",
      "patterns are so different between objects of the known class\n",
      "that the segmentation model segments the known tattoo as\n",
      "belonging to the unknown class.\n",
      "\n",
      "\n",
      "_**C. OBJECTS OF KNOWN AND UNKNOWN CLASSES**_\n",
      "_**SEGMENTED WITH LOW PRECISION**_\n",
      "This challenge incorporates several subproblems encountered in closed-set segmentation and also in open-set segmentation. In segmenting closed sets, the models suffer\n",
      "from the subproblem of overlap between objects, making it difficult to separate the boundaries between objects\n",
      "accurately. Another common subproblem is small objects\n",
      "present in images, generally classified as objects from other\n",
      "classes, such as the background class. The open-set semantic\n",
      "segmentation task inherits all these subproblems. However,\n",
      "in open-set segmentation, we still have the challenge of accurately classifying regions of completely unknown objects\n",
      "while dealing with the subproblems derived from closed-set\n",
      "segmentation.\n",
      "\n",
      "\n",
      "_**D. DATA AUGMENTATION FOR SEMANTIC**_\n",
      "_**SEGMENTATION**_\n",
      "The data augmentation proposal presented here may be\n",
      "promising for incorporating more data into tattoo datasets,\n",
      "the annotation of which is costly and time-consuming. A\n",
      "possible path is to combine the approach presented in [13]\n",
      "with the data augmentation ideas proposed in this paper in\n",
      "order to create greater variability of classes and examples,\n",
      "including data in an open set, to enable the training of\n",
      "more complex models and, consequently, increase final\n",
      "performance.\n",
      "\n",
      "\n",
      "_**E. COMPUTATIONAL COMPLEXITY**_\n",
      "We understand that a detailed analysis of computational\n",
      "complexity must be conducted for certain applications,\n",
      "mainly involving embedded systems. This analysis must\n",
      "include the training and deployment of the model according\n",
      "to the target device. Specific architectures for this type of\n",
      "application can also be evaluated, as demonstrated in [71].\n",
      "\n",
      "\n",
      "**REFERENCES**\n",
      "\n",
      "\n",
      "[1] S. T. Acton and A. Rossi, â€œMatching and retrieval of tattoo images: Active\n",
      "contour CBIR and glocal image features,â€ in Proceedings of the IEEE\n",
      "Southwest Symposium on Image Analysis and Interpretation, 2008, pp.\n",
      "21â€“24.\n",
      "\n",
      "[2] T. Harbert, â€œFBI wants better automated image analysis for tattoos\n",
      "\n",
      "[news],â€ IEEE Spectrum, vol. 52, no. 9, pp. 13â€“16, 2015.\n",
      "\n",
      "[3] J.-E. Lee, R. Jin, and A. K. Jain, â€œRank-based distance metric learning: An\n",
      "application to image retrieval,â€ in Proceedings of the IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, 2008, pp. 1â€“8.\n",
      "\n",
      "[4] S. Fang, J. Coverdale, P. Nguyen, and M. Gordon, â€œTattoo recognition in\n",
      "screening for victims of human trafficking,â€ Journal of Nervous & Mental\n",
      "Disease, vol. 206, no. 10, pp. 824â€“827, 2018.\n",
      "\n",
      "[5] F. Bacchini and L. Lorusso, â€œA tattoo is not a face. ethical aspects of tattoobased biometrics,â€ Journal of Information, Communication and Ethics in\n",
      "Society, vol. 16, no. 2, pp. 110â€“122, 2017.\n",
      "\n",
      "[6] R. T. Da Silva and H. S. Lopes, â€œA transfer learning approach for the\n",
      "tattoo classification problem,â€ in 2022 IEEE Latin American Conference\n",
      "on Computational Intelligence (LA-CCI). IEEE, 2022, pp. 1â€“6.\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 17\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[7] Y. Mo, Y. Wu, X. Yang, F. Liu, and Y. Liao, â€œReview the state-ofthe-art technologies of semantic segmentation based on deep learning,â€\n",
      "Neurocomputing, vol. 493, pp. 626â€“646, 2022.\n",
      "\n",
      "[8] C. Jiawang and Z. Yuan, â€œTattoo recognition based on triplet GAN,â€ in\n",
      "Proceedings pf 37th Chinese Control Conference. IEEE, 2018, pp. 9595â€“\n",
      "9597.\n",
      "\n",
      "[9] T. HrkaÂ´c, K. BrkiÂ´c, and Z. KalafatiÂ´c, â€œTattoo detection for soft biometric\n",
      "de-identification based on convolutional neural networks,â€ in Proceedings\n",
      "of the OAGM & ARW Joint Workshop. Verlag der Technischen UniversitÃ¤t Graz, 2016.\n",
      "\n",
      "[10] J. Dong, X. Qu, and H. Li, â€œColor tattoo segmentation based on skin\n",
      "color space and k-mean clustering,â€ in Proceedings of the IEEE 4th\n",
      "International Conference on Information, Cybernetics and Computational\n",
      "Social Systems, 2017, pp. 53â€“56.\n",
      "\n",
      "[11] M. NicolÃ¡s-DÃ­az, A. Morales-GonzÃ¡lez, and H. MÃ©ndez-VÃ¡zquez,\n",
      "â€œWeighted average pooling of deep features for tattoo identification,â€\n",
      "Multimedia Tools and Applications, vol. 81, no. 18, pp. 25 853â€“25 875,\n",
      "2022.\n",
      "\n",
      "[12] M. Ngan and P. Grother, â€œTattoo recognition technology - challenge (tattc): an open tattoo database for developing tattoo recognition research,â€ in\n",
      "Proceedings of the IEEE International Conference on Identity, Security\n",
      "and Behavior Analysis, 2015, pp. 1â€“6.\n",
      "\n",
      "[13] L. J. Gonzalez-Soler, C. Rathgeb, and D. Fischer, â€œSemi-synthetic data\n",
      "generation for tattoo segmentation,â€ in 2023 11th International Workshop\n",
      "on Biometrics and Forensics, 2023, pp. 1â€“6.\n",
      "\n",
      "[14] C. Geng, S.-j. Huang, and S. Chen, â€œRecent advances in open set recognition: A survey,â€ IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, vol. 43, no. 10, pp. 3614â€“3631, 2020.\n",
      "\n",
      "[15] C. C. Da Silva, K. Nogueira, H. N. Oliveira, and J. A. dos Santos, â€œTowards\n",
      "open-set semantic segmentation of aerial images,â€ in Proceedings of the\n",
      "IEEE Latin American GRSS & ISPRS Remote Sensing Conference.\n",
      "Santiago, Chile: IEEE, 2020, pp. 16â€“21.\n",
      "\n",
      "[16] I. Nunes, M. B. Pereira, H. Oliveira, J. A. dos Santos, and M. Poggi,\n",
      "â€œConditional reconstruction for open-set semantic segmentation,â€ in Proceedings of the IEEE International Conference on Image Processing.\n",
      "Bordeaux, France: IEEE, 2022, pp. 946â€“950.\n",
      "\n",
      "[17] M. Gutoski, A. E. Lazzaretti, and H. S. Lopes, â€œDeep metric learning\n",
      "for open-set human action recognition in videos,â€ Neural Computing and\n",
      "Applications, vol. 33, pp. 1207â€“1220, 2021.\n",
      "\n",
      "[18] D. Miller, N. Sunderhauf, M. Milford, and F. Dayoub, â€œClass anchor\n",
      "clustering: A loss for distance-based open set recognition,â€ in Proceedings\n",
      "of the IEEE/CVF Winter Conference on Applications of Computer Vision.\n",
      "Waikoloa, USA: IEEE, 2021, pp. 3570â€“3578.\n",
      "\n",
      "[19] W. Liu, Y. Wen, Z. Yu, and M. Yang, â€œLarge-margin softmax loss for\n",
      "convolutional neural networks,â€ arXiv preprint arXiv:1612.02295, 2016.\n",
      "\n",
      "[20] T. Kobayashi, â€œLarge margin in softmax cross-entropy loss,â€ in Proceedings of the British Machine Vision Conference (BMVC), 2019.\n",
      "\n",
      "[21] X. Li, D. Chang, T. Tian, and J. Cao, â€œLarge-margin regularized softmax\n",
      "cross-entropy loss,â€ IEEE access, vol. 7, pp. 19 572â€“19 578, 2019.\n",
      "\n",
      "[22] Q. Xu, S. Ghosh, X. Xu, Y. Huang, and A. W. K. Kong, â€œTattoo detection\n",
      "based on CNN and remarks on the NIST database,â€ in Proceedings of the\n",
      "IEEE International Conference on Biometrics, 2016, pp. 1â€“7.\n",
      "\n",
      "[23] H. Han, J. Li, A. K. Jain, S. Shan, and X. Chen, â€œTattoo image search at\n",
      "scale: Joint detection and compact representation learning,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 10, pp.\n",
      "2333â€“2348, 2019.\n",
      "\n",
      "[24] M. NicolÃ¡s-DÃ­az, A. Morales-GonzÃ¡lez, and H. MÃ©ndez-VÃ¡zquez, â€œDeep\n",
      "generic features for tattoo identification,â€ in Proceedings of the 24th\n",
      "Iberoamerican Congress on Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. Berlin, Heidelberg: SpringerVerlag, 2019, p. 272â€“282.\n",
      "\n",
      "[25] A. Jain, Y. Chen, and U. Park, â€œScars, marks & tattoos (SMT): Physical\n",
      "attributes for person identification,â€ Michigan State University, East Lansing, USA, Tech. Rep. CSE 07-22, 2007.\n",
      "\n",
      "[26] A. K. Jain, J.-E. Lee, and R. Jin, â€œTattoo-ID: Automatic tattoo image\n",
      "retrieval for suspect and victim identification,â€ in Advances in Multimedia\n",
      "Information Processing. Heidelberg: Springer, 2007, pp. 256â€“265.\n",
      "\n",
      "[27] J. D. Allen, N. Zhao, J. Yuan, and X. Liu, â€œUnsupervised tattoo segmentation combining bottom-up and top-down cues,â€ in Proceedings of\n",
      "SPIE Mobile Multimedia/Image Processing, Security, and Applications\n",
      "Conference, S. S. Agaian, S. A. Jassim, and Y. Du, Eds., 2011, pp. 1â€“7.\n",
      "\n",
      "[28] B. Heflin, W. Scheirer, and T. E. Boult, â€œDetecting and classifying scars,\n",
      "marks, and tattoos found in the wild,â€ in Proceedings of the IEEE Fifth In\n",
      "\n",
      "\n",
      "ternational Conference on Biometrics: Theory, Applications and Systems\n",
      "(BTAS), 2012, pp. 31â€“38.\n",
      "\n",
      "[29] P. Duangphasuk and W. Kurutach, â€œTattoo skin detection and segmentation\n",
      "using image negative method,â€ in Proceedings of the IEEE 13th International Symposium on Communications and Information Technologies\n",
      "(ISCIT), 2013, pp. 354â€“359.\n",
      "\n",
      "[30] J. Kim, A. Parra, H. Li, and E. J. Delp, â€œEfficient graph-cut tattoo\n",
      "segmentation,â€ in Visual Information Processing and Communication VI,\n",
      "A. Said, O. G. Guleryuz, and R. L. Stevenson, Eds. SPIE, 2015, pp. 1â€“8.\n",
      "\n",
      "[31] T. HrkaÂ´c, K. B. S. RibariÂ´c, and D. MarË‡cetiÂ´c, â€œDeep learning architectures\n",
      "for tattoo detection and de-identification,â€ in Proceedings of the IEEE\n",
      "First International Workshop on Sensing, Processing and Learning for\n",
      "Intelligent Machines (SPLINE), 2016, pp. 1â€“5.\n",
      "\n",
      "[32] J. Long, E. Shelhamer, and T. Darrell, â€œFully convolutional networks\n",
      "for semantic segmentation,â€ in Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition, 2015, pp. 3431â€“3440.\n",
      "\n",
      "[33] A. Brilhador, A. E. Lazzaretti, and H. S. Lopes, â€œA comparative study\n",
      "for open set semantic segmentation methods,â€ in Anais do 15 Congresso\n",
      "Brasileiro de InteligÃªncia Computacional. Joinville, Brazil: SBIC, 2021,\n",
      "pp. 1â€“8.\n",
      "\n",
      "[34] I. Nunes, C. Laranjeira, H. Oliveira, and J. A. dos Santos, â€œA systematic\n",
      "review on open-set segmentation,â€ Computers & Graphics, vol. 115, pp.\n",
      "296â€“308, 2023.\n",
      "\n",
      "[35] Y. Tian, D. Su, S. Lauria, and X. Liu, â€œRecent advances on loss functions\n",
      "in deep learning for computer vision,â€ Neurocomputing, vol. 497, pp. 129â€“\n",
      "158, 2022.\n",
      "\n",
      "[36] Y. Liu, Y. Tang, L. Zhang, L. Liu, M. Song, K. Gong, Y. Peng, J. Hou, and\n",
      "T. Jiang, â€œHyperspectral open set classification with unknown classes rejection towards deep networks,â€ International Journal of Remote Sensing,\n",
      "vol. 41, no. 16, pp. 6355â€“6383, 2020.\n",
      "\n",
      "[37] Z. Cui, W. Longshi, and R. Wang, â€œOpen set semantic segmentation\n",
      "with statistical test and adaptive threshold,â€ in Proceedings of the IEEE\n",
      "International Conference on Multimedia and Expo, 2020, pp. 1â€“6.\n",
      "\n",
      "[38] H. Oliveira, C. Silva, G. L. Machado, K. Nogueira, and J. A. dos Santos,\n",
      "â€œFully convolutional open set segmentation,â€ Machine Learning, pp. 1â€“52,\n",
      "2021.\n",
      "\n",
      "[39] D. Hendrycks, S. Basart, M. Mazeika, A. Zou, J. Kwon, M. Mostajabi,\n",
      "J. Steinhardt, and D. Song, â€œScaling out-of-distribution detection for\n",
      "real-world settings,â€ in Proceedings of the 39th International Conference\n",
      "on Machine Learning, ser. Proceedings of Machine Learning Research,\n",
      "K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,\n",
      "Eds., vol. 162. PMLR, 17â€“23 Jul 2022, pp. 8759â€“8773.\n",
      "\n",
      "[40] M. GrciÂ´c, P. BevandiÂ´c, and S. Å egviÂ´c, â€œDensehybrid: Hybrid anomaly\n",
      "detection for dense open-set recognition,â€ in European Conference on\n",
      "Computer Vision. Springer, 2022, pp. 500â€“517.\n",
      "\n",
      "[41] X. Guo, J. Liu, T. Liu, and Y. Yuan, â€œHandling open-set noise and\n",
      "novel target recognition in domain adaptive semantic segmentation,â€ IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 8,\n",
      "pp. 9846â€“9861, 2023.\n",
      "\n",
      "[42] J. Cen, P. Yun, J. Cai, M. Y. Wang, and M. Liu, â€œDeep metric learning\n",
      "for open world semantic segmentation,â€ in Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision, 2021, pp. 15 333â€“15 342.\n",
      "\n",
      "[43] J. Hong, W. Li, J. Han, J. Zheng, P. Fang, M. Harandi, and L. Petersson,\n",
      "â€œGoss: Towards generalized open-set semantic segmentation,â€ The Visual\n",
      "Computer, pp. 1â€“14, 2023.\n",
      "\n",
      "[44] H. Zhang and H. Ding, â€œPrototypical matching and open set rejection\n",
      "for zero-shot semantic segmentation,â€ in Procceding of the IEEE/CVF\n",
      "International Conference on Computer Vision, 2021, pp. 6974â€“6983.\n",
      "\n",
      "[45] B. Yu, T. Liu, M. Gong, C. Ding, and D. Tao, â€œCorrecting the triplet\n",
      "selection bias for triplet loss,â€ in Proceedings of the European Conference\n",
      "on Computer Vision (ECCV), 2018, pp. 71â€“87.\n",
      "\n",
      "[46] S. N. Rai, F. Cermelli, B. Caputo, and C. Masone, â€œMask2anomaly:\n",
      "Mask transformer for universal open-set segmentation,â€ arXiv preprint\n",
      "arXiv:2309.04573, 2023.\n",
      "\n",
      "[47] M. J. Wilber, E. Rudd, B. Heflin, Y.-M. Lui, and T. E. Boult, â€œExemplar\n",
      "codes for facial attributes and tattoo recognition,â€ in Proceedings of the\n",
      "IEEE Winter Conference on Applications of Computer Vision, 2014, pp.\n",
      "205â€“212.\n",
      "\n",
      "[48] W. He, J. Wang, L. Wang, R. Pan, and W. Gao, â€œA semantic segmentation\n",
      "algorithm for fashion images based on modified mask rcnn,â€ Multimedia\n",
      "Tools and Applications, vol. 82, no. 18, pp. 28 427â€“28 444, 2023.\n",
      "\n",
      "[49] W. Wan, Y. Zhong, T. Li, and J. Chen, â€œRethinking feature distribution\n",
      "for loss functions in image classification,â€ in Proceedings of the IEEE\n",
      "\n",
      "\n",
      "\n",
      "18 VOLUME 4, 2016\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "\n",
      "\n",
      "\n",
      "conference on computer vision and pattern recognition, 2018, pp. 9117â€“\n",
      "9126.\n",
      "\n",
      "[50] K. Crammer and Y. Singer, â€œOn the algorithmic implementation of multiclass kernel-based vector machines,â€ Journal of machine learning research,\n",
      "vol. 2, no. Dec, pp. 265â€“292, 2001.\n",
      "\n",
      "[51] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. DollÃ¡r, â€œFocal loss for dense\n",
      "object detection,â€ in Proceedings of the IEEE international conference on\n",
      "computer vision, 2017, pp. 2980â€“2988.\n",
      "\n",
      "[52] H.-M. Yang, X.-Y. Zhang, F. Yin, Q. Yang, and C.-L. Liu, â€œConvolutional\n",
      "prototype network for open set recognition,â€ IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence, vol. 44, no. 5, pp. 2358â€“2370, 2022.\n",
      "\n",
      "[53] X. Sun, Z. Yang, C. Zhang, K.-V. Ling, and G. Peng, â€œConditional\n",
      "gaussian distribution learning for open set recognition,â€ in Proceedingsof\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n",
      "2020, pp. 13 480â€“13 489.\n",
      "\n",
      "[54] R. Shwartz-Ziv and N. Tishby, â€œOpening the black box of deep neural\n",
      "networks via information,â€ ArXiv preprint, vol. 1703.00810, 2017.\n",
      "\n",
      "[55] R. K. Srivastava, K. Greff, and J. Schmidhuber, â€œHighway networks,â€\n",
      "ArXiv preprint, vol. 1505.00387, 2015.\n",
      "\n",
      "[56] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, â€œDensely\n",
      "connected convolutional networks,â€ in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, USA:\n",
      "IEEE, 2017, pp. 4700â€“4708.\n",
      "\n",
      "[57] M. E. Tipping and C. M. Bishop, â€œMixtures of probabilistic principal\n",
      "component analyzers,â€ Neural Computation, vol. 11, no. 2, pp. 443â€“482,\n",
      "1999.\n",
      "\n",
      "[58] T. DeVries and G. W. Taylor, â€œImproved regularization of convolutional\n",
      "neural networks with cutout,â€ arXiv preprint arXiv:1708.04552, 2017.\n",
      "\n",
      "[59] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\n",
      "A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, â€œImageNet Large Scale Visual Recognition Challenge,â€ International Journal\n",
      "of Computer Vision, vol. 115, no. 3, pp. 211â€“252, 2015.\n",
      "\n",
      "[60] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, â€œCutmix:\n",
      "Regularization strategy to train strong classifiers with localizable features,â€\n",
      "in Proceedings of the IEEE/CVF international conference on computer\n",
      "vision, 2019, pp. 6023â€“6032.\n",
      "\n",
      "[61] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,\n",
      "T. Xiang, P. H. Torr et al., â€œRethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers,â€ in Proceedings of\n",
      "the IEEE/CVF conference on computer vision and pattern recognition,\n",
      "2021, pp. 6881â€“6890.\n",
      "\n",
      "[62] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, â€œSwin\n",
      "transformer: Hierarchical vision transformer using shifted windows,â€ in\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 012â€“10 022.\n",
      "\n",
      "[63] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, â€œSegformer: Simple and efficient design for semantic segmentation with transformers,â€ Advances in Neural Information Processing Systems, vol. 34,\n",
      "pp. 12 077â€“12 090, 2021.\n",
      "\n",
      "[64] T. Dozat, â€œIncorporating Nesterov Momentum into Adam,â€ in Proceedings\n",
      "of the 4th International Conference on Learning Representations, 2016, pp.\n",
      "1â€“4.\n",
      "\n",
      "[65] H.-M. Yang, X.-Y. Zhang, F. Yin, and C.-L. Liu, â€œRobust classification\n",
      "with convolutional prototype learning,â€ in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3474â€“\n",
      "3482.\n",
      "\n",
      "[66] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, â€œLearning imbalanced\n",
      "datasets with label-distribution-aware margin loss,â€ Advances in neural\n",
      "information processing systems, vol. 32, 2019.\n",
      "\n",
      "[67] M. Yeung, L. Rundo, Y. Nan, E. Sala, C.-B. SchÃ¶nlieb, and G. Yang,\n",
      "â€œCalibrating the dice loss to handle neural network overconfidence for\n",
      "biomedical image segmentation,â€ Journal of Digital Imaging, vol. 36,\n",
      "no. 2, pp. 739â€“752, 2023.\n",
      "\n",
      "[68] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E. Boult, â€œToward\n",
      "open set recognition,â€ IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, vol. 35, no. 7, pp. 1757â€“1772, 2013.\n",
      "\n",
      "[69] A. Bendale and T. E. Boult, â€œTowards open set deep networks,â€ in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "2016, pp. 1563â€“1572.\n",
      "\n",
      "[70] Z. Wang, J. Li, Z. Tan, X. Liu, and M. Li, â€œSwin-upernet: A semantic\n",
      "segmentation model for mangroves and spartina alterniflora loisel based\n",
      "on upernet,â€ Electronics, vol. 12, no. 5, p. 1111, 2023.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[71] B. Olimov, J. Kim, and A. Paul, â€œRef-net: Robust, efficient, and fast\n",
      "network for semantic segmentation applications using devices with limited\n",
      "computational resources,â€ IEEE Access, vol. 9, pp. 15 084â€“15 098, 2021.\n",
      "\n",
      "\n",
      "**ANDERSON BRILHADOR** received the MSc\n",
      "degree in Computer Science from the Federal\n",
      "University of Technology - ParanÃ¡ (UTFPR),\n",
      "Brazil, in 2015. He is pursuing a Ph.D. in Electrical Engineering and Industrial Informatics with\n",
      "UTFPR. He is currently a professor in the computer science program at UTFPR. His research\n",
      "interests include computer vision, machine learning, deep learning, and data mining.\n",
      "\n",
      "\n",
      "**RODRIGO TCHALSKI DA SILVA** received\n",
      "MSc degree in Industrial Computing from the\n",
      "Federal University of Technology             - ParanÃ¡\n",
      "(UTFPR), Brazil, in 2022, and he is currently\n",
      "pursuing a Ph.D. degree in Industrial Computing\n",
      "at the UTFPR. His research interests include computer vision, tattoo recognition, machine learning,\n",
      "and complex networks.\n",
      "\n",
      "\n",
      "**CARLOS ROBERTO MODINEZ-JUNIOR** is\n",
      "currently pursuing an undergraduate degree in\n",
      "Electronic Engineering at Federal University of\n",
      "Technology - ParanÃ¡ (UTFPR), Brazil. He has\n",
      "been working with image processing and his\n",
      "research interests include semantic segmentation.\n",
      "\n",
      "\n",
      "**GABRIEL DE ALMEIDA SPADAFORA** is currently pursuing an undergraduate degree in Computer Engineering at the Federal University of\n",
      "Technology - ParanÃ¡ (UTFPR), Brazil. His research interests include image processing and\n",
      "neural networks.\n",
      "\n",
      "\n",
      "**HEITOR SILVÃ‰RIO LOPES** received the BSc\n",
      "and MSc degree in Electrical Engineering from\n",
      "Federal University of Technology             - ParanÃ¡\n",
      "(UTFPR), in 1984 and 1990, respectively, and\n",
      "his PhD from the Federal University of Santa\n",
      "Catarina in 1996. Currently, he is a tenured full\n",
      "Professor with the Department of Eletronics and\n",
      "the Graduate Program on Electrical Engineering and Applied Computer Science (CPGEI) at\n",
      "UTFPR, Curitiba. His major research interests are\n",
      "in the fields of computer vision, deep learning, evolutionary computation,\n",
      "and data mining.\n",
      "\n",
      "\n",
      "**ANDRÃ‰ EUGÃŠNIO LAZZARETTI (MEMBER,**\n",
      "**IEEE)** received his BSc, MSc, and DSc degrees\n",
      "in Electrical Engineering from the Federal University of Technology - ParanÃ¡ in 2007, 2010\n",
      "and 2015. He is currently a professor with the\n",
      "Department of Electronics at the Federal University of Technology - ParanÃ¡. His research\n",
      "interests include machine learning, deep learning,\n",
      "and digital signal processing.\n",
      "\n",
      "\n",
      "\n",
      "VOLUME 4, 2016 19\n",
      "\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in chunks:\n",
    "    print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
