{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca76f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9707ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações globais do pandas para não truncar dados nos resultados\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862f6086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEste chunking adaptativo é:\\n✔ Rápido (sem LLM)\\n✔ Escalável\\n✔ Ideal para artigos científicos\\n✔ Otimizado para Recall@k\\n✔ Padrão de produção em RAG sério\\n\\nVocê pode agora:\\n- Indexar no FAISS\\n- Avaliar Recall@k\\n- Ajustar target_size empiricamente\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHUNKING ADAPTATIVO PROFISSIONAL PARA ARTIGOS CIENTÍFICOS\n",
    "========================================================\n",
    "\n",
    "Objetivo:\n",
    "- Criar chunks semânticos de alta qualidade\n",
    "- Otimizar Recall@k para queries em artigos científicos\n",
    "- NÃO usar LLM (rápido, reprodutível, escalável)\n",
    "- Respeitar limites de tamanho (<= 1000 caracteres)\n",
    "- Preservar estrutura científica (seções, parágrafos)\n",
    "\n",
    "Pipeline:\n",
    "1) Limpeza básica do texto\n",
    "2) Detecção de seções científicas\n",
    "3) Split em parágrafos\n",
    "4) Agrupamento adaptativo por tamanho\n",
    "5) Overlap semântico por parágrafo\n",
    "6) Geração de Documents prontos para RAG\n",
    "\n",
    "Python recomendado: 3.11\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. CONFIGURAÇÕES GLOBAIS\n",
    "# ============================================================\n",
    "\n",
    "# Limites de tamanho\n",
    "CHUNK_TARGET_SIZE = 800     # alvo ideal\n",
    "CHUNK_MAX_SIZE = 1000       # limite rígido\n",
    "\n",
    "# Overlap semântico (em parágrafos)\n",
    "OVERLAP_PARAGRAPHS = 1\n",
    "\n",
    "# Cabeçalhos científicos comuns\n",
    "SECTION_HEADERS = [\n",
    "    \"abstract\",\n",
    "    \"introduction\",\n",
    "    \"background\",\n",
    "    \"related work\",\n",
    "    \"related works\",\n",
    "    \"available datasets\",\n",
    "    \"methodology\",\n",
    "    \"proposed methods\",\n",
    "    \"methods\",\n",
    "    \"materials and methods\",\n",
    "    \"experiments\",\n",
    "    \"results\",\n",
    "    \"discussion\",\n",
    "    \"results and discussions\",\n",
    "    \"conclusion\",\n",
    "    \"conclusions\",\n",
    "    \"future work\",\n",
    "    \"references\",\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. LIMPEZA BÁSICA DO TEXTO\n",
    "# ============================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto extraído de PDF:\n",
    "    - Remove espaços excessivos\n",
    "    - Corrige quebras estranhas\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. DETECÇÃO DE SEÇÕES\n",
    "# ============================================================\n",
    "\n",
    "def split_by_sections(text: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Divide o texto em seções científicas detectadas via regex.\n",
    "\n",
    "    Retorna:\n",
    "    [\n",
    "        {\n",
    "            \"section\": \"introduction\",\n",
    "            \"content\": \"texto da seção\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    pattern = r\"(?i)\\n(\" + \"|\".join(SECTION_HEADERS) + r\")\\n\"\n",
    "    parts = re.split(pattern, text)\n",
    "\n",
    "    sections = []\n",
    "    current_section = \"unknown\"\n",
    "\n",
    "    for part in parts:\n",
    "        part_clean = part.strip()\n",
    "\n",
    "        if part_clean.lower() in SECTION_HEADERS:\n",
    "            current_section = part_clean.lower()\n",
    "        elif part_clean:\n",
    "            sections.append(\n",
    "                {\n",
    "                    \"section\": current_section,\n",
    "                    \"content\": part_clean,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. SPLIT EM PARÁGRAFOS\n",
    "# ============================================================\n",
    "\n",
    "def split_into_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide texto em parágrafos válidos.\n",
    "    Ignora parágrafos muito curtos.\n",
    "    \"\"\"\n",
    "    paragraphs = [\n",
    "        p.strip()\n",
    "        for p in text.split(\"\\n\\n\")\n",
    "        if len(p.strip()) >= 50\n",
    "    ]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. AGRUPAMENTO ADAPTATIVO DE PARÁGRAFOS\n",
    "# ============================================================\n",
    "\n",
    "def adaptive_group_paragraphs(\n",
    "    paragraphs: List[str],\n",
    "    target_size: int = CHUNK_TARGET_SIZE,\n",
    "    max_size: int = CHUNK_MAX_SIZE,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Agrupa parágrafos respeitando:\n",
    "    - Não quebrar parágrafo\n",
    "    - Alvo de tamanho\n",
    "    - Limite máximo rígido\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        candidate = (\n",
    "            current_chunk + \"\\n\\n\" + paragraph\n",
    "            if current_chunk\n",
    "            else paragraph\n",
    "        )\n",
    "\n",
    "        if len(candidate) <= max_size:\n",
    "            current_chunk = candidate\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = paragraph\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. OVERLAP SEMÂNTICO POR PARÁGRAFO\n",
    "# ============================================================\n",
    "\n",
    "def apply_paragraph_overlap(\n",
    "    chunks: List[str],\n",
    "    overlap_paragraphs: int = OVERLAP_PARAGRAPHS,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Aplica overlap reutilizando os últimos parágrafos\n",
    "    do chunk anterior.\n",
    "    \"\"\"\n",
    "    if overlap_paragraphs <= 0:\n",
    "        return chunks\n",
    "\n",
    "    overlapped_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i == 0:\n",
    "            overlapped_chunks.append(chunk)\n",
    "            continue\n",
    "\n",
    "        prev_paragraphs = chunks[i - 1].split(\"\\n\\n\")\n",
    "        overlap = \"\\n\\n\".join(prev_paragraphs[-overlap_paragraphs:])\n",
    "\n",
    "        combined = overlap + \"\\n\\n\" + chunk\n",
    "        overlapped_chunks.append(combined)\n",
    "\n",
    "    return overlapped_chunks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. PIPELINE PRINCIPAL DE CHUNKING ADAPTATIVO\n",
    "# ============================================================\n",
    "\n",
    "def adaptive_chunking(text: str, source_name: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Executa o pipeline completo de chunking adaptativo\n",
    "    e retorna Documents prontos para RAG.\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    sections = split_by_sections(text)\n",
    "\n",
    "    documents = []\n",
    "    global_chunk_id = 0\n",
    "\n",
    "    for section_data in sections:\n",
    "        section_name = section_data[\"section\"]\n",
    "        section_text = section_data[\"content\"]\n",
    "\n",
    "        paragraphs = split_into_paragraphs(section_text)\n",
    "        base_chunks = adaptive_group_paragraphs(paragraphs)\n",
    "        final_chunks = apply_paragraph_overlap(base_chunks)\n",
    "\n",
    "        for chunk in final_chunks:\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": source_name,\n",
    "                        \"section\": section_name,\n",
    "                        \"chunk_id\": global_chunk_id,\n",
    "                        \"chunking\": \"adaptive\",\n",
    "                        \"length\": len(chunk),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            global_chunk_id += 1\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. EXEMPLO DE USO\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "text = load_pdf(\"artigo_cientifico.pdf\")\n",
    "\n",
    "documents = adaptive_chunking(\n",
    "    text=text,\n",
    "    source_name=\"artigo_cientifico.pdf\"\n",
    ")\n",
    "\n",
    "print(f\"Total de chunks gerados: {len(documents)}\")\n",
    "\n",
    "# Visualizar um chunk\n",
    "print(documents[0].metadata)\n",
    "print(documents[0].page_content[:500])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. CONCLUSÃO PROFISSIONAL\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Este chunking adaptativo é:\n",
    "✔ Rápido (sem LLM)\n",
    "✔ Escalável\n",
    "✔ Ideal para artigos científicos\n",
    "✔ Otimizado para Recall@k\n",
    "✔ Padrão de produção em RAG sério\n",
    "\n",
    "Você pode agora:\n",
    "- Indexar no FAISS\n",
    "- Avaliar Recall@k\n",
    "- Ajustar target_size empiricamente\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3753dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks gerados: 6\n",
      "{'source': 'tattoo.pdf', 'section': 'unknown', 'chunk_id': 0, 'chunking': 'adaptive', 'length': 11933}\n",
      "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n",
      "Digital Object Identifier 10.1109/ACCESS.2017.DOI\n",
      "Open-Set Tattoo Semantic Segmentation\n",
      "ANDERSON BRILHADOR 1, RODRIGO TCHALSKI DA SIL VA 1, CARLOS ROBERTO\n",
      "MODINEZ-JUNIOR 1, GABRIEL DE ALMEIDA SP ADAFORA 1, HEITOR SIL VÉRIO\n",
      "LOPES 1, AND ANDRÉ EUGÊNIO LAZZARETTI 1, (Member, IEEE).\n",
      "1Federal University of Technology - Paraná, Av. Sete de Setembro, 3165, Curitiba, 80230-901, Paraná, Brazil.\n",
      "Corresponding author: Anderson Brilhador (e-mail: andersonbrilhador@gmail.com).\n",
      "ABSTRACT Tattoos can serve as an essential source of biometric information for public security, aiding\n",
      "in identifying suspects and victims. In order to automate tattoo classification, tasks like classification\n",
      "require more detailed image content analysis, such as semantic segmentation. However, a dataset with\n",
      "appropriate semantic segmentation annotations is currently lacking. Also, there are countless ways to\n",
      "categorize tattoo classes, and many are not directly categorizable, either because they belong to a specific\n",
      "artistic trait or characterize an object with previously undefined semantics. An effective way to overcome\n",
      "these limitations is to build recognition systems based on open-set assumptions. Nevertheless, state-of-\n",
      "the-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to\n",
      "the significant class imbalance (predominant background). To the best of our knowledge, this paper is\n",
      "the first to explore semantic segmentation in closed and open-set scenarios for tattoos. In this sense, this\n",
      "paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set\n",
      "classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible\n",
      "test set, enabling comparisons and future research in this area. The proposed approach outperforms other\n",
      "methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU\n",
      "for the unknown class, demonstrating the feasibility of this approach for automatic tattoo analysis. The\n",
      "paper also highlights key limitations and open research areas in this challenging field. Dataset and codes\n",
      "are available at https://github.com/Brilhador/tssd2023.\n",
      "INDEX TERMS open-world, open-set, semantic segmentation, large-margin learning, tattoo classifica-\n",
      "tion.\n",
      "I. INTRODUCTION\n",
      "Tattoos are forms of human expression and are also con-\n",
      "sidered an art. In their almost unique features, tattoos\n",
      "go beyond artistic expressions and can serve as essential\n",
      "sources of biometric information. Consequently, it can be\n",
      "useful in identifying their bearers, mainly for public security\n",
      "[1], [2] because tattoos can be used to identify not only\n",
      "suspects but also victims [3], [4]. In addition, the subject\n",
      "has raised studies on ethical and social issues that may\n",
      "encompass the topic [5].\n",
      "Compared to other biometrics, tattoos bring a series of\n",
      "characteristics that make them very difficult to recognize.\n",
      "Other biometrics usually have well-defined standards, robust\n",
      "techniques, well-established methods for their treatment and\n",
      "recognition, standardized data capture and storage, and other\n",
      "factors that help their reliability and robustness. However,\n",
      "tattoos still need to have such characteristics and require-\n",
      "ments. Apart from the issues related to processing and\n",
      "using general biometrics, tattoo recognition has a singular\n",
      "complexity because it can be divided into several sub-\n",
      "problems, each equally significant [6].\n",
      "First, an image can be submitted to detect, locate, and\n",
      "segment (outline or instance) the tattoo contained therein.\n",
      "Subsequently, the image can be classified, de-identified,\n",
      "or re-identified (image-to-image, sketch-to-image, partial,\n",
      "or similar). After preprocessing an image, the best result\n",
      "could be a well-segmented tattoo without any pollution or\n",
      "background. Then, for all classification, de-identification, or\n",
      "re-identification tasks, these images contain only the most\n",
      "essential information to store and, later, process [6].\n",
      "Nonetheless, tasks that consider the meaning of the\n",
      "content of images, such as classification, may require a\n",
      "more detailed separation of objects in a tattoo image, called\n",
      "semantic segmentation [7]. At this point, a segmented tattoo\n",
      "could identify and detach each object in the tattoo, after\n",
      "which each could be analyzed separately. For instance,\n",
      "security and biometric recognition systems could benefit\n",
      "from tattoo semantic split at the pixel-level for fine-grained\n",
      "VOLUME 4, 2016 1\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "feature extraction by semantic classes, reduce false posi-\n",
      "tives by precisely delineating the boundaries of biometrics\n",
      "features, and allow selective anonymization of regions of an\n",
      "image. Moreover, tattoo biometrics systems may require the\n",
      "identification of multiple semantic classes in multiple areas\n",
      "of the image, which can be performed accurately by tattoo\n",
      "semantic segmentations.\n",
      "Although this topic has been widely explored in studies\n",
      "related to images and videos in many different areas, se-\n",
      "mantic segmentation is still underexplored in the context\n",
      "of tattoos. Several works only focus on tattoo classification\n",
      "and detection [6], [8], and only a few researches deal with\n",
      "segmentation [9], [10] without identifying the semantics of\n",
      "the components that compose the tattoos.\n",
      "One of the reasons that make semantic segmentation dif-\n",
      "ficult is related to the complexity that tattooing can have. As\n",
      "mentioned, tattoos are expressions of art, and their features\n",
      "can be as varied as possible and imaginable. In this way, ob-\n",
      "jects can be positioned very closely, mixed, overlapped, and\n",
      "distorted, and abstract images can also be present, among\n",
      "many other hindering factors [11]. Additionally, the lack\n",
      "of public and comprehensive datasets makes it even more\n",
      "challenging to develop efficient methods for segmentation.\n",
      "Some recent works propose public datasets, such as [12]\n",
      "and [13]. However, in the case of [13], using semi-synthetic\n",
      "images without employing the semantics associated with\n",
      "each class makes it difficult to generalize the proposed –\n",
      "characteristic also observed in [12].\n",
      "Still, in this context, it is essential to emphasize the\n",
      "complexity and semantic variability of tattoo classes. Tattoo\n",
      "categories can vary significantly, and some are not easily cat-\n",
      "egorized due to their association with specific artistic styles\n",
      "or objects with undefined semantics. The tattoo recognition\n",
      "scenario, especially from a public safety perspective, is also\n",
      "somewhat challenging, especially given the circumstances\n",
      "in which the information is obtained and analyzed. It is not\n",
      "uncommon for tattoo information to be obtained partially,\n",
      "and, therefore, semantic segmentation has great relevance\n",
      "in the identification process, as in the following scenarios:\n",
      "(i) semantic segmentation can be used to create databases\n",
      "with automatic textual annotations, as it is common for a\n",
      "witness or victim to remember or have visual contact with\n",
      "only parts of a tattoo of a wanted person, and, in this way,\n",
      "from the description, it would be possible to identify tattoos\n",
      "with those parts visualized; (ii) semantic segmentation is\n",
      "important in the pre-processing of images in preparation\n",
      "for information recognition processes, such as for partial\n",
      "re-identification of tattoos, wherein an automatic process,\n",
      "the segmented parts can be recovered separately in cases of\n",
      "partial image collections [6].\n",
      "An effective way to overcome these limitations is to\n",
      "build recognition systems based on dynamic and open-set\n",
      "perception. These systems are designed to handle objects\n",
      "from unknown classes commonly encountered in real-world\n",
      "applications. Open-set recognition has extensively studied\n",
      "the ability to recognize new classes [14]. Open-set semantic\n",
      "segmentation, in turn, is an approach that incorporates open-\n",
      "set perception into semantic segmentation. The main differ-\n",
      "ence with closed-set semantic segmentation is that open-\n",
      "set semantic segmentation must correctly classify samples\n",
      "belonging to known classes while rejecting those belonging\n",
      "to unknown classes. In the context of this work, open-set\n",
      "semantic segmentation can be an ally in improving databases\n",
      "and models for identifying and classifying tattoo objects,\n",
      "allowing the improvement of annotations and descriptions\n",
      "of complex tattoos. Therefore, semantic segmentation must\n",
      "also be seen as a middle process, not just as an end process\n",
      "in the tattoo recognition roadmap.\n",
      "Studies have explored the use of open-set semantic seg-\n",
      "mentation in different applications [15], [16]. These studies\n",
      "focus on adapting or building open-set classifiers to make\n",
      "closed-set semantic segmentation models capable of recog-\n",
      "nizing unknown classes. While the outcomes of these studies\n",
      "are promising, the performance of these approaches is lim-\n",
      "ited due to the low representation of the obtained features,\n",
      "resulting in an “irregular” logit space with low discrimi-\n",
      "nation among the classes. Recent research [17], [18] has\n",
      "demonstrated that incorporating metric learning techniques\n",
      "can enhance open-set recognition. Metric learning aids in\n",
      "obtaining more discriminative features and building a logit\n",
      "space that tightly clusters known classes while maintaining\n",
      "a considerable distance from unknown classes. However, it\n",
      "is essential to acknowledge that applying metric learning\n",
      "in the context of semantic segmentation can be impractical.\n",
      "This is primarily due to the exponential complexity of the\n",
      "task, as calculating pairwise distances among logit vectors\n",
      "of pixels becomes computationally expensive.\n",
      "Recent studies have investigated the potential of large-\n",
      "margin learning to acquire more discriminative fea-\n",
      "tures, yielding improved outcomes in image classification\n",
      "tasks [19]–[21]. Hence, strategies established on large-\n",
      "margin learning present promising and viable alternatives\n",
      "for building a well-defined logit space that enhances the\n",
      "separation among decision boundaries of semantic classes.\n",
      "Motivated by these results, our study explores the dis-\n",
      "criminative capabilities of large-margin learning to produce\n",
      "more distinctive features for tattoo semantic segmentation.\n",
      "This approach effectively increases the spatial separation\n",
      "among decision boundaries to different semantic classes,\n",
      "forcing the build of ideal logit space as illustrated in\n",
      "Figure 1b. Furthermore, this expanded separation among\n",
      "decision boundaries will set the stage for accommodating\n",
      "unknown classes in the future, enhancing the effectiveness\n",
      "of tattoo semantic segmentation within an open-set scenario.\n",
      "Visual comparisons depicting the differences in logit space\n",
      "resulting from the presence of unknown classes can be\n",
      "observed in Figure 1.\n",
      "Given the limitations presented so far and the fact that,\n",
      "to the extent of our knowledge, open-set classification has\n",
      "not been used in the context of tattoo recognition and is\n",
      "an open research gap, this work aims to propose a new\n",
      "large-margin-based loss function adapted to the context of\n",
      "2 VOLUME 4, 2016\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \n",
      "content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3438557\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
      "Logit Space Label Space\n",
      "Closed Set \n",
      "Segmentation\n",
      "Closed Set \n",
      "Segmentation with \n",
      "Unknown classes\n",
      "Open Set \n",
      "Segmentation with \n",
      "“Ideal” Logit Space\n",
      "(a) (b) (c) (d)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def load_pdf(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "text = load_pdf(\"datasets/tattoo.pdf\")\n",
    "\n",
    "documents = adaptive_chunking(\n",
    "    text=text,\n",
    "    source_name=\"tattoo.pdf\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414d1318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Chunk global index: 0\n",
      "Metadata:\n",
      "  source: tattoo.pdf\n",
      "  section: unknown\n",
      "  chunk_id: 0\n",
      "  chunking: adaptive\n",
      "  length: 11933\n",
      "--------------------------------------------------------------------------------\n",
      "Conteúdo (início do chunk):\n",
      "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n",
      "Digital Object Identifier 10.1109/ACCESS.2017.DOI\n",
      "Open-Set Tattoo Semantic Segmentation\n",
      "ANDERSON BRILHADOR 1, RODRIGO TCHALSKI DA SIL VA 1, CARLOS ROBERTO\n",
      "MODINEZ-JUNIOR 1, GABRIEL DE ALMEIDA SP ADAFORA 1, HEITOR SIL VÉRIO\n",
      "LOPES 1, AND ANDRÉ EUGÊNIO LAZZARETTI 1, (Member, IEEE).\n",
      "1Federal University of Technology - Paraná, Av. Sete de Setembro, 3165, Curitiba, 80230-901, Paraná, Brazil.\n",
      "Corresponding author: Anderson Brilha\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Chunk global index: 1\n",
      "Metadata:\n",
      "  source: tattoo.pdf\n",
      "  section: background\n",
      "  chunk_id: 1\n",
      "  chunking: adaptive\n",
      "  length: 145\n",
      "--------------------------------------------------------------------------------\n",
      "Conteúdo (início do chunk):\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "Unknown \n",
      "classes\n",
      "E.g. Bee. was \n",
      "defined as \n",
      "Unknown Class.\n",
      "Open Set \n",
      "Segmentation with \n",
      "“Irregular” Logit Space\n",
      "Stem\n",
      "Leaf\n",
      "Flower\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Chunk global index: 2\n",
      "Metadata:\n",
      "  source: tattoo.pdf\n",
      "  section: background\n",
      "  chunk_id: 2\n",
      "  chunking: adaptive\n",
      "  length: 108\n",
      "--------------------------------------------------------------------------------\n",
      "Conteúdo (início do chunk):\n",
      "Building\n",
      "Imp. Surfaces\n",
      "Car\n",
      "Tree\n",
      "Unk. Class\n",
      "Open Space\n",
      "Flower Stem\n",
      "Leaf\n",
      "Unk. ClassBackground\n",
      "Flower\n",
      "Leaf\n",
      "Stem\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Chunk global index: 3\n",
      "Metadata:\n",
      "  source: tattoo.pdf\n",
      "  section: background\n",
      "  chunk_id: 3\n",
      "  chunking: adaptive\n",
      "  length: 23376\n",
      "--------------------------------------------------------------------------------\n",
      "Conteúdo (início do chunk):\n",
      "Background\n",
      "FIGURE 1. Closed set and open set in dense labeling scenarios. The label space represents the pixel-level predictions, and logit space refers to a subset of pixel\n",
      "samples in a 2D manifold separated by labels and decision boundaries for each class. a) Closed set without the presence of unknown classes. b) Closed set with\n",
      "the presence of unknown classes. These unknown classes result in misclassification, being segmented as known classes. c) Open set segmentation with “irregular”\n",
      "logit s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Chunk global index: 4\n",
      "Metadata:\n",
      "  source: tattoo.pdf\n",
      "  section: background\n",
      "  chunk_id: 4\n",
      "  chunking: adaptive\n",
      "  length: 56269\n",
      "--------------------------------------------------------------------------------\n",
      "Conteúdo (início do chunk):\n",
      "Heart\n",
      "Mermaid\n",
      "Skull\n",
      "Star\n",
      "FIGURE 2. TSSD2023 Classes. KKCs tattoos correspond to the 33 labeled classes, while the UUCs tattoos are represented by the 23 classes defined as\n",
      "“unknown” (black label). These classes can be viewed within a conceptual taxonomy, facilitating an understanding of the domain coverage provided by the dataset.\n",
      "0,00%\n",
      "1,00%\n",
      "2,00%\n",
      "3,00%\n",
      "4,00%\n",
      "5,00%\n",
      "tigerskulleaglewolf lion\n",
      "octopus\n",
      "owl cat\n",
      "flowerscorpion\n",
      "fish dogsnakeribbon\n",
      "bird\n",
      "mermaidbutterflycrownanchorsharkgunheartwater\n",
      "fox \n",
      "\n",
      "\n",
      "================================================================================\n",
      "Chunk global index: 5\n",
      "Metadata:\n",
      "  source: tattoo.pdf\n",
      "  section: references\n",
      "  chunk_id: 5\n",
      "  chunking: adaptive\n",
      "  length: 18564\n",
      "--------------------------------------------------------------------------------\n",
      "Conteúdo (início do chunk):\n",
      "[1] S. T. Acton and A. Rossi, “Matching and retrieval of tattoo images: Active\n",
      "contour CBIR and glocal image features,” in Proceedings of the IEEE\n",
      "Southwest Symposium on Image Analysis and Interpretation, 2008, pp.\n",
      "21–24.\n",
      "[2] T. Harbert, “FBI wants better automated image analysis for tattoos\n",
      "[news],” IEEE Spectrum, vol. 52, no. 9, pp. 13–16, 2015.\n",
      "[3] J.-E. Lee, R. Jin, and A. K. Jain, “Rank-based distance metric learning: An\n",
      "application to image retrieval,” in Proceedings of the IEEE Conference\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ITERAR POR TODOS OS CHUNKS GERADOS\n",
    "# ============================================================\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Chunk global index: {i}\")\n",
    "    print(\"Metadata:\")\n",
    "    for k, v in doc.metadata.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Conteúdo (início do chunk):\")\n",
    "    print(doc.page_content[:500])  # limita para não poluir o terminal\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9232a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITERAR APENAS CHUNKS DE UMA SEÇÃO ESPECÍFICA\n",
    "# ============================================================\n",
    "\n",
    "target_section = \"abstract\"\n",
    "\n",
    "for doc in documents:\n",
    "    if doc.metadata.get(\"section\") == target_section:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Section: {target_section}\")\n",
    "        print(f\"Chunk ID: {doc.metadata['chunk_id']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(doc.page_content[:500])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6133f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 6\n",
      "Tamanho médio: 18399.2\n",
      "Menor chunk: 108\n",
      "Maior chunk: 56269\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INSPEÇÃO RÁPIDA DE QUALIDADE DOS CHUNKS\n",
    "# ============================================================\n",
    "\n",
    "lengths = [len(doc.page_content) for doc in documents]\n",
    "\n",
    "print(f\"Total de chunks: {len(lengths)}\")\n",
    "print(f\"Tamanho médio: {sum(lengths) / len(lengths):.1f}\")\n",
    "print(f\"Menor chunk: {min(lengths)}\")\n",
    "print(f\"Maior chunk: {max(lengths)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
