{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d586db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268cbf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 21:50:23,473 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-11 21:50:23,498 - INFO - Going to convert document batch...\n",
      "2026-01-11 21:50:23,499 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-11 21:50:23,500 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,536 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,552 [RapidOCR] download_file.py:60: File exists and is valid: /home/clayton/miniconda3/envs/ml311/lib/python3.11/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,553 [RapidOCR] main.py:53: Using /home/clayton/miniconda3/envs/ml311/lib/python3.11/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,641 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,643 [RapidOCR] download_file.py:60: File exists and is valid: /home/clayton/miniconda3/envs/ml311/lib/python3.11/site-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,644 [RapidOCR] main.py:53: Using /home/clayton/miniconda3/envs/ml311/lib/python3.11/site-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,690 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Carregando: datasets/tattoo.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-01-11 21:50:23,726 [RapidOCR] download_file.py:60: File exists and is valid: /home/clayton/miniconda3/envs/ml311/lib/python3.11/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-11 21:50:23,727 [RapidOCR] main.py:53: Using /home/clayton/miniconda3/envs/ml311/lib/python3.11/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-01-11 21:50:23,829 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2026-01-11 21:50:23,829 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-11 21:50:24,647 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-11 21:50:25,125 - INFO - Processing document tattoo.pdf\n",
      "\u001b[33m[WARNING] 2026-01-11 21:50:34,887 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-11 21:50:34,887 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-11 21:50:45,122 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-11 21:50:46,521 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-11 21:50:48,341 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-11 21:50:48,342 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-11 21:50:54,877 - INFO - Finished converting document tattoo.pdf in 31.41 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 109 chunks criados\n",
      "üîÆ Criando vectorstore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 21:51:03,250 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vectorstore pronto!\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG OTIMIZADO PRONTO!\n",
      "================================================================================\n",
      "\n",
      "Uso b√°sico:\n",
      "  ask(vectorstore, 'sua pergunta')\n",
      "\n",
      "Uso avan√ßado:\n",
      "  ask(vectorstore, 'sua pergunta', k=8, score_threshold=0.6)\n",
      "\n",
      "Diagn√≥stico:\n",
      "  diagnostico(vectorstore, 'sua pergunta')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ÉO DO OLLAMA\n",
    "# ==============================================================================\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model='gemma3:12b',\n",
    "    temperature=0.3,  # Aumentado para respostas menos rob√≥ticas\n",
    "    num_ctx=8192,     # Contexto maior para processar mais chunks\n",
    "    num_predict=1024, # Mais tokens para respostas completas\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# CARREGAR PDF COM DOCLING (MELHORADO)\n",
    "# ==============================================================================\n",
    "def load_pdf(pdf_path):\n",
    "    \"\"\"Carrega PDF preservando estrutura com Docling.\"\"\"\n",
    "    print(f\"üìÑ Carregando: {pdf_path}\")\n",
    "    \n",
    "    # Converter PDF para markdown preservando estrutura\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(pdf_path)\n",
    "    markdown = result.document.export_to_markdown()\n",
    "    \n",
    "    # Criar documento\n",
    "    doc = Document(page_content=markdown, metadata={\"source\": pdf_path})\n",
    "    \n",
    "    # Chunking otimizado para markdown\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,      # Chunks maiores para mais contexto\n",
    "        chunk_overlap=300,    # Overlap maior para n√£o perder contexto\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Respeita estrutura markdown\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = splitter.split_documents([doc])\n",
    "    \n",
    "    print(f\"‚úÖ {len(chunks)} chunks criados\")\n",
    "    return chunks\n",
    "\n",
    "# ==============================================================================\n",
    "# CRIAR VECTORSTORE\n",
    "# ==============================================================================\n",
    "def create_vectorstore(chunks):\n",
    "    \"\"\"Cria vectorstore com embeddings.\"\"\"\n",
    "    print(\"üîÆ Criando vectorstore...\")\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    print(\"‚úÖ Vectorstore pronto!\")\n",
    "    return vectorstore\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZER PERGUNTA COM PROMPT OTIMIZADO\n",
    "# ==============================================================================\n",
    "def ask(vectorstore, query, k=5, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Faz pergunta com prompt otimizado.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: O vectorstore com os documentos\n",
    "        query: A pergunta\n",
    "        k: N√∫mero de chunks a recuperar (padr√£o: 5)\n",
    "        score_threshold: Limiar de similaridade (0-1, padr√£o: 0.5 = 50%)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç {query}\\n\")\n",
    "    \n",
    "    # Buscar mais chunks com filtro de qualidade\n",
    "    docs_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Filtrar apenas chunks relevantes (score baixo = alta similaridade)\n",
    "    # Score < 1.0 geralmente indica boa relev√¢ncia\n",
    "    filtered_docs = [(doc, score) for doc, score in docs_scores if score < (2 - score_threshold*2)]\n",
    "    \n",
    "    if not filtered_docs:\n",
    "        print(\"‚ö†Ô∏è Nenhum trecho relevante encontrado. Tentando com threshold mais baixo...\")\n",
    "        filtered_docs = docs_scores[:3]  # Pega os 3 melhores mesmo assim\n",
    "    \n",
    "    # Montar contexto numerado para melhor rastreabilidade\n",
    "    context_parts = []\n",
    "    for i, (doc, _) in enumerate(filtered_docs, 1):\n",
    "        context_parts.append(f\"[Trecho {i}]\\n{doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Prompt otimizado para respostas melhores\n",
    "    prompt = f\"\"\"Voc√™ √© um assistente especializado em an√°lise de documentos. Use APENAS as informa√ß√µes dos trechos fornecidos para responder.\n",
    "\n",
    "TRECHOS DO DOCUMENTO:\n",
    "{context}\n",
    "\n",
    "INSTRU√á√ïES:\n",
    "- Responda de forma clara, completa e estruturada\n",
    "- Cite os trechos espec√≠ficos quando relevante (ex: \"Conforme o Trecho 2...\")\n",
    "- Se a informa√ß√£o n√£o estiver nos trechos, diga claramente que n√£o encontrou\n",
    "- Use exemplos e detalhes dos trechos quando dispon√≠veis\n",
    "- Organize a resposta em par√°grafos ou t√≥picos quando apropriado\n",
    "\n",
    "PERGUNTA: {query}\n",
    "\n",
    "RESPOSTA:\"\"\"\n",
    "    \n",
    "    # Obter resposta\n",
    "    response = llm.invoke(prompt).content\n",
    "    \n",
    "    # Exibir resposta\n",
    "    print(\"üí¨ Resposta:\")\n",
    "    print(\"=\"*80)\n",
    "    print(response)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Exibir chunks com scores\n",
    "    print(f\"\\nüìö {len(filtered_docs)} Chunks utilizados:\\n\")\n",
    "    for i, (doc, score) in enumerate(filtered_docs, 1):\n",
    "        similarity = max(0, (1 - score/2) * 100)\n",
    "        preview = doc.page_content[:].replace('\\n', ' ')\n",
    "        \n",
    "        # Indicador visual de qualidade\n",
    "        quality = \"üü¢\" if similarity > 70 else \"üü°\" if similarity > 50 else \"üî¥\"\n",
    "        \n",
    "        print(f\"{quality} Chunk {i}: {similarity:.1f}% similar\")\n",
    "        print(f\"   {preview}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"chunks\": filtered_docs,\n",
    "        \"num_chunks_used\": len(filtered_docs)\n",
    "    }\n",
    "\n",
    "# ==============================================================================\n",
    "# FUN√á√ÉO DE DIAGN√ìSTICO\n",
    "# ==============================================================================\n",
    "def diagnostico(vectorstore, query, k=10):\n",
    "    \"\"\"Mostra os top K chunks para diagn√≥stico sem fazer pergunta ao LLM.\"\"\"\n",
    "    print(f\"\\nüî¨ DIAGN√ìSTICO: '{query}'\\n\")\n",
    "    \n",
    "    docs_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(docs_scores, 1):\n",
    "        similarity = max(0, (1 - score/2) * 100)\n",
    "        quality = \"üü¢\" if similarity > 70 else \"üü°\" if similarity > 50 else \"üî¥\"\n",
    "        \n",
    "        print(f\"{quality} Chunk {i}: {similarity:.1f}% (score: {score:.4f})\")\n",
    "        print(f\"Conte√∫do: {doc.page_content[:300]}...\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# SETUP (EXECUTE ESTA C√âLULA UMA VEZ)\n",
    "# ==============================================================================\n",
    "pdf_path = \"datasets/tattoo.pdf\"  # ‚Üê MUDE AQUI\n",
    "\n",
    "# Carregar e processar\n",
    "chunks = load_pdf(pdf_path)\n",
    "vectorstore = create_vectorstore(chunks)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ RAG OTIMIZADO PRONTO!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nUso b√°sico:\")\n",
    "print(\"  ask(vectorstore, 'sua pergunta')\")\n",
    "print(\"\\nUso avan√ßado:\")\n",
    "print(\"  ask(vectorstore, 'sua pergunta', k=8, score_threshold=0.6)\")\n",
    "print(\"\\nDiagn√≥stico:\")\n",
    "print(\"  diagnostico(vectorstore, 'sua pergunta')\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0defdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 109\n",
      "\n",
      "Primeiro chunk (primeiros 500 chars):\n",
      "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n",
      "\n",
      "Digital Object Identifier 10.1109/ACCESS.2017.DOI\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Estat√≠sticas dos chunks:\n",
      "  M√©dia: 1043 caracteres\n",
      "  Menor: 102 caracteres\n",
      "  Maior: 1498 caracteres\n",
      "  Chunks vazios: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. VERIFICAR O QUE FOI EXTRA√çDO\n",
    "print(f\"Total de chunks: {len(chunks)}\")\n",
    "print(f\"\\nPrimeiro chunk (primeiros 500 chars):\")\n",
    "print(chunks[0].page_content[:])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# 2. VERIFICAR TAMANHOS DOS CHUNKS\n",
    "tamanhos = [len(c.page_content) for c in chunks]\n",
    "print(f\"\\nEstat√≠sticas dos chunks:\")\n",
    "print(f\"  M√©dia: {sum(tamanhos)/len(tamanhos):.0f} caracteres\")\n",
    "print(f\"  Menor: {min(tamanhos)} caracteres\")\n",
    "print(f\"  Maior: {max(tamanhos)} caracteres\")\n",
    "print(f\"  Chunks vazios: {sum(1 for t in tamanhos if t < 50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b2497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 21:55:57,490 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç show the content of TABLE 2. Tattoo segmentation models.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 21:57:07,501 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Resposta:\n",
      "================================================================================\n",
      "Conforme solicitado, apresento o conte√∫do da TABLE 2. Tattoo segmentation models, extra√≠da dos trechos fornecidos:\n",
      "\n",
      "**TABLE 2. Tattoo segmentation models.**\n",
      "\n",
      "| Ref.   | Network       | Semantic?   | Open-Set?   |\n",
      "|--------|---------------|-------------|-------------|\n",
      "| [9]    | ConvNet       | No          | No          |\n",
      "| [31]   | AlexNet + VGG | No          | No          |\n",
      "| [13]   | ViT-based     | No          | No          |\n",
      "| Ours   | SegFormer     | Yes         | Yes         |\n",
      "\n",
      "**Observa√ß√µes:**\n",
      "\n",
      "*   A tabela lista quatro modelos de segmenta√ß√£o de tatuagens: [9], [31], [13] e \"Ours\" (que se refere ao modelo apresentado no documento). (Trecho 5)\n",
      "*   Para cada modelo, a tabela indica a arquitetura da rede neural utilizada (\"Network\"), se o modelo realiza segmenta√ß√£o sem√¢ntica (\"Semantic?\") e se √© um modelo de \"Open-Set?\". (Trecho 5)\n",
      "*   O modelo \"Ours\" (do pr√≥prio trabalho) utiliza a arquitetura SegFormer e √© tanto um modelo de segmenta√ß√£o sem√¢ntica quanto um modelo de \"Open-Set\". (Trecho 5)\n",
      "*   O modelo [9] utilizou uma rede ConvNet para identificar se uma imagem continha tatuagens, utilizando uma janela deslizante para testar segmentos da imagem. (Trecho 5)\n",
      "================================================================================\n",
      "\n",
      "üìö 5 Chunks utilizados:\n",
      "\n",
      "üü¢ Chunk 1: 85.0% similar\n",
      "   ## II. RELATED WORKS  In order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front....\n",
      "\n",
      "üü¢ Chunk 2: 85.0% similar\n",
      "   ## II. RELATED WORKS  In order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front....\n",
      "\n",
      "üü¢ Chunk 3: 85.0% similar\n",
      "   ## II. RELATED WORKS  In order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front....\n",
      "\n",
      "üü¢ Chunk 4: 84.6% similar\n",
      "   ## A. AVAILABLE DATASETS  Regarding the datasets, we chose to organize in the Table 1 a summary of the main characteristics of the most common datasets in tattoo detection, classification, and segmentation problems. These criteria include the number of samples, public availability, and type of annotation (classification, detection of objects with bounding box - BB, or segmentation). Regarding the annotation focused on segmentation, we also included whether it contains semantic segmentation.  TABLE 1. Tattoo datasets....\n",
      "\n",
      "üü¢ Chunk 5: 82.0% similar\n",
      "   As far as our research has reached, only two studies have used deep learning methods for the problem of tattoo segmentation.  TABLE 2. Tattoo segmentation models.  | Ref.   | Network       | Semantic?   | Open-Set?   | |--------|---------------|-------------|-------------| | [9]    | ConvNet       | No          | No          | | [31]   | AlexNet + VGG | No          | No          | | [13]   | ViT-based     | No          | No          | | Ours   | SegFormer     | Yes         | Yes         |  Based on the study on CNNs, [9] used the structure of a ConvNet network to train small pieces of images to learn to identify which ones have or do not have pieces of tattoos. After training the network, a sliding window was passed through the image to be tested, and each segment of the sliding window was tested as a piece with a tattoo or not, marking the positive pieces. Parts with possible tattoos would be segmented piece by piece at the end of the slide....\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Conforme solicitado, apresento o conte√∫do da TABLE 2. Tattoo segmentation models, extra√≠da dos trechos fornecidos:\\n\\n**TABLE 2. Tattoo segmentation models.**\\n\\n| Ref.   | Network       | Semantic?   | Open-Set?   |\\n|--------|---------------|-------------|-------------|\\n| [9]    | ConvNet       | No          | No          |\\n| [31]   | AlexNet + VGG | No          | No          |\\n| [13]   | ViT-based     | No          | No          |\\n| Ours   | SegFormer     | Yes         | Yes         |\\n\\n**Observa√ß√µes:**\\n\\n*   A tabela lista quatro modelos de segmenta√ß√£o de tatuagens: [9], [31], [13] e \"Ours\" (que se refere ao modelo apresentado no documento). (Trecho 5)\\n*   Para cada modelo, a tabela indica a arquitetura da rede neural utilizada (\"Network\"), se o modelo realiza segmenta√ß√£o sem√¢ntica (\"Semantic?\") e se √© um modelo de \"Open-Set?\". (Trecho 5)\\n*   O modelo \"Ours\" (do pr√≥prio trabalho) utiliza a arquitetura SegFormer e √© tanto um modelo de segmenta√ß√£o sem√¢ntica quanto um modelo de \"Open-Set\". (Trecho 5)\\n*   O modelo [9] utilizou uma rede ConvNet para identificar se uma imagem continha tatuagens, utilizando uma janela deslizante para testar segmentos da imagem. (Trecho 5)',\n",
       " 'chunks': [(Document(metadata={'source': 'datasets/tattoo.pdf'}, page_content='## II. RELATED WORKS\\n\\nIn order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front.'),\n",
       "   0.30097681283950806),\n",
       "  (Document(metadata={'source': 'datasets/tattoo.pdf'}, page_content='## II. RELATED WORKS\\n\\nIn order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front.'),\n",
       "   0.30097681283950806),\n",
       "  (Document(metadata={'source': 'datasets/tattoo.pdf'}, page_content='## II. RELATED WORKS\\n\\nIn order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front.'),\n",
       "   0.30097681283950806),\n",
       "  (Document(metadata={'source': 'datasets/tattoo.pdf'}, page_content='## A. AVAILABLE DATASETS\\n\\nRegarding the datasets, we chose to organize in the Table 1 a summary of the main characteristics of the most common datasets in tattoo detection, classification, and segmentation problems. These criteria include the number of samples, public availability, and type of annotation (classification, detection of objects with bounding box - BB, or segmentation). Regarding the annotation focused on segmentation, we also included whether it contains semantic segmentation.\\n\\nTABLE 1. Tattoo datasets.'),\n",
       "   0.3077482283115387),\n",
       "  (Document(metadata={'source': 'datasets/tattoo.pdf'}, page_content='As far as our research has reached, only two studies have used deep learning methods for the problem of tattoo segmentation.\\n\\nTABLE 2. Tattoo segmentation models.\\n\\n| Ref.   | Network       | Semantic?   | Open-Set?   |\\n|--------|---------------|-------------|-------------|\\n| [9]    | ConvNet       | No          | No          |\\n| [31]   | AlexNet + VGG | No          | No          |\\n| [13]   | ViT-based     | No          | No          |\\n| Ours   | SegFormer     | Yes         | Yes         |\\n\\nBased on the study on CNNs, [9] used the structure of a ConvNet network to train small pieces of images to learn to identify which ones have or do not have pieces of tattoos. After training the network, a sliding window was passed through the image to be tested, and each segment of the sliding window was tested as a piece with a tattoo or not, marking the positive pieces. Parts with possible tattoos would be segmented piece by piece at the end of the slide.'),\n",
       "   0.35996222496032715)],\n",
       " 'num_chunks_used': 5}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(vectorstore, \"show the content of TABLE 2. Tattoo segmentation models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36c1c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n",
      "\n",
      "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n",
      "\n",
      "Digital Object Identifier 10.1109/ACCESS.2017.DOI' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## Open-Set Tattoo Semantic Segmentation\n",
      "\n",
      "ANDERSON BRILHADOR 1 , RODRIGO TCHALSKI DA SILVA 1 , CARLOS ROBERTO MODINEZ-JUNIOR 1 , GABRIEL DE ALMEIDA SPADAFORA 1 , HEITOR SILV√âRIO LOPES 1 , AND ANDR√â EUG√äNIO LAZZARETTI 1 , (Member, IEEE).\n",
      "\n",
      "1 Federal University of Technology - Paran√°, Av. Sete de Setembro, 3165, Curitiba, 80230-901, Paran√°, Brazil.\n",
      "\n",
      "Corresponding author: Anderson Brilhador (e-mail: andersonbrilhador@gmail.com).' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='ABSTRACT Tattoos can serve as an essential source of biometric information for public security, aiding in identifying suspects and victims. In order to automate tattoo classification, tasks like classification require more detailed image content analysis, such as semantic segmentation. However, a dataset with appropriate semantic segmentation annotations is currently lacking. Also, there are countless ways to categorize tattoo classes, and many are not directly categorizable, either because they belong to a specific artistic trait or characterize an object with previously undefined semantics. An effective way to overcome these limitations is to build recognition systems based on open-set assumptions. Nevertheless, state-ofthe-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to the significant class imbalance (predominant background). To the best of our knowledge, this paper is the first to explore semantic segmentation in closed and open-set scenarios for tattoos. In this sense, this paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible test set, enabling comparisons and future research in this area' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='. In this sense, this paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible test set, enabling comparisons and future research in this area. The proposed approach outperforms other methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU for the unknown class, demonstrating the feasibility of this approach for automatic tattoo analysis. The paper also highlights key limitations and open research areas in this challenging field. Dataset and codes are available at https://github.com/Brilhador/tssd2023.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='INDEX TERMS open-world, open-set, semantic segmentation, large-margin learning, tattoo classification.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## I. INTRODUCTION\n",
      "\n",
      "Tattoos are forms of human expression and are also considered an art. In their almost unique features, tattoos go beyond artistic expressions and can serve as essential sources of biometric information. Consequently, it can be useful in identifying their bearers, mainly for public security [1], [2] because tattoos can be used to identify not only suspects but also victims [3], [4]. In addition, the subject has raised studies on ethical and social issues that may encompass the topic [5].\n",
      "\n",
      "Compared to other biometrics, tattoos bring a series of characteristics that make them very difficult to recognize. Other biometrics usually have well-defined standards, robust techniques, well-established methods for their treatment and recognition, standardized data capture and storage, and other factors that help their reliability and robustness. However, tattoos still need to have such characteristics and requirements. Apart from the issues related to processing and using general biometrics, tattoo recognition has a singular complexity because it can be divided into several subproblems, each equally significant [6].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='First, an image can be submitted to detect, locate, and segment (outline or instance) the tattoo contained therein. Subsequently, the image can be classified, de-identified, or re-identified (image-to-image, sketch-to-image, partial, or similar). After preprocessing an image, the best result could be a well-segmented tattoo without any pollution or background. Then, for all classification, de-identification, or re-identification tasks, these images contain only the most essential information to store and, later, process [6].\n",
      "\n",
      "Nonetheless, tasks that consider the meaning of the content of images, such as classification, may require a more detailed separation of objects in a tattoo image, called semantic segmentation [7]. At this point, a segmented tattoo could identify and detach each object in the tattoo, after which each could be analyzed separately. For instance, security and biometric recognition systems could benefit from tattoo semantic split at the pixel-level for fine-grained\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "feature extraction by semantic classes, reduce false positives by precisely delineating the boundaries of biometrics features, and allow selective anonymization of regions of an image. Moreover, tattoo biometrics systems may require the identification of multiple semantic classes in multiple areas of the image, which can be performed accurately by tattoo semantic segmentations.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Although this topic has been widely explored in studies related to images and videos in many different areas, semantic segmentation is still underexplored in the context of tattoos. Several works only focus on tattoo classification and detection [6], [8], and only a few researches deal with segmentation [9], [10] without identifying the semantics of the components that compose the tattoos.\n",
      "\n",
      "One of the reasons that make semantic segmentation difficult is related to the complexity that tattooing can have. As mentioned, tattoos are expressions of art, and their features can be as varied as possible and imaginable. In this way, objects can be positioned very closely, mixed, overlapped, and distorted, and abstract images can also be present, among many other hindering factors [11]. Additionally, the lack of public and comprehensive datasets makes it even more challenging to develop efficient methods for segmentation. Some recent works propose public datasets, such as [12] and [13]. However, in the case of [13], using semi-synthetic images without employing the semantics associated with each class makes it difficult to generalize the proposed characteristic also observed in [12].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Still, in this context, it is essential to emphasize the complexity and semantic variability of tattoo classes. Tattoo categories can vary significantly, and some are not easily categorized due to their association with specific artistic styles or objects with undefined semantics. The tattoo recognition scenario, especially from a public safety perspective, is also somewhat challenging, especially given the circumstances in which the information is obtained and analyzed. It is not uncommon for tattoo information to be obtained partially, and, therefore, semantic segmentation has great relevance in the identification process, as in the following scenarios: (i) semantic segmentation can be used to create databases with automatic textual annotations, as it is common for a witness or victim to remember or have visual contact with only parts of a tattoo of a wanted person, and, in this way, from the description, it would be possible to identify tattoos with those parts visualized; (ii) semantic segmentation is important in the pre-processing of images in preparation for information recognition processes, such as for partial re-identification of tattoos, wherein an automatic process, the segmented parts can be recovered separately in cases of partial image collections [6].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='An effective way to overcome these limitations is to build recognition systems based on dynamic and open-set perception. These systems are designed to handle objects from unknown classes commonly encountered in real-world applications. Open-set recognition has extensively studied the ability to recognize new classes [14]. Open-set semantic segmentation, in turn, is an approach that incorporates openset perception into semantic segmentation. The main difference with closed-set semantic segmentation is that openset semantic segmentation must correctly classify samples belonging to known classes while rejecting those belonging to unknown classes. In the context of this work, open-set semantic segmentation can be an ally in improving databases and models for identifying and classifying tattoo objects, allowing the improvement of annotations and descriptions of complex tattoos. Therefore, semantic segmentation must also be seen as a middle process, not just as an end process in the tattoo recognition roadmap.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Studies have explored the use of open-set semantic segmentation in different applications [15], [16]. These studies focus on adapting or building open-set classifiers to make closed-set semantic segmentation models capable of recognizing unknown classes. While the outcomes of these studies are promising, the performance of these approaches is limited due to the low representation of the obtained features, resulting in an 'irregular' logit space with low discrimination among the classes. Recent research [17], [18] has demonstrated that incorporating metric learning techniques can enhance open-set recognition. Metric learning aids in obtaining more discriminative features and building a logit space that tightly clusters known classes while maintaining a considerable distance from unknown classes. However, it is essential to acknowledge that applying metric learning in the context of semantic segmentation can be impractical. This is primarily due to the exponential complexity of the task, as calculating pairwise distances among logit vectors of pixels becomes computationally expensive.\n",
      "\n",
      "Recent studies have investigated the potential of largemargin learning to acquire more discriminative features, yielding improved outcomes in image classification tasks [19]-[21]. Hence, strategies established on largemargin learning present promising and viable alternatives for building a well-defined logit space that enhances the separation among decision boundaries of semantic classes.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Motivated by these results, our study explores the discriminative capabilities of large-margin learning to produce more distinctive features for tattoo semantic segmentation. This approach effectively increases the spatial separation among decision boundaries to different semantic classes, forcing the build of ideal logit space as illustrated in Figure 1b. Furthermore, this expanded separation among decision boundaries will set the stage for accommodating unknown classes in the future, enhancing the effectiveness of tattoo semantic segmentation within an open-set scenario. Visual comparisons depicting the differences in logit space resulting from the presence of unknown classes can be observed in Figure 1.\n",
      "\n",
      "Given the limitations presented so far and the fact that, to the extent of our knowledge, open-set classification has not been used in the context of tattoo recognition and is an open research gap, this work aims to propose a new large-margin-based loss function adapted to the context of' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Given the limitations presented so far and the fact that, to the extent of our knowledge, open-set classification has not been used in the context of tattoo recognition and is an open research gap, this work aims to propose a new large-margin-based loss function adapted to the context of\n",
      "\n",
      "open set semantic segmentation in tattoos. This novel loss function seeks to overeat the closed set segmentation results and enable and increase the open set segmentation results compared to other state-of-the-art semantic segmentation loss functions. Furthermore, we propose using a publicly available test dataset, with an annotation aimed at semantic segmentation in an open-set context, whose high complexity will serve as a benchmark for comparing methods in this area. The main contributions of this paper are then summarized as follows:' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- Test set publicly available for the dataset, allowing comparisons and future work in the open-set and closed-set scenarios;\n",
      "- Novel class semantic augmentation method to expand the tattoo samples;\n",
      "- Novel large-margin loss function for open-set tattoo semantic segmentation to build more discriminative features and handle the class imbalance;\n",
      "- A generalized open-set classifier approach based on open principal component scoring with incremental learning called G-OpenIPCS;\n",
      "- Detailed and in-depth comparison with different stateof-the-art loss functions and open-set semantic segmentation methods;\n",
      "- Statement of the main challenges for the open-set tattoo semantic segmentation.\n",
      "\n",
      "This paper is organized as follows. Section II presents related works, mainly including related datasets, tattoo segmentation, and open-set semantic segmentation. Our proposed methods, particularly the novel tattoo semantic segmentation dataset, novel tattoo semantic augmentation method, and the novel loss function and open set classifier, are detailed in Section III. The experiment setup is discussed in Section IV, with results, discussions, and comparisons with state-of-the-art approaches presented in Section V.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "IEEE AccesS'' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "IEEE AccesS'\n",
      "\n",
      "Imp. Surfaces Tree Unk. Class Open Space FIGURE 1. Closed set and open set in dense labeling scenarios. The label space represents the pixel-level predictions, and logit space refers to a subset of pixel samples in a 2D manifold separated by labels and decision boundaries for each class. a) Closed set without the presence of unknown classes. b) Closed set with the presence of unknown classes. These unknown classes result in misclassification, being segmented as known classes. c) Open set segmentation with 'irregular' logit space. The term 'irregular' suggests undefined or overlapping decision boundaries among classes. d) Open set segmentation with 'ideal' logit space. The term 'ideal' refers to well-defined decision regions between the classes and enough space (Open space) between them to include new classes [14], [18].\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Building\n",
      "\n",
      "Car\n",
      "\n",
      "Finally, Section VI shows the conclusions, with the open challenges in open-set tattoo semantic segmentation discussed in Section VII.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## II. RELATED WORKS\n",
      "\n",
      "In order to present a general overview of the state-of-theart in the context of tattoo segmentation, we divided the related works into three parts where this paper presents main contributions: datasets, tattoo segmentation, and open set semantic segmentation. The following subsections detail each of these works, pointing out the innovative aspects of this work on each front.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## A. AVAILABLE DATASETS\n",
      "\n",
      "Regarding the datasets, we chose to organize in the Table 1 a summary of the main characteristics of the most common datasets in tattoo detection, classification, and segmentation problems. These criteria include the number of samples, public availability, and type of annotation (classification, detection of objects with bounding box - BB, or segmentation). Regarding the annotation focused on segmentation, we also included whether it contains semantic segmentation.\n",
      "\n",
      "TABLE 1. Tattoo datasets.\n",
      "\n",
      "| Ref.   | Total   | Public?   | Annot.   | Semantic?   |\n",
      "|--------|---------|-----------|----------|-------------|\n",
      "| [12]   | 7,526   | No        | BB       | -           |\n",
      "| [9]    | 890     | Yes       | Seg.     | No          |\n",
      "| [22]   | 5,740   | Yes       | Class.   | -           |\n",
      "| [23]   | 5,000   | Yes       | BB       | -           |\n",
      "| [24]   | 210     | Yes       | Class.   | -           |\n",
      "| [13]   | 5,500   | Yes       | Seg.     | No          |\n",
      "| Ours   | 2,106   | 315       | Seg.     | Yes         |\n",
      "\n",
      "In the case of [12], despite the relatively large number of samples and an annotation focused on tattoo detection, the\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='In the case of [12], despite the relatively large number of samples and an annotation focused on tattoo detection, the\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "dataset is private, which makes comparisons, establishing benchmarks, and analyzing new methods for the dataset difficult. On the other hand, the dataset proposed in [23] is publicly available and annotated only with BB, i.e., it is not possible to use it in the context of segmentation. The datasets proposed in [24] and [22], in turn, are focused exclusively on classification and provide only one label for each image or image patch, restricting their use to multiclass classification problems, without the location of the tattoo in the image.\n",
      "\n",
      "[9] were the first to address tattoo segmentation. The proposed method aimed to de-identify soft biometric identifiers (tattoos) by discriminating tattoo and non-tattoo image patches with a deep neural network. In this sense, the proposed dataset presents a pixel-level annotation of the presence or absence of a tattoo in the image. However, the authors did not individualize the tattoo classes in the proposed annotation. The same is observed in the dataset presented in [13]. Furthermore, the authors presented a proposal using semi-synthetic images. This characteristic can sometimes lead to an image far from a real tattoo, compromising the segmentation approach.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Our dataset, in turn, presents some original and innovative features that can complement currently available datasets: (i) Inclusion of a semantic segmentation annotation; (ii) Several classes of tattoos in the same image, increasing complexity; (iii) Quite varied sizes of tattoos and classes of tattoos in the same image; and (iv) Tattoos in different regions of the body, maintaining the variability that exists in real situations. As will be detailed later, only the test set is made publicly available since most of the images available in these scenarios contain public use restrictions. However, we believe that as it is the first dataset with semantic annotation in segmentation, it will allow the comparison and evaluation of different approaches to this problem, which is significantly challenging.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## B. TATTOO SEGMENTATION\n",
      "\n",
      "Tattoo segmentation methods were presented in many studies, but their results were suppressed, maybe because segmentation was not the main focus. Furthermore, as many of them were carried out a long time ago, the research did not address deep learning methods, for example, and the methods cannot be directly compared with the approach adopted here.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='In such cases, authors have performed their researches using methods based on: (i) Content-Base Image Retrieval (CBIR) and Edge Direction Coherence Vector (EDCV) [25]; (ii) 3 √ó 3 Sobel filter [26], Active Contour CBIR (ACCBIR), and Vector Field Convolution (VFC) [1]; (iii) a complex system combining bottom-up and top-down priorities that transfer tattoo segmentation to detection split-merge skin detection, followed by figure-ground tattoo segmentation [27]; (iv) LoG (Laplacian of Gaussian) and Sobel kernel filters called quasi-connected components (QCC), using the GrabCut algorithm to produce the final segmented tattoo image [28]; (v) a negative image method with HSV (hue, saturation, and value, or lighting) model [29]; (vi) identification of pixels of skin in regions close to the tattoos and a graph-cut model based on skin color and a visual bump map [30], and (vii) a k -means cluster used in LAB color space to detect the skin area with a morphology processing used to smooth the clear graphic of the tattoo image segment [10]. The main limitation of these hand-crafted-based methods is that the feature extractor may have adequate performance for some classes and datasets but significantly lower performance for others, with compromised generalization. This is accentuated for datasets with greater variability in tattoo images.\n",
      "\n",
      "As far as our research has reached, only two studies have used deep learning methods for the problem of tattoo segmentation.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='As far as our research has reached, only two studies have used deep learning methods for the problem of tattoo segmentation.\n",
      "\n",
      "TABLE 2. Tattoo segmentation models.\n",
      "\n",
      "| Ref.   | Network       | Semantic?   | Open-Set?   |\n",
      "|--------|---------------|-------------|-------------|\n",
      "| [9]    | ConvNet       | No          | No          |\n",
      "| [31]   | AlexNet + VGG | No          | No          |\n",
      "| [13]   | ViT-based     | No          | No          |\n",
      "| Ours   | SegFormer     | Yes         | Yes         |\n",
      "\n",
      "Based on the study on CNNs, [9] used the structure of a ConvNet network to train small pieces of images to learn to identify which ones have or do not have pieces of tattoos. After training the network, a sliding window was passed through the image to be tested, and each segment of the sliding window was tested as a piece with a tattoo or not, marking the positive pieces. Parts with possible tattoos would be segmented piece by piece at the end of the slide.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='[31] proposed a continuation of work presented in [9], this time testing three different networks for tattoo segmentation: (i) an architecture consisting only of multiple fully connected layers, without convolutional layers; (ii) an architecture inspired by the AlexNet network; and (iii) an architecture inspired by the VGGNet network. On the other hand, state-of-the-art segmentation models based on Vision Transformer (ViT) were evaluated in [13]; however, the main idea of that work was the unsupervised tattoo generator that allowed the creation of many semi-synthetic images with tattooed subjects. Hence, as shown in Table 2, related approaches used still needed to follow a semantic segmentation methodology and did not use an open-set semantic segmentation view, which is the focus of our current study and detailed as follows.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## C. OPEN-SET SEMANTIC SEGMENTATION\n",
      "\n",
      "The success of the fully convolutional network (FCN) in closed-set semantic segmentation [32] has led to the successful implementation of various neural network models for closed-set semantic segmentation on different applications [7]. However, these methods are unsuitable for openset scenarios, which are common in real-world computer vision. This is because the closed-set perception fails when\n",
      "\n",
      "unknown classes from training are found in the test phase [33]. Due to that, several proposals have been developed for the open-set context in different applications, mainly autonomous driving, remote sensing, and data collection [34].\n",
      "\n",
      "For open-set semantic segmentation, the loss function selection to guide the optimization process is an important aspect of the achieved results [35]. In general, studies limit themselves to using the successful and widely employed cross-entropy loss (CE) [15], [16], [33], [36]-[40]. This loss function measures the disparity between the predicted values and the ground truth, guiding the model's learning process based on labeled data. Hence, the studies focus on adapting or building open-set classifiers to make closedset semantic segmentation models capable of recognizing unknown classes, even in label noise scenarios [41].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='In this sense, metric learning has recently gained significant attention in addressing the open-set problem. Metric learning is an approach based on learning a distance metric that reinforces similarity between objects in the latent space. It has been studied in various fields, such as image classification [14], semantic segmentation [42], [43], and zero-shot segmentation [44]. According to [42], the metric learning strategy aims to direct the feature extraction process to obtain a well-controlled latent space, maximizing interclass spacing and minimizing intra-class spacing based on a distance metric. Thus, the unknown samples are repelled into open space, as can be seen in Figure 4.\n",
      "\n",
      "Similarly, the large-margin-based loss (LM) functions [19]-[21] maximize the margins between classes by imposing a regularization on the logit vectors of pixels to induce an increased separation between the boundary regions of the semantic classes. This strategy improves the generalization of models and provides open space between classes that can be valuable in the context of open-set semantic segmentation, allowing unknown samples to be projected into these open spaces. Furthermore, the large-margin loss adopts a more efficient training strategy than other metric learning approaches, such as those involving cubic costs for computing pairwise distances between the logit vectors of pixels [45].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Recent studies [40], [46] have made improved open-set semantic segmentation for autonomous driving applications by employing negative auxiliary data. Unlike these studies, our approach uses no auxiliary data to enhance the performance of open-set semantic segmentation. Table 3 summarizes the approaches used for open-set semantic segmentation.\n",
      "\n",
      "In the context of tattoo semantic segmentation, as stated in the previous Section, to the extent of our knowledge, no works focus on the open set context-the most closely related works are [23] and [47]. [23] built a tattoo search approach that can learn tattoo detection and compact representation jointly in a single CNN via multi-task learning is presented. However, the compactness proposed by the authors is more focused on the compressive yet discriminative feature learning for large-scale visual search and\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "TABLE 3. Open-set semantic segmentation approaches.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "TABLE 3. Open-set semantic segmentation approaches.\n",
      "\n",
      "| Ref.                         | Loss   | Context        | Aux. data?   |\n",
      "|------------------------------|--------|----------------|--------------|\n",
      "| [15], [16], [36], [38], [39] | CE     | Remote Sensing | No           |\n",
      "| [33], [37]                   | CE     | General        | No           |\n",
      "| [33]                         | CE     | Synthetic Data | No           |\n",
      "| [40], [46]                   | CE     | Aut. Driving   | Yes          |\n",
      "| [42]                         | Metric | Aut. Driving   | No           |\n",
      "| [44]                         | Metric | General        | No           |\n",
      "| [43]                         | Metric | General        | No           |\n",
      "| Ours                         | LM     | Tattoo         | No           |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='instance retrieval applications, i.e., the efficiency of the search procedure. Open-set classification is not presented, and discussions of the proposed multi-task procedure are not encouraged for other applications. While [47] presented a classification method based on the Extreme Value Theory for tattoo classification. However, the focus of the proposed approach was the mid-level representations as a tool to adjust the trade-off between accuracy and efficiency. Hence, the results are mainly dedicated to real-world computer vision systems, where high accuracy is maintained even on commodity hardware with a low computational budget. Details regarding open-set are also not addressed.\n",
      "\n",
      "To the best of our knowledge, this is the first work to explore tattoo semantic segmentation in closed and open-set scenarios, establishing benchmarks for both conditions using our publicly available test set. Our approach introduces the large-margin loss function as an efficient learning strategy to build a well-defined logit space and handle class imbalance, using contemporary network architecture based on transformers and presenting the generalist approach to integrating a robust open-set classifier for semantic segmentation tasks.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## III. PROPOSED METHODS\n",
      "\n",
      "This section outlines the proposed methods. Firstly, in Section III-A, a novel-built TSSD2023 dataset is presented. Then, Section III-B introduces a novel tattoo semantic augmentation to expand tattoo samples of the TSSD2023 dataset. Subsequently, in Section III-C, a novel large-margin loss function is proposed to handle class imbalance and enhance the discriminative of the classes in the TSSD2023 dataset. Lastly, in Section III-D, a generalized approach for OpenIPCS is proposed for open-set semantic segmentation.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## A. TATTOO SEMANTIC SEGMENTATION DATASET\n",
      "\n",
      "Identifying the various things that make up a tattoo can be defined as a semantic segmentation task. This process allows machines to comprehend the meaning behind a tattoo better. Its usefulness is particularly evident in security systems, where it assists in identifying and monitoring individuals through surveillance systems, for instance, locating a person based on a brief tattoo description. However, as previously presented, current tattoo datasets are limited to image classification [22], [24], object detection [12], [23], or tattoo segmentation [9], [13], which only separate the tattoo\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "IEEEAcCeSS'\n",
      "\n",
      "from the background and fail to provide a comprehensive understanding of the significance of the tattoos.\n",
      "\n",
      "In this manner, we created the Tattoo Semantic Segmentation Dataset (TSSD2023), respecting the copyrights of image owners to the greatest extent possible. To build this dataset, web scraping was conducted on Flickr 1 . Numerous terms related to tattoos were used as search queries on the platform. Then, a visual inspection was performed by humans to confirm whether the images obtained contained tattoos and whether their content was suitable for sharing. Finally, the licenses users attribute to these images when sharing them on the platform were considered to define the training, validation, and test sets.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='As a result, the test dataset exclusively comprises images for which sharing permissions had been granted. In contrast, the training and validation datasets consist only of images for which sharing was unauthorized. This division strategy was adopted due to the limited availability of images with public sharing licenses. Thus, the test sets will be publicly available for comparison and development of future work. However, the training and validation sets will be kept private to ensure that none of the authors' copyrights are infringed.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Figure 2 shows 33 classes 2 for Known Known Class (KKC) tattoos and 23 classes for Unknown Unknown Class (UUC) tattoos selected for annotation in TSSD2023 3 . The motivation for choosing this split for KKC and UUC classes was mainly based on the number of images available for each semantic class. Classes with reduced representation in the universe of available tattoos and, consequently, public samples were selected exclusively to compose the UUCs due to the impossibility of successfully training the segmentation models on these classes. Furthermore, the proposed division guarantees that the test semantic classes defined as UUC are not found on the training and validation sets. Hence, we consider the proposed division sufficient to evaluate the open-set methods proposed in this work. Once the data were divided, UUCs were chosen to form part of the test set for open-set evaluation. This approach enabled the representation of similar and dissimilar semantic classes compared to KKCs, allowing for evaluation in straightforward and complex scenarios.\n",
      "\n",
      "Each KKC class was meticulously labeled with unique identifiers to enable the model to distinguish each object semantically. In contrast, all UUC tattoos were assigned the same 'unknown' class label. Notably, all KKC classes are represented in the training and validation sets and the test\n",
      "\n",
      "1 A photo and video hosting platform established in 2004. Available in: https://flickr.com/' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='1 A photo and video hosting platform established in 2004. Available in: https://flickr.com/\n",
      "\n",
      "2 The dataset also includes annotations for the 'stem/branch' and 'rope' classes, which were omitted from the analysis due to the limited number of annotations.\n",
      "\n",
      "3 The closed-set approach assumes that the training and testing pixels belong to the same label space (defined as KKCs), meaning that the train and test sets contain the same classes. However, this assumption does not hold in real-world scenarios, especially in earth observation applications. During the prediction phase, the model may face pixels from classes not seen during the training phase (UUCs). We direct the reader to [14] for a deeper reading about these definitions.\n",
      "\n",
      "set. However, UUC tattoos are exclusively found in the test open-set. A human inspection process was also conducted to ensure these open-set recognition conditions [14].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='set. However, UUC tattoos are exclusively found in the test open-set. A human inspection process was also conducted to ensure these open-set recognition conditions [14].\n",
      "\n",
      "Table 4 presents four possible tattoo composition situations found in TSSD2023: (i) Single tattoo: only one tattoo class is present in the image; (ii) Multiple tattoos: multiple tattoo classes can be found in a single image; (iii) Multiple tattoos (with overlap): similar to the previous scenario, but the tattoos are overlapped with each other. Due to this overlap, this scenario is more challenging than the previous one [33]; and (iv) Tattoos (unlabeled) as background: Tattoos (unlabeled) as background: Specific tattoo classes were categorized as background classes due to two primary reasons. First, this decision was necessary because, at times, it was impossible to determine their specific semantic tattoo class. Second, due to a limited number of available samples, assigning individual semantic labels to these classes was not feasible. These situations provide a comprehensive representation of tattoo compositions within the dataset.\n",
      "\n",
      "TABLE 4. Samples of the images and annotations of the TSSD2023 dataset.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='TABLE 4. Samples of the images and annotations of the TSSD2023 dataset.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "In conclusion, TSSD2023 contains 2,106 tattoo images without specific image resolution standards that have been annotated at the pixel level. These images are divided into the following subsets: 1,404 for training, 387 for validation, and 254 for the closed-set test. Additionally, 61 images with UUC tattoos are included to form the test open-set with 315 images. Figure 3 illustrates the distribution of pixel percentages for each class within TSSD2023, demonstrating that the dataset is notably imbalanced, particularly in the background class, which accounts for an average of ‚âà 80% of pixels across the subsets. It is also important to highlight that the unknown class represents approximately 3% of the pixels present in the test open-set. Samples of images from TSSD2023 can be observed in Table 4, showcasing tattoos presented in diverse scenarios, sizes, styles, positions, and combinations. These variations aim to maintain the variability of real-world situations.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "IEEE AccesS'\n",
      "\n",
      "Rope Stem/Branch FIGURE 2. TSSD2023 Classes. KKCs tattoos correspond to the 33 labeled classes, while the UUCs tattoos are represented by the 23 classes defined as 'unknown' (black label). These classes can be viewed within a conceptual taxonomy, facilitating an understanding of the domain coverage provided by the dataset.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "FIGURE 3. The distribution of pixels per class and set. These values are organized based on their quantities in the training set. The values of background classes were suppressed due to discrepancies with the other classes. The reference values for the background are 79.8% in training, 79.4% validation images, 81.4% test closed-set images, and 81.7% test open-set images.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## B. CLASS SEMANTIC AUGMENTATION\n",
      "\n",
      "Motivated by the limited number of samples within TSSD2023 compared to the extensive diversity of real-world tattoos, this work introduces a novel data augmentation named class semantic augmentation (CSA) to increase the variety of tattoos contained in TSSD2023. This method applies pixel-level transformations to distinct classes, enabling unique augmentation for different classes within an image, as illustrated in Table 5. In this instance, the pixels of the heart class go through different transformations of colorations, gray styles, and color tones, trying to approximate the infinite possibilities of representing tattoos in the real world from limited semantic data.\n",
      "\n",
      "TABLE 5. Samples of the data augmentations on only the heart class, except the mix column that applies the data augmentations on all semantic classes.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Algorithm 1 details the proposed implementation of tattoo semantic augmentation. The algorithm takes five parameters: the input image X , the ground truth Y , a list of pixel- level transformations to increase the variability of semantic classes t , a list containing the probabilities of executing each transformation p , and a list of semantic class indices ic that remain unchanged during data augmentation. It is important to note that the parameters p and ic are optional. The algorithm first identifies the indices of semantic classes within the image. Subsequently, it iterates through each index, applying one of the transformations specified in t . As shown in Table 5, a heart can be represented in various ways while other classes remain unchanged. It demonstrates that this semantic augmentation allows specific adjustments for each class in the dataset, making it useful in various application domains, such as autonomous driving [42] and fashion images [48]. In addition, different pixel-level transformations can be selected for more suitable application contexts. For instance, the color of a flower may vary. At the same time, the leaf and stem could be confined to modifying the original color tones, preserving the real-world patterns. On the other hand, all semantic classes in an image can be changed without restrictions, as observed in the mix column of Table 5 and as employed in this study' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='. At the same time, the leaf and stem could be confined to modifying the original color tones, preserving the real-world patterns. On the other hand, all semantic classes in an image can be changed without restrictions, as observed in the mix column of Table 5 and as employed in this study. It is worth noting that this augmentation technique is limited to pixel-level transformations.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## C. PROPOSED LOSS FUNCTION\n",
      "\n",
      "Classical convolutional neural networks (CNNs) based semantic segmentation networks [32] can be divided into two\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "| Algorithm 1: Class semantic   | augmentation.                                      |\n",
      "|-------------------------------|----------------------------------------------------|\n",
      "| Input:                        | Image ( X )                                        |\n",
      "| Input:                        | Ground truth ( Y )                                 |\n",
      "| Input:                        | List of transforms ( t )                           |\n",
      "| Input:                        | List of probability of choice ( p )                |\n",
      "| Input:                        | List of ignore classes ( ic )                      |\n",
      "| Output: 1:                    | Transformed image ( ÀÜ X ) seg _ ids = unique ( Y ) |\n",
      "| 2:                            | seg _ ids = remove _ ic ( seg _ ids, ic )          |\n",
      "| 3:                            | ÀÜ X = copy ( X )                                   |\n",
      "| 4:                            | Foreach: i ‚àà seg _ ids do                          |\n",
      "| 5:                            | y = new _ array _ zeros ( Y.shape )                |\n",
      "| 6:                            | y [ Y == i ] = 1                                   |\n",
      "| 7:                            | T = random _ choice ( t, p )                       |\n",
      "| 8:                            | ÀÜ X = apply _ transform ( T, ÀÜ X,y )               |\n",
      "| 9:                            | end foreach                                        |\n",
      "| 10:                           | return ÀÜ X                                         |\n",
      "\n",
      "Ã∏' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Ã∏\n",
      "\n",
      "parts: a feature extractor that includes several convolution layers followed by max-pooling and activation function, and the linear classifier f = W ‚ä§ x + b in the last fully-connected layer applied on the feature vector x of the penultimate layer for obtaining the logit vector f ‚àà R C of each pixel of the input image, in which C represents the number of classes. To solve the overfitting problem and produce more discriminative logit vectors f in training, classifier margin has been exploited [19], [21], [49]. Following [50], the classification margin is the difference between the predicted score f c ‚àó and target score f y , where y indicates the ground truth class labels and c ‚àó = argmax c = y f c . Based on the margin f y -f c ‚àó , the traditional classifier margin loss function can be expressed as follows:\n",
      "\n",
      "Ã∏\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "in which œÅ is a boundary control parameter, usually œÅ ‚â• 0 . Thus, increasing the value of œÅ results in a larger classification distance between labels.\n",
      "\n",
      "According to [20], the cross-entropy loss ( L CE ) can partially encourage the development of a large-margin classifier within the CNNs. Based on this analysis, a symmetric Kullback-Leibler (KL) divergence term L LM was introduced as a regularization component for L CE , inducing a more large-margin classifier in the original L CE . The combination was termed the large-margin cross-entropy loss ( L CE + LM ), and it is the formula is presented as follows:\n",
      "\n",
      "<!-- formula-not-decoded -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- formula-not-decoded -->\n",
      "\n",
      "in which L CE is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and L LM is expressed as:\n",
      "\n",
      "Ã∏\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Ã∏\n",
      "\n",
      "where Œª is a regularization parameter. Increasing the value of the Œª enlarges the space between classes, thereby more resistance faced by the learning objectives. The detailed derivation has been explored in [19].\n",
      "\n",
      "Considering the class imbalance between the background and the foreground classes in the tattoo semantic segmentation dataset, we modify the L CE + LM replacing the L CE by the focal loss L FCL . This choice is based on the fact that L FCL is a variant of the L CE that preserves the discriminative capacity of the original loss while dealing with the imbalance among the classes [51]. Thus, our proposed large-margin focal loss ( L FCL + LM ) is described by:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "L FCL is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The L FCL applies a modulating term to the L CE to focus learning on hard examples and down-weight the numerous easy examples, where Œ± control the class weights and Œ≥ reduce the loss contribution from easy examples. Thus, we obtain a loss that deals with imbalances between classes while increasing the classification margin to improve the discriminability of the trained model.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## D. OPEN-SET CLASSIFIER FOR SEMANTIC SEGMENTATION\n",
      "\n",
      "In semantic segmentation networks, the logit vector f ‚àà R C are commonly normalized using the softmax function into a probability distribution for each class y ‚àà { 1 , ..., C } to perform the final classification of each pixel.Therefore, the final classification of each pixel is defined as ÀÜ Y close f = argmax p y ( f ) .\n",
      "\n",
      "This learnable classifier cannot recognize UUC, making it unsuitable for open-set recognition as it assigns all features to KKCs [42]. Thus, open-set classifiers must be developed to classify KKCs while recognizing UUCs accurately. Previous studies have investigated this development in semantic segmentation task [16], [38], [42], [52]. Based on those studies, one can present the general open set classifier as follows:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where C UUC denotes the UUC, and Œª out the cutoff threshold to determinate the UUC pixels.\n",
      "\n",
      "VOLUME 4, 2016\n",
      "\n",
      "Ã∏' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- formula-not-decoded -->\n",
      "\n",
      "where C UUC denotes the UUC, and Œª out the cutoff threshold to determinate the UUC pixels.\n",
      "\n",
      "VOLUME 4, 2016\n",
      "\n",
      "Ã∏\n",
      "\n",
      "A recent study [38] presented an open-set classifier method based on principal components analysis (PCA) on the internal logic vectors of CNNs to provide an open-set semantic segmentation, named Open Principal Component Scoring with Incremental Learning (OpenIPCS). Its training approach is efficient, which is crucial due to the exponential nature of pixel-level classification compared to image classification. Furthermore, the study notes that OpenIPCS outperforms open-set semantic segmentation strategies based on probability maps by a significant margin.\n",
      "\n",
      "OpenIPCS was inspired by the Conditional Gaussian Distribution Learning (CGDL) proposed in [53], which is a Variational Autoencoder (VAE) model for conditional Gaussian distribution estimation, capable of learning conditional distributions of KKC and rejecting UUC examples. In distinction, the OpenIPCS replaced the VAE with PCA and uses multiple internal activation layers to adjust the generative model with validation samples only.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='[54] showed that logit vectors get closer to the label space as CNN layers deepen. Thus, in addition to the last layer f , OpenIPCS considers the enabling aspects of the previous layers f, f 1 , . . . , f L , in which L denotes the total number of the CNNs layers. This approach combines lowlevel and high-level semantic information, thus enhancing the discrimination capability of the model.\n",
      "\n",
      "Then, for each pixel, a corresponding feature vector ÀÜ f train is built by concatenating the network's internal logit vectors f, f 1 , . . . , f L . Such a concatenation produces highdimensional and redundant features due to the hundreds or thousands of activation channels present in the CNN and the FCN layers [55], [56].\n",
      "\n",
      "As described by [57], PCA can serve a dual purpose. Apart from its primary role in reducing dimensionality, it can also act as a probability density estimator with Gaussian priors. These features allow OpenIPCs to use PCA as a generative model ( G ) for UUC recognition while solving the high-dimensionality problem of feature vectors.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='This G is incrementally adjusted using only the validation images. This process consists of adjusting the PCA with a batch of samples from the validation set. The classification step using the G consists of projecting the feature vector ÀÜ f test of the test images in the latent space obtained by adjusting the PCA on the validation images and performing the inverse process to obtaining the ÀÜ f G test . The difference between the original feature vector ÀÜ f test and the ÀÜ f G test is calculated. Consequently, pixels of KKCs have low difference values, while pixels belonging to the UUCs have high difference values. Thus, in accordance with [38], open-set recognition from OpenIPCS can be achieved as follows:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The Œª out do not represent equal statistical entities with the Equation 7. Thus, as in [38], the Œª out value was defined from preset values of True Positive Rate (TPR).\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- formula-not-decoded -->\n",
      "\n",
      "The Œª out do not represent equal statistical entities with the Equation 7. Thus, as in [38], the Œª out value was defined from preset values of True Positive Rate (TPR).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "As outlined in [54], the deeper layers are closer to the label space. Based on this analysis, this work proposes a generalized version of OpenIPCS (G-OpenIPCS) that focuses only on the last and penultimate layers of CNNs. These layers contain more substantial semantic information, which proves highly valuable for training OpenIPCS. Consequently, it becomes possible to disregard the other network layers within the CNNs. This aspect simplifies our approach and is easy to incorporate into different network architectures, unlike the original OpenIPCS building exclusively on the FCN decoder.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## IV. EXPERIMENTS SETUP\n",
      "\n",
      "Due to the open-set recognition process being still dependent on models built in a closed-set [33], [38], our experiments are divided into three parts. First, in Section IV-A, we compare our proposed loss function, described in Section III-C, with other significant loss functions widely used in semantic segmentation or recently introduced to improve discriminability in deep neural networks. Subsequently, in Section IV-B, we employ the proposed G-OpenIPCS approach detailed in Section III-D to evaluate the performance of models trained with the loss functions from the previous experiment in an open-set tattoo semantic segmentation scenario. Finally, we compare the performance of our openset semantic segmentation approach with other state-ofthe-art open-set semantic segmentation methods. Figure 4 provides an overview of the proposed method. Each stage of Figure 4 is detailed as follows.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## A. CLOSED-SET SEMANTIC SEGMENTATION\n",
      "\n",
      "Datasets . A total of 2,045 images from TSSD2023 were used to train and evaluate the closed-set models. These images follow the following division: 1,404 for training, 387 for validation, and 254 for closed set testing, as described in Section III-A.\n",
      "\n",
      "Pre-processing and Data augmentations . This step involves a sequence of transformations to train and improve the semantic segmentation network, motivated by the limited dataset of training images available in TSSD2023 around the high diversity of tattoos that virtually have no limits in the real world. This process involves the following steps: a) Implement a resizing operation, which can be either random or center crop, or resize to get an image with a resolution of 224 √ó 224; b) Next, there is a 50% probability of applying one of the geometric transformations to the image, including horizontal and vertical flipping, shifting, scaling, or rotation. These transformations have a scale and shift limit of 0.2, and rotation is limited to a maximum of positive or negative 45 degrees; c) Executing the tattoo semantic augmentation method proposed by this work, as described in more detail below. This augmentation involves RGB shifting, conversion to grayscale, and the application of random tone curve transformations. Each of these transformations has a 25% probability of being applied to each class within an image; d) Subsequently, there is a 50% chance of applying certain\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "IEEEAcCeSS'\n",
      "\n",
      "Logit space Network Input FIGURE 4. Overview of the proposed methods. Closed-set segmentations are obtained from models trained by different loss functions. The proposed Large-Margin Focal Loss expands the decision boundaries between classes to improve class discrimination while dealing with the problem of imbalance between classes. Increasing the spacing between classes allows unknown samples to be projected into open space. The open-set segmentation process is highlighted within the green block. The proposed G-OpenIPCS recognizes UUC pixels, leaning on discrepancies between feature vectors. This approach uses only the last two layers of the segmentation model. The penultimate layer equals 768 channels for SegFormer and 512 channels for Swin+UPNet.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "Feature vector C + 768 G OpenIPCS 3 x H x W global modifications to the image. These modifications can consist of random adjustments to brightness and contrast with a limit of 0.3, Gaussian blur with a 5-neighbor mask, or fancy PCA transformation; e) Next, coarse dropout transformation [58] with a maximum of 8 plots per image with the maximum resolution of 32x32 is applied; f) Next, the images are normalized using the mean and standard deviation values obtained from the ImageNet [59]; g) Finally, there is a 50% probability of applying the CutMix transformation [60] at the batch level to blend the images. All of these transformations were applied or developed with the on of the Albumentations library 4 .\n",
      "\n",
      "Open-set Semantic Segmentation Open-set classifier Implementation details . Due to recent advances presented by the semantic segmentation networks based on transformers [61]-[63], this work uses the SegFormer [63], which consists of a hierarchical Transformer encoder with lightweight multilayer perceptron (MLP) decoders. The model was pre-trained on the ImageNet dataset [59] and was acquired through the Hugging Face library 5 . During closed-set training, a batch size of 16 images on an RTX 3090 with 500 epochs was utilized, employing a patience factor of 10 epochs. The Nadam optimizer [64] was used, with an initial learning rate set at 0 . 0001 and a weight decay coefficient of 0 . 00001 .' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Baseline loss functions . In order to investigate the performance of large-margin focal loss ( L LMFCL ) in both closedset and open-set semantic segmentation, we conducted a comparative study with five other loss functions: i) crossentropy loss ( L CE ) - it was selected because it is widely used and a standard choice for semantic segmentation; ii) focal loss ( L FCL ) [51] - this is a commonly used variation\n",
      "\n",
      "4 https://albumentations.ai/\n",
      "\n",
      "5 https://huggingface.co/' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Training Closed-set Semantic Segmentation Large-Margin Focal Loss C x H x W UUC KKC 1 KKC 2 Test KKC 3 of cross-entropy that is effective in addressing class imbalance; iii) Distance-based cross-entropy loss ( L DCE ) [65] and iv) Distance-based cross-entropy loss combined with Variance Loss ( L DCE + V L ) [42] were selected as they have been proposed to enhance the results of open-set recognition by increasing the inter-class distance and decreasing the intra-class distance to produce more discriminative features; v) label-distribution-aware margin loss ( L LDAM ) [66] was chosen as a key loss for long-tailed recognition. It increases the margin for tail classes and decreases for main classes based on class frequency; vi) dice similarity coefficient loss ( L DSC ++ ) [67] recently introduced loss that combines the robustness of the L DSC to class imbalance while penalizing overconfident predictions, hence improving the performance of semantic segmentation on out-of-distribution data; vii) large-margin cross-entropy loss ( L LMCE ) [20] was chosen because it has recently been proposed to improve the results of closed-set classifiers by increasing the space between the decision boundaries of each class' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='. Moreover, we utilized the L LMCE as a foundation to develop our proposed loss function for the open-set tattoo semantic segmentation on a scenario that presents various challenges, such as high intraclass variability, class imbalance, and small tattoos, while enhanced discriminative capabilities in closed-set and openset semantic segmentation. Table 6 presents the loss function parameters used for model training.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Metrics . In order to assess the results of closed-set semantic segmentation, followed by open-set segmentation, we employed the following metrics. Firstly, we used the Area Under the ROC Curve (AUROC) because it has been recently utilized to assess open-set semantic segmentation [38], [42]. This metric aids in measuring the model's ability to distinguish between classes, particularly with respect\n",
      "\n",
      "TABLE 6. Parameters per each loss function utilized for the model training.\n",
      "\n",
      "| Ref.   | Loss       | Parameters                |\n",
      "|--------|------------|---------------------------|\n",
      "| -      | L CE       | -                         |\n",
      "| [51]   | L FCL      | Œ± : 1.0, Œ≥ : 2.0          |\n",
      "| [65]   | L DCE      | T : 3                     |\n",
      "| [66]   | L LDAM     | s : 30, Œª max : 0.5       |\n",
      "| [20]   | L LMCE     | Œª : 0.3                   |\n",
      "| [42]   | L DCE + VL | T : 3, Œª vl : 0.01        |\n",
      "| [67]   | L DSC ++   | Œ≥ : 2.0                   |\n",
      "| Ours   | L LMFCL    | Œª : 0.3, Œ± : 1.0, Œ≥ : 2.0 |\n",
      "\n",
      "to UUC. Secondly, the macro-averaged F1-Score (Macro F1), recommended in previous works for open-class classification [68], [69]. It helps evaluate the model's precision and recall of the segmented pixels. Lastly, the Mean Intersection over Union (mIoU) is a standard metric for semantic segmentation evaluation that provides an overview of the model's performance across all classes, including both KKCs and UUC.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## B. OPEN-SET SEMANTIC SEGMENTATION\n",
      "\n",
      "Datasets . G-OpenIPCS was trained using just the 387 validation images. The test open-set with 315 images was used to evaluate the proposed method, as detailed in Section III-A.\n",
      "\n",
      "Implementation details . For each model trained using the evaluated loss functions in closed-set semantic segmentation, a G-OpenIPCS model was trained to perform semantic segmentation in an open-set scenario. As depicted in Figure 4, our approach utilizes the last two connected layers of the segmentation network to construct feature vectors for each pixel. In the case of SegFormer, this feature vector is equal to the number of KKCs plus the logit vector of the inner layer, with a length of 768. Subsequently, we defined 64 principal components for the adjustment in GOpenIPCS. It is worth noting that the feature vector size and the number of principal components may vary depending on the semantic segmentation model and the application domain. Once G-OpenIPCS is trained, it becomes possible to recognize UUCs by considering the discrepancy between the raw feature vector and the projected feature vector, as outlined in Equation 8.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Baseline . Initially, a comparative analysis of the openset semantic segmentation results was conducted using GOpenIPCS trained for each evaluated loss function. This step serves to validate the performance of the proposed loss function in the open-set scenario. Subsequently, our approach was compared with other significant open-set segmentation methods proposed in the state-of-the-art literature. These methods include OpenIPCS [38], Anomalous Probability Map (APM) [42], Maximum Unnormalized Logit (MaxLogit) [39], and Maximum Softmax Probability (MSP) [43]. For these methods, training followed the same configurations as described in Section IV-A, except that only geometric transformations were applied. All networks were pre-trained on the ImageNet dataset [59]. Specifically, for DRN50+PSPNet [42] and RN101+PSPNet [39], images were resized to dimensions of 250, 300, 350, 400, and 450,\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "with a down-sampling factor of 8 for DRN50. Additionally, we implemented SoftMax-Thresholding (SoftMax-T) [38] and OpenMax [36] as the baselines for our implementation. As this is the first study on tattoo semantic segmentation and without other tattoo semantic datasets, we only compare our approach with state-of-the-art methods that do not use auxiliary data to supply their open-set recognition models. Finally, to demonstrate the generalization of our approach with baselines, we replicated the experiments, replacing the SegFormer with the Swin+UperNet model [70].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Cutoff thresholds . All the methods compared in Section V-C require a cutoff threshold to distinguish between KKC and UUC pixels. To realistically undertake an open-set recognition task, these limits are defined empirically based on the available KKCs during the validation data following the conditions of the original papers. No information about the UUCs is used to select the cutoff thresholds for the openset classifiers. For the proposed G-OpenIPCS, the preset values of the cutoff thresholds ( Œª out ) are determined based on TPR quantiles, as outlined in [38].\n",
      "\n",
      "Metrics . To evaluate the semantic segmentation results in an open set, we kept the AUROC, Macro F1, and mIoU used in the experiment in a closed set, described in Section IV-A. Additionally, we highlight the results involving UUCs.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## V. RESULTS AND DISCUSSIONS\n",
      "\n",
      "This section presents the results and discussions obtained from our experiments. The purpose is to validate the impact of the proposed Large-Margin Focal Loss on closed and open-set tattoo semantic segmentation, in addition to demonstrating the effectiveness of the proposed GOpenIPCS for open-set semantic segmentation. Initially, in Section V-A, we present the results of the loss functions evaluated in this work in a closed set scenario. Next, in Section V-B, we evaluate the performance of models trained using the proposed G-OpenIPCS method. Finally, in Section V-C, we compare our proposed approach with other state-of-the-art techniques.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## A. CLOSED-SET SEMANTIC SEGMENTATION\n",
      "\n",
      "This section focuses on evaluating the performance impact of the proposed L LMFCL in a closed-set semantic segmentation scenario. To demonstrate this, several loss functions from the literature were compared, as described in Section IV-A. Table 7 presents the overall results for all evaluated loss functions in terms of AUROC, Macro F1, and mIoU. When evaluating the performance of loss functions, L LMFCL outperforms all other metrics in terms of Macro F1 and maintains consistent results across AUROC and mIoU. This makes L LMFCL a suitable choice for closedset semantic segmentation. However, L LDAM and L LMCE exhibit an outperform over L LMFCL in terms of AUROC and mIoU, respectively. Additionally, L FCL stood out by producing results that closely resemble those of L LMFCL and L LMFCL , despite not incorporating a large margin in\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "TABLE 7. Comparison of the closed-set segmentation results achieved per each loss function. Bold values indicate the best overall results, including all loss functions. The results were obtained using SegFormer.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "TABLE 7. Comparison of the closed-set segmentation results achieved per each loss function. Bold values indicate the best overall results, including all loss functions. The results were obtained using SegFormer.\n",
      "\n",
      "| Ref.   | Loss       | AUROC        | Macro F1     | mIoU         |\n",
      "|--------|------------|--------------|--------------|--------------|\n",
      "| -      | L CE       | .7966 ¬± .122 | .6179 ¬± .224 | .4805 ¬± .216 |\n",
      "| [51]   | L FCL      | .8024 ¬± .111 | .6433 ¬± .213 | .5053 ¬± .209 |\n",
      "| [65]   | L DCE      | .8030 ¬± .118 | .6108 ¬± .219 | .4724 ¬± .218 |\n",
      "| [66]   | L LDAM     | .8295 ¬± .105 | .6497 ¬± .181 | .5057 ¬± .191 |\n",
      "| [20]   | L LMCE     | .8110 ¬± .112 | .6457 ¬± .217 | .5097 ¬± .216 |\n",
      "| [42]   | L DCE + VL | .7843 ¬± .120 | .5933 ¬± .217 | .4519 ¬± .205 |\n",
      "| [67]   | L DSC ++   | .8108 ¬± .108 | .6303 ¬± .200 | .4874 ¬± .196 |\n",
      "| Ours   | L LMFCL    | .8142 ¬± .102 | .6514 ¬± .189 | .5090 ¬± .194 |\n",
      "\n",
      "its formulation. This emphasizes its capability to address class imbalance encountered in TSSD2023.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='its formulation. This emphasizes its capability to address class imbalance encountered in TSSD2023.\n",
      "\n",
      "On the other hand, L CE , L DSC ++ , and particularly L DCE and L DCE + V L exhibit lower effectiveness compared to the other loss functions. Among these less favorable outcomes, L DCE + V L yielded the poorest results. This could indicate that the significant variability present in TSSD2023 posed challenges for the metric learning process. Thus, functions based on large-margin, such as proposed L LMFCL and L LMCE , were demonstrated to be more effective for closed-set semantic tattoo segmentations.\n",
      "\n",
      "TABLE 8. Closed-set segmentation results separated by classes obtained from the SegFormer trained using the proposed L LMFCL . The results were sorted in descending order of mIoU ‚Üë values. The top-5 best classes are highlighted in green , while the top-5 worst classes are highlighted in red .' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Table 8 presents the closed-set segmentation results categorized by class obtained from proposed L LMFCL . It emphasizes the top 5 best and top 5 worst results, excluding the background class. The top-5 best have high IoU values, indicating that the model's segmentation performance for these categories is particularly accurate. Additionally, they exhibit relatively high AUROC and Macro F1 scores. Notably, the classes within the top 5 best results do not necessarily constitute the majority of pixel quantity in TSSD2023, as illustrated in Figure 3. This suggests that the excellent performance comes from other aspects, such as the semantic dissimilarity of this set of tattoos to other classes, and mainly due to the recurrence of these tattoos being tattooed individually, without overlapping with other semantic classes, as can be briefly observed in Table 9. In contrast, the top 5 worst classes can be categorized as complementary tattoos, meaning they are rarely encountered in isolation but are typically accompanied by other predominantly dominant tattoos in terms of pixel quantity. For instance, a crown is almost always found with another element, such as a human or animal face. Leaves often complement flowers and stems, and water is nearly always associated with aquatic creatures like fishes, sharks, and octopuses.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='In outline, L LMFCL showed considerable impact in the closed-set semantic segmentation, presenting the best results overall. However, aspects can still be considered to improve performance in closed-set segmentation. In addition to dealing with class imbalance, it is still necessary to deal with the overlap between semantic classes, improving the accuracy of segmentations, especially in complementary tattoo classes. This will be detailed in Section VII.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## B. OPEN-SET SEMANTIC SEGMENTATION\n",
      "\n",
      "This section evaluates the impact on the performance of the proposed L LMFCL in an open-set scenario. Table 10' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='| Class      |   AUROC |   Macro F1 |   mIoU ‚Üë |\n",
      "|------------|---------|------------|----------|\n",
      "| background |  0.8886 |     0.9669 |   0.9359 |\n",
      "| tiger      |  0.9632 |     0.8656 |   0.7631 |\n",
      "| octopus    |  0.9102 |     0.8552 |   0.747  |\n",
      "| snake      |  0.927  |     0.8451 |   0.7318 |\n",
      "| key        |  0.9136 |     0.829  |   0.7079 |\n",
      "| owl        |  0.8813 |     0.8287 |   0.7076 |\n",
      "| butterfly  |  0.8729 |     0.8164 |   0.6897 |\n",
      "| lion       |  0.8714 |     0.8106 |   0.6816 |\n",
      "| star       |  0.8787 |     0.8028 |   0.6706 |\n",
      "| dog        |  0.8863 |     0.7868 |   0.6485 |\n",
      "| scorpion   |  0.9149 |     0.7586 |   0.611  |\n",
      "| flower     |  0.8212 |     0.7412 |   0.5888 |\n",
      "| fox        |  0.8302 |     0.7083 |   0.5483 |\n",
      "| fish       |  0.8278 |     0.7042 |   0.5435 |\n",
      "| shark      |  0.8604 |     0.6978 |   0.5359 |\n",
      "| gun        |  0.8507 |     0.6914 |   0.5284 |\n",
      "| cat        |  0.7772 |     0.6878 |   0.5241 |\n",
      "| anchor     |  0.8192 |     0.6565 |   0.4886 |\n",
      "| bird       |  0.7641 |     0.649  |   0.4804 |\n",
      "| diamond    |  0.853  |     0.628  |   0.4577 |\n",
      "| mermaid    |  0.915  |     0.6256 |   0.4551 |\n",
      "| eagle      |  0.8788 |     0.618  |   0.4472 |\n",
      "| spide      |  0.8606 |     0.5998 |   0.4283 |\n",
      "| heart      |  0.7696 |     0.5809 |   0.4093 |\n",
      "| ribbon     |  0.7572 |     0.5496 |   0.3789 |\n",
      "| wolf       |  0.8008 |     0.5403 |   0.3702 |\n",
      "| skull      |  0.7562 |     0.5393 |   0.3693 |\n",
      "| shield     |  0.7086 |     0.5375 |   0.3675 |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='| spide      |  0.8606 |     0.5998 |   0.4283 |\n",
      "| heart      |  0.7696 |     0.5809 |   0.4093 |\n",
      "| ribbon     |  0.7572 |     0.5496 |   0.3789 |\n",
      "| wolf       |  0.8008 |     0.5403 |   0.3702 |\n",
      "| skull      |  0.7562 |     0.5393 |   0.3693 |\n",
      "| shield     |  0.7086 |     0.5375 |   0.3675 |\n",
      "| crown      |  0.7241 |     0.4718 |   0.3087 |\n",
      "| leaf       |  0.6564 |     0.431  |   0.2747 |\n",
      "| water      |  0.6409 |     0.3939 |   0.2452 |\n",
      "| knife      |  0.5633 |     0.1796 |   0.0987 |\n",
      "| fire       |  0.5262 |     0.0989 |   0.052  |\n",
      "| all        |  0.8142 |     0.6514 |   0.509  |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='compares the open-set segmentation results obtained using various loss functions, all evaluated using the proposed GOpenIPCS method. Among the loss functions, L LMFCL achieves the highest AUROC score of 0.8013, Macro F1 with 0.6318, and also outperforms the mIoU score with 0.4900. These results indicate that L LMFCL is the most effective loss function in terms of overall performance. Considering only UUC, although L CE has the best performance in terms of AUROC (UUC). L LMFCL remains competitive in this metric while providing superior performance in terms of the F1-Score (UUC) and mIoU (UUC).\n",
      "\n",
      "Table 11 presents an evaluation of class-level performance from open-set semantic segmentation results from proposed L LMFCL . The top-5 best practically remained the same as\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "IEEE AccesS'\n",
      "\n",
      "TABLE 9. Visual result samples of open-set semantic segmentation for each loss function. UUCs are depicted as black pixels. The respective colors for other semantic classes can be found in Figure 2. The UUC has been highlighted in yellow , while the top-5 best classes are highlighted in green , and the top-5 worst classes are highlighted in red . The results were obtained using SegFormer and proposed G-OpenIPCS.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "TABLE 10. Comparison of the open-set segmentation results achieved per each loss function. All results are obtained on SegFormer and the proposed G-OpenIPCS method. Bold values indicate the best overall results, including all loss functions.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "TABLE 10. Comparison of the open-set segmentation results achieved per each loss function. All results are obtained on SegFormer and the proposed G-OpenIPCS method. Bold values indicate the best overall results, including all loss functions.\n",
      "\n",
      "| Ref.   | Loss       | AUROC        |   AUROC UUC | Macro F1     |   F1 Score UUC | mIoU         |   IoU UUC |\n",
      "|--------|------------|--------------|-------------|--------------|----------------|--------------|-----------|\n",
      "| -      | L CE       | .7737 ¬± .122 |      0.8204 | .5892 ¬± .234 |         0.3759 | .4525 ¬± .220 |    0.2314 |\n",
      "| [51]   | L FCL      | .7865 ¬± .112 |      0.7212 | .5919 ¬± .216 |         0.2886 | .4505 ¬± .207 |    0.1686 |\n",
      "| [65]   | L DCE      | .7827 ¬± .119 |      0.7333 | .5770 ¬± .215 |         0.3782 | .4357 ¬± .209 |    0.2332 |\n",
      "| [66]   | L LDAM     | .8077 ¬± .118 |      0.4977 | .5851 ¬± .202 |         0.0428 | .4399 ¬± .193 |    0.0219 |\n",
      "| [20]   | L LMCE     | .7975 ¬± .111 |      0.6916 | .6070 ¬± .209 |         0.4052 | .4650 ¬± .204 |    0.2541 |\n",
      "| [42]   | L DCE + VL | .7693 ¬± .116 |      0.7345 | .5681 ¬± .214 |         0.3903 | .4254 ¬± .201 |    0.2425 |\n",
      "| [67]   | L DSC ++   | .7939 ¬± .110 |      0.6353 | .5822 ¬± .203 |         0.2605 | .4375 ¬± .197 |    0.1497 |\n",
      "| Ours   | L LMFCL    | .8013 ¬± .104 |      0.7827 | .6318 ¬± .201 |         0.4318 | .4900 ¬± .201 |    0.2753 |\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "the closed-set results, which is natural given the dependence of the closed-set model of the proposed approach on openset recognition. However, the 'star' class was an exception, replacing the 'snake' class among the top 5 in terms of IoU. This is probably due to the low similarity of the 'star' class to the other semantic classes in TSSD2023, which practically maintained the performance obtained in the closed set. Classes that show more significant similarity to other classes, regardless of whether a KKC or a UUC, become more challenging for open-set segmentation, as they generate considerable uncertainty for the segmentation model.\n",
      "\n",
      "This case of declining performance owing to class similarity is also evident among the top 5 worst classes. Classes such as 'leaf', 'water', 'knife', and 'fire' remained in this category, underscoring their significant reliance on the closed-set model. However, there was an exception in the 'wolf' class, which, despite its poor performance in the closed set, experienced a deterioration after openset segmentation. This decline is attributed to its semantic similarity with other KKC animals, such as 'dog' and 'fox', and its resemblance to UUCs, particularly in bear tattoos. This provides misclassifications, as depicted in Figure 2.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Furthermore, KKCs can face challenges due to the high semantic variability exhibited within tattoos of the same class, arising from the infinite possibilities for representing an object through tattoos. For illustration, consider the representation of a 'cat' in various styles, such as cartoonish, realistic, stick-figure, geometric, and so on. It becomes exceedingly challenging to group all these diverse representations to ensure no shape is mistakenly isolated as an outlier and segmented as part of the unknown-unknown class (UUC). We endeavored to construct a robust data augmentation pipeline to address this limitation, as we believe it offers a potential solution (detailed in Section VII).\n",
      "\n",
      "Regarding the UUC, the results obtained using L LMFCL remain somewhat limited but show great promise. Notably, the UUC exhibited superior results in terms of IoU and F1Score when compared to the top 5 worst classes. However, in terms of AUROC, 12 KKCs are surpassed.\n",
      "\n",
      "In conclusion, utilizing the proposed L LMFCL in conjunction with the G-OpenIPCS achieves the best overall results in open-set segmentation tasks. Its robust segmentation performance, capable of effectively distinguishing between UUCs and KKCs, makes it a promising choice for challenging scenarios with high semantic variability between classes. However, it is essential to note that there are still tough situations and areas where further improvements are needed in the context of open-set segmentation for TSSD2023.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## C. COMPARISON WITH OTHERS OPEN-SET SEMANTIC SEGMENTATION METHODS\n",
      "\n",
      "Table 12 comprehensively compares different state-of-theart open-set semantic segmentation methods, including the proposed approach, and two more baselines: SoftMax-T and OpenMax. The proposed approach using L LMFCL and G-\n",
      "\n",
      "TABLE 11. Open-set segmentation results are separated by classes obtained from the SegFormer trained using the proposed L LMFCL . All results are obtained on the proposed G-OpenIPCS method. The results were sorted in descending order of mIoU ‚Üë values. The UUC has been highlighted in yellow , while the top-5 best classes are highlighted in green , and the top-5 worst classes are highlighted in red .' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='| Class      |   AUROC |   Macro F1 |   mIoU ‚Üë |\n",
      "|------------|---------|------------|----------|\n",
      "| background |  0.9025 |     0.9643 |   0.931  |\n",
      "| tiger      |  0.9599 |     0.8828 |   0.7901 |\n",
      "| key        |  0.9118 |     0.8589 |   0.7527 |\n",
      "| octopus    |  0.8978 |     0.8417 |   0.7267 |\n",
      "| owl        |  0.8777 |     0.8205 |   0.6956 |\n",
      "| star       |  0.8594 |     0.8188 |   0.6932 |\n",
      "| snake      |  0.9177 |     0.808  |   0.6779 |\n",
      "| butterfly  |  0.8714 |     0.8064 |   0.6756 |\n",
      "| dog        |  0.8584 |     0.769  |   0.6247 |\n",
      "| scorpion   |  0.894  |     0.7424 |   0.5903 |\n",
      "| diamond    |  0.842  |     0.7195 |   0.5619 |\n",
      "| lion       |  0.8449 |     0.7192 |   0.5615 |\n",
      "| fox        |  0.8279 |     0.7124 |   0.5533 |\n",
      "| flower     |  0.7971 |     0.7043 |   0.5435 |\n",
      "| fish       |  0.8221 |     0.6991 |   0.5374 |\n",
      "| gun        |  0.843  |     0.6972 |   0.5351 |\n",
      "| cat        |  0.7758 |     0.6965 |   0.5344 |\n",
      "| shark      |  0.8604 |     0.6798 |   0.515  |\n",
      "| mermaid    |  0.9158 |     0.6754 |   0.5099 |\n",
      "| bird       |  0.7631 |     0.6544 |   0.4863 |\n",
      "| anchor     |  0.8075 |     0.6431 |   0.474  |\n",
      "| eagle      |  0.878  |     0.5941 |   0.4226 |\n",
      "| skull      |  0.7502 |     0.5733 |   0.4019 |\n",
      "| shield     |  0.7063 |     0.5429 |   0.3725 |\n",
      "| spide      |  0.8202 |     0.542  |   0.3718 |\n",
      "| ribbon     |  0.7594 |     0.5402 |   0.37   |\n",
      "| crown      |  0.7121 |     0.4761 |   0.3124 |\n",
      "| heart      |  0.6827 |     0.4573 |   0.2964 |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='| skull      |  0.7502 |     0.5733 |   0.4019 |\n",
      "| shield     |  0.7063 |     0.5429 |   0.3725 |\n",
      "| spide      |  0.8202 |     0.542  |   0.3718 |\n",
      "| ribbon     |  0.7594 |     0.5402 |   0.37   |\n",
      "| crown      |  0.7121 |     0.4761 |   0.3124 |\n",
      "| heart      |  0.6827 |     0.4573 |   0.2964 |\n",
      "| unknown    |  0.7827 |     0.4318 |   0.2753 |\n",
      "| wolf       |  0.76   |     0.4178 |   0.2641 |\n",
      "| leaf       |  0.6462 |     0.4068 |   0.2553 |\n",
      "| water      |  0.6416 |     0.4017 |   0.2513 |\n",
      "| knife      |  0.5426 |     0.1322 |   0.0708 |\n",
      "| fire       |  0.5134 |     0.052  |   0.0267 |\n",
      "| all        |  0.8013 |     0.6318 |   0.49   |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='OpenIPCS stands out as the best-performing method. It achieves the highest AUROC, Macro F1, mIoU, and IoU (UUC) values, indicating its superiority in open-set tattoo semantic segmentation. The scores obtained for AUROC of 0.8013, Macro F1 of 0.6318, mIoU of 0.4900, and IoU (UUC) of 0.2753 are significant compared to other approaches in the literature. However, the performance difference is less to the baselines, except for the values obtained from IoU to UUC. This indicates that the proposed approach produces more accurate and visually coherent segmentation results.\n",
      "\n",
      "In comparison to the original OpenIPCS, it is essential to emphasize the strengths that make our approach superior. The original OpenIPCS relies on an FCN decoder that combines multiple layers from various levels of the network to construct the feature vector. In contrast, our G-OpenIPCS approach follows a more straightforward and intuitive path, considering that the critical features for class discrimination are primarily situated in the latter layers of the segmentation network. This approach leads to improved results and avoids the potential scrambling of high-level and low-level\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "TABLE 12. Comparison between the proposed approach and state-of-the-art open-set semantic segmentation techniques. Bold values indicate the best overall results, including all methods.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='| Ref.     | Loss       | Network           | Open Set Classifier   | AUROC        | Macro F1     | mIoU         |   IoU UUC |\n",
      "|----------|------------|-------------------|-----------------------|--------------|--------------|--------------|-----------|\n",
      "| Baseline | L CE       | SegFormer         | SoftMax-T             | .7855 ¬± .126 | .5782 ¬± .229 | .4398 ¬± .216 |    0.135  |\n",
      "| Baseline | L CE       | SegFormer         | OpenMax               | .7785 ¬± .123 | .5781 ¬± .227 | .4389 ¬± .212 |    0.1611 |\n",
      "| Baseline | L CE       | Swin + UPerNet    | SoftMax-T             | .7234 ¬± .095 | .4852 ¬± .198 | .3428 ¬± .188 |    0.1145 |\n",
      "| Baseline | L CE       | Swin + UPerNet    | OpenMax               | .7134 ¬± .095 | .4736 ¬± .197 | .3325 ¬± .181 |    0.1258 |\n",
      "| [38]     | L CE       | DN121 + FCN       | OpenIPCS              | .6523 ¬± .098 | .3596 ¬± .217 | .2422 ¬± .183 |    0.0632 |\n",
      "| [38]     | L CE       | WRN50 + FCN       | OpenIPCS              | .6695 ¬± .098 | .3983 ¬± .214 | .2722 ¬± .185 |    0.047  |\n",
      "| [42]     | L DCE + VL | DRN50 + PSPNet    | APM                   | .7123 ¬± .139 | .4290 ¬± .284 | .3152 ¬± .244 |    0.1657 |\n",
      "| [39]     | L CE       | RN101 + PSPNet    | MaxLogit              | .7358 ¬± .139 | .5173 ¬± .293 | .4031 ¬± .268 |    0.1439 |\n",
      "| [43]     | L CE + PC  | RN50 + DeepLabV3+ | MaxLogit              | .7048 ¬± .105 | .4471 ¬± .211 | .3126 ¬± .191 |    0.1327 |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='| [39]     | L CE       | RN101 + PSPNet    | MaxLogit              | .7358 ¬± .139 | .5173 ¬± .293 | .4031 ¬± .268 |    0.1439 |\n",
      "| [43]     | L CE + PC  | RN50 + DeepLabV3+ | MaxLogit              | .7048 ¬± .105 | .4471 ¬± .211 | .3126 ¬± .191 |    0.1327 |\n",
      "| [43]     | L CE + PC  | RN50 + DeepLabV3+ | MSP                   | .7401 ¬± .097 | .4694 ¬± .207 | .3157 ¬± .190 |    0.1354 |\n",
      "| Ours     | L LMFCL    | SegFormer         | G-OpenIPCS            | .8013 ¬± .104 | .6318 ¬± .201 | .4900 ¬± .201 |    0.2753 |\n",
      "| Ours     | L LMFCL    | Swin + UPerNet *  | G-OpenIPCS            | .7921 ¬± .116 | .5927 ¬± .211 | .4518 ¬± .213 |    0.1981 |' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='information that can occur when using multiple layers, as in the original OpenIPCS.\n",
      "\n",
      "Due to this more streamlined design choice, our approach integrates with other modern segmentation architectures, such as transform-based structures. For instance, we employed the SegFormer and Swin+UperNet networks, outperforming FCN-based models in open-set tattoo semantic segmentation. Notably, the combination using the SegFormer outperforms other methods by a significant margin. This underscores that our design choice directly enhances performance, especially considering the high reliance on features in open-set segmentation models [42].\n",
      "\n",
      "Moreover, it is worth noting that the original OpenIPCS is built upon L CE . While this loss function consistently delivers results in closed-set semantic segmentation, the challenge of open-set segmentation demands segmentation models with a heightened discriminative capacity, which is precisely what our proposed L LMFCL aims to enhance.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## D. ABLATION STUDIES OF THE DATA AUGMENTATION COMPONENTS\n",
      "\n",
      "We performed an ablation study to examine the impact of various data augmentation techniques on our approach to open-set recognition for tattoo semantic segmentation. These techniques were divided into four categories, as explained in Section IV-A: geometric, image adjustments, dropout, and our proposed CSA method. We also included a no-augmentation experiment as a baseline for comparison.\n",
      "\n",
      "The results in Table 13 indicate that our approach performs better when all data augmentation methods are combined. Among these methods, image adjustments consistently showed performance gains in mIoU and IoU for UUC. The other approaches, including geometric and proposed CSA, improved the values of IoU for UUC, while the dropout method improved the mIoU. Notably, the proposed CSA significantly improved performance when combined with the other three methods. In summary, our approach is appropriate for effective open-set tattoo semantic segmentation.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## E. EVALUATION OF THE PERFORMANCE IN THE CLOSED-SET TATTOO SEGMENTATION SCENARIO\n",
      "\n",
      "Semantic segmentation focuses on recognizing specific classes of tattoos, such as cats, dogs, stars, and others. Tattoo segmentation only aims to isolate the tattoo region from the background. This distinction is beneficial in situations that do not require detailed semantic information about tattoos. However, it may be interesting that our approach also behaves adequately in closed-set tattoo segmentation scenarios. Thus, we evaluated the performance of our proposed approach in a closed tattoo segmentation scenario using the DeMSI dataset [9], where tattoos were manually annotated without any semantic differentiation. In addition to measuring the IoU for tattoos, we also evaluate the False Positive Rate (FPR), which indicates the proportion of background pixels incorrectly classified as tattoos, and the False Negative Rate (FNR), which means the proportion of tattoo pixels incorrectly classified as background.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='The quantitative results are presented in Table 14. We observed a degradation in the performance of the proposed approach when directly applying the model trained on TSSD2023 and converting the multiclass output to a binary output. This degradation is primarily reflected in the FNR values, indicating that many tattoo pixels were misclassified as background. We attribute this behavior to the labeling design used in TSSD2023. The primary factor is the detailed annotations in TSSD2023, designed to accurately classify the pixel classes, as seen in Table 4. For example, the shark's mouth was separated from the shadow of the tattoo. In contrast, DeMSI does not provide refined annotations for tattoos, leading to multiple background pixels being incorrectly annotated as part of a tattoo, as shown in Table 15. The second factor relates to tattoo classes with insufficient semantic samples, such as dragons, letters, and spider webs, which were excluded in TSSD2023 and are ignored by our approach, as seen by the text tattoo in Table 15. Due to these differences in annotation design between DeMSI and TSSD2023 datasets, performance degradation occurs when directly applying our approach.\n",
      "\n",
      "To alleviate the performance degradation caused by annotation differences between the datasets, we fine-tuned the\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "IEEEAcCeSS'\n",
      "\n",
      "TABLE 13. Evaluation of the effect of data augmentation components. The results were obtained using proposed L LMFCL , SegFormer, and proposed G-OPenIPCS.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='To alleviate the performance degradation caused by annotation differences between the datasets, we fine-tuned the\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "IEEEAcCeSS'\n",
      "\n",
      "TABLE 13. Evaluation of the effect of data augmentation components. The results were obtained using proposed L LMFCL , SegFormer, and proposed G-OPenIPCS.\n",
      "\n",
      "| No Aug.   | Geometric   | Image Adjustments   | Dropout   | CSA (Ours)   | mIoU          |   IoU UUC |\n",
      "|-----------|-------------|---------------------|-----------|--------------|---------------|-----------|\n",
      "| ‚úì         |             |                     |           |              | 0.4556 ¬± .186 |    0.1547 |\n",
      "|           | ‚úì           |                     |           |              | 0.4320 ¬± .182 |    0.1797 |\n",
      "|           |             | ‚úì                   |           |              | 0.4814 ¬± .203 |    0.1884 |\n",
      "|           |             |                     | ‚úì         |              | 0.4959 ¬± .189 |    0.1538 |\n",
      "|           |             |                     |           | ‚úì            | 0.4576 ¬± .203 |    0.177  |\n",
      "|           | ‚úì           | ‚úì                   | ‚úì         |              | 0.4733 ¬± .196 |    0.2303 |\n",
      "|           | ‚úì           | ‚úì                   | ‚úì         | ‚úì            | 0.4900 ¬± .201 |    0.2753 |\n",
      "\n",
      "TABLE 14. Evaluation of the performance on the DeMSI dataset in the closed-set tattoo segmentation scenario. The results were obtained using the proposed L LMFCL and SegFormer.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='TABLE 14. Evaluation of the performance on the DeMSI dataset in the closed-set tattoo segmentation scenario. The results were obtained using the proposed L LMFCL and SegFormer.\n",
      "\n",
      "| Fine-tuned   |   IoU Tattoo |    FPR |    FNR |\n",
      "|--------------|--------------|--------|--------|\n",
      "|              |       0.3878 | 0.0234 | 0.5784 |\n",
      "| ‚úì            |       0.8232 | 0.0278 | 0.0945 |\n",
      "\n",
      "SegFormer network's last layer using the DeMSI dataset. For the fit, was used 60% of the images for training and 40% for testing as proposed by [13]. After this adjustment, our approach proved effective for performing closed-set tattoo segmentation, as evidenced by the significantly reduced FNR values in Table 14. Visually, the improvement in segmentation quality before and after fine-tuning can be observed in Table 15.\n",
      "\n",
      "TABLE 15. Visual result samples of closed-set tattoo segmentation on the DeMSI dataset. The tattoo and background pixels are depicted as black and white, respectively. The results were obtained using the proposed L LMFCL and SegFormer.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## VI. CONCLUSIONS\n",
      "\n",
      "This paper built a novo tattoo semantic segmentation dataset called TSSD2023, introducing an unexplored and challenging problem in semantic tattoo recognition. This dataset can serve as a valuable basis for future research.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='Furthermore, the paper has presented the proposed LargeMargin Focal Loss ( L LMFCL ) to enhance tattoo semantic segmentation outcomes in both closed and open-set scenarios. In the closed-set scenario, L LMFCL performed competitively and outperformed other evaluated loss functions in terms of Macro F1, demonstrating its suitability for closed-set semantic segmentation. An in-depth analysis at the class level revealed that superior performance is generally observed in classes depicted by high semantic dissimilarity and minimal overlap with other tattoos within an image. Conversely, the most challenging classes rarely appear in isolation, often serving as complementary components to other tattoos. In the open-set scenario, this paper proposes a generalized approach for the OpenIPCS method named G-OpenIPCS that facilitates the integration of this open-set classifier with more modern segmentation network architectures, such as transform-based networks. Using GOpenIPCS, we compared the performance of different loss functions with the proposed L LMFCL , which showed higher scores regarding AUROC, Macro F1, and overall mIoU. When considering only the UUCs, L LMFCL performed competitively in AUROC while outperforming F1-Score and IoU. This highlights its effectiveness in the domain of open-set tattoo semantic segmentation' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='. When considering only the UUCs, L LMFCL performed competitively in AUROC while outperforming F1-Score and IoU. This highlights its effectiveness in the domain of open-set tattoo semantic segmentation. A more thorough examination revealed that classes bearing high similarity to other classes, overlapped, and with limited samples, it represented the significant challenges for open-set tattoo semantic segmentation.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='It is worth noting that when a semantic tattoo class has high variability, segmentation errors can occur due to false outliers. To address this issue, we propose a new data augmentation technique named Class Semantic Augmentation (CSA) that increases the available semantic information of classes for better model generalization. Nevertheless, there are opportunities for further advancements in this research area.\n",
      "\n",
      "When comparing our approach, which combines L LMFCL , SegFormer, and G-OpenIPCS, with other stateof-the-art methods for open-set tattoo semantic segmentation, our approach consistently attains the highest overall results. It surpasses the performance of other state-of-the-art methods, underscoring its potential for addressing challenging scenarios in open-set semantic tattoo segmentation.\n",
      "\n",
      "Furthermore, our approach applicability extends beyond the domain of tattoo segmentation, and it holds the potential to enhance open-set semantic segmentation in various other application domains. Finally, it is essential to note that while our approach demonstrates promise, there remains room for further improvements in specific areas. This includes improving the handling of class imbalance, class overlap, high intra-class semantic variability, and, in some cases, high inter-class similarity, detailed as follows.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## VII. CHALLENGES IN OPEN SET TATTOO SEMANTIC SEGMENTATION\n",
      "\n",
      "Although the approach discussed here is promising and with results superior to state-of-the-art methods for openset semantic segmentation, in the context of tattoos, we can\n",
      "\n",
      "observe numerous difficulties and open points for this line of research. Below, we detail some of those that we consider most relevant in the proposed context:' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## A. SEMANTIC SHIFT\n",
      "\n",
      "Semantic shift is caused by classes that influence model predictions due to their semantic similarity to other classes, causing segmentation errors [33], [37]. This semantic shift can be caused in three different ways in the tattoo dataset. The first is the presence of invisible objects in the background class that are similar to known and unknown classes. The background class is a large grouping of objects relatively irrelevant to the application. However, tattoos are semantically very similar to illustrations projected onto different surfaces. For example, a skull on a tattoo artist's chair can be easily identified as a tattoo, generating a misclassification. Furthermore, due to the great difficulty in annotating a large volume of tattoos for semantic segmentation, some tattoos are 'ignored' in the labeling process and noted as belonging to the background class, which can cause misclassifications.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='The second comprises objects of known classes are similar to each other. Some semantic classes are similar to others within the set of known classes. For example, the 'wolf', 'dog', and 'fox' classes belong to the same animal family, thus presenting similar characteristics that even humans may find difficult to distinguish. This high similarity makes the process of recognition by the segmentation model difficult, which can, in some cases, generate classification errors. Furthermore, this semantic similarity can extend to small parts of the tattoos, which can cause small segmentation errors. Finally, the third includes objects of unknown classes similar to known classes. This challenge is similar to the previous one but more demanding. The segmentation model must be trained to produce robust decision boundaries to separate known data from each other and separate it from unknown data. While in the previous problem, it is only necessary to distinguish among known classes.\n",
      "\n",
      "A possible way to alleviate these factors' influence is to make a descriptive note of the tattoo using natural language. This can help identify tattoos more effectively, avoiding reducing the artistic feature of a tattoo to a set of labels.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## B. INTRA-CLASS VARIABILITY\n",
      "\n",
      "An open challange is also related to objects from known classes with different characteristics from the group of objects from the same semantic class. Because tattoos are a form of artistic expression based on drawings, the number of designs that can be thought of to create a tattoo is practically unlimited. This infinite universe of possibilities makes each tattoo unique, with the variability of tattoos present in the real world being practically immeasurable. When defining a semantic class for a set of objects, we intuitively state that these objects are similar, which is generally true. However, concerning tattoos, it is possible that despite belonging to the same semantic class, practically none of the patterns are\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "identically replicated between tattoos. In some cases, the patterns are so different between objects of the known class that the segmentation model segments the known tattoo as belonging to the unknown class.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## C. OBJECTS OF KNOWN AND UNKNOWN CLASSES SEGMENTED WITH LOW PRECISION\n",
      "\n",
      "This challenge incorporates several subproblems encountered in closed-set segmentation and also in open-set segmentation. In segmenting closed sets, the models suffer from the subproblem of overlap between objects, making it difficult to separate the boundaries between objects accurately. Another common subproblem is small objects present in images, generally classified as objects from other classes, such as the background class. The open-set semantic segmentation task inherits all these subproblems. However, in open-set segmentation, we still have the challenge of accurately classifying regions of completely unknown objects while dealing with the subproblems derived from closed-set segmentation.\n",
      "\n",
      "## D. DATA AUGMENTATION FOR SEMANTIC SEGMENTATION\n",
      "\n",
      "The data augmentation proposal presented here may be promising for incorporating more data into tattoo datasets, the annotation of which is costly and time-consuming. A possible path is to combine the approach presented in [13] with the data augmentation ideas proposed in this paper in order to create greater variability of classes and examples, including data in an open set, to enable the training of more complex models and, consequently, increase final performance.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## E. COMPUTATIONAL COMPLEXITY\n",
      "\n",
      "We understand that a detailed analysis of computational complexity must be conducted for certain applications, mainly involving embedded systems. This analysis must include the training and deployment of the model according to the target device. Specific architectures for this type of application can also be evaluated, as demonstrated in [71].' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='## REFERENCES\n",
      "\n",
      "- [1] S. T. Acton and A. Rossi, 'Matching and retrieval of tattoo images: Active contour CBIR and glocal image features,' in Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation, 2008, pp. 21-24.\n",
      "- [2] T. Harbert, 'FBI wants better automated image analysis for tattoos [news],' IEEE Spectrum, vol. 52, no. 9, pp. 13-16, 2015.\n",
      "- [3] J.-E. Lee, R. Jin, and A. K. Jain, 'Rank-based distance metric learning: An application to image retrieval,' in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2008, pp. 1-8.\n",
      "- [4] S. Fang, J. Coverdale, P. Nguyen, and M. Gordon, 'Tattoo recognition in screening for victims of human trafficking,' Journal of Nervous &amp; Mental Disease, vol. 206, no. 10, pp. 824-827, 2018.\n",
      "- [5] F. Bacchini and L. Lorusso, 'A tattoo is not a face. ethical aspects of tattoobased biometrics,' Journal of Information, Communication and Ethics in Society, vol. 16, no. 2, pp. 110-122, 2017.\n",
      "- [6] R. T. Da Silva and H. S. Lopes, 'A transfer learning approach for the tattoo classification problem,' in 2022 IEEE Latin American Conference on Computational Intelligence (LA-CCI). IEEE, 2022, pp. 1-6.\n",
      "\n",
      "<!-- image -->' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [7] Y. Mo, Y. Wu, X. Yang, F. Liu, and Y. Liao, 'Review the state-ofthe-art technologies of semantic segmentation based on deep learning,' Neurocomputing, vol. 493, pp. 626-646, 2022.\n",
      "- [8] C. Jiawang and Z. Yuan, 'Tattoo recognition based on triplet GAN,' in Proceedings pf 37th Chinese Control Conference. IEEE, 2018, pp. 95959597.\n",
      "- [9] T. Hrka¬¥ c, K. Brki¬¥ c, and Z. Kalafati¬¥ c, 'Tattoo detection for soft biometric de-identification based on convolutional neural networks,' in Proceedings of the OAGM &amp; ARW Joint Workshop. Verlag der Technischen Universit√§t Graz, 2016.\n",
      "- [10] J. Dong, X. Qu, and H. Li, 'Color tattoo segmentation based on skin color space and k-mean clustering,' in Proceedings of the IEEE 4th International Conference on Information, Cybernetics and Computational Social Systems, 2017, pp. 53-56.\n",
      "- [11] M. Nicol√°s-D√≠az, A. Morales-Gonz√°lez, and H. M√©ndez-V√°zquez, 'Weighted average pooling of deep features for tattoo identification,' Multimedia Tools and Applications, vol. 81, no. 18, pp. 25 853-25 875, 2022.\n",
      "- [12] M. Ngan and P. Grother, 'Tattoo recognition technology - challenge (tattc): an open tattoo database for developing tattoo recognition research,' in Proceedings of the IEEE International Conference on Identity, Security and Behavior Analysis, 2015, pp. 1-6.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [12] M. Ngan and P. Grother, 'Tattoo recognition technology - challenge (tattc): an open tattoo database for developing tattoo recognition research,' in Proceedings of the IEEE International Conference on Identity, Security and Behavior Analysis, 2015, pp. 1-6.\n",
      "- [13] L. J. Gonzalez-Soler, C. Rathgeb, and D. Fischer, 'Semi-synthetic data generation for tattoo segmentation,' in 2023 11th International Workshop on Biometrics and Forensics, 2023, pp. 1-6.\n",
      "- [14] C. Geng, S.-j. Huang, and S. Chen, 'Recent advances in open set recognition: A survey,' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 3614-3631, 2020.\n",
      "- [15] C. C. Da Silva, K. Nogueira, H. N. Oliveira, and J. A. dos Santos, 'Towards open-set semantic segmentation of aerial images,' in Proceedings of the IEEE Latin American GRSS &amp; ISPRS Remote Sensing Conference. Santiago, Chile: IEEE, 2020, pp. 16-21.\n",
      "- [16] I. Nunes, M. B. Pereira, H. Oliveira, J. A. dos Santos, and M. Poggi, 'Conditional reconstruction for open-set semantic segmentation,' in Proceedings of the IEEE International Conference on Image Processing. Bordeaux, France: IEEE, 2022, pp. 946-950.\n",
      "- [17] M. Gutoski, A. E. Lazzaretti, and H. S. Lopes, 'Deep metric learning for open-set human action recognition in videos,' Neural Computing and Applications, vol. 33, pp. 1207-1220, 2021.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [17] M. Gutoski, A. E. Lazzaretti, and H. S. Lopes, 'Deep metric learning for open-set human action recognition in videos,' Neural Computing and Applications, vol. 33, pp. 1207-1220, 2021.\n",
      "- [18] D. Miller, N. Sunderhauf, M. Milford, and F. Dayoub, 'Class anchor clustering: A loss for distance-based open set recognition,' in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. Waikoloa, USA: IEEE, 2021, pp. 3570-3578.\n",
      "- [19] W. Liu, Y. Wen, Z. Yu, and M. Yang, 'Large-margin softmax loss for convolutional neural networks,' arXiv preprint arXiv:1612.02295, 2016.\n",
      "- [20] T. Kobayashi, 'Large margin in softmax cross-entropy loss,' in Proceedings of the British Machine Vision Conference (BMVC), 2019.\n",
      "- [21] X. Li, D. Chang, T. Tian, and J. Cao, 'Large-margin regularized softmax cross-entropy loss,' IEEE access, vol. 7, pp. 19 572-19 578, 2019.\n",
      "- [22] Q. Xu, S. Ghosh, X. Xu, Y. Huang, and A. W. K. Kong, 'Tattoo detection based on CNN and remarks on the NIST database,' in Proceedings of the IEEE International Conference on Biometrics, 2016, pp. 1-7.\n",
      "- [23] H. Han, J. Li, A. K. Jain, S. Shan, and X. Chen, 'Tattoo image search at scale: Joint detection and compact representation learning,' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 10, pp. 2333-2348, 2019.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [23] H. Han, J. Li, A. K. Jain, S. Shan, and X. Chen, 'Tattoo image search at scale: Joint detection and compact representation learning,' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 10, pp. 2333-2348, 2019.\n",
      "- [24] M. Nicol√°s-D√≠az, A. Morales-Gonz√°lez, and H. M√©ndez-V√°zquez, 'Deep generic features for tattoo identification,' in Proceedings of the 24th Iberoamerican Congress on Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. Berlin, Heidelberg: SpringerVerlag, 2019, p. 272-282.\n",
      "- [25] A. Jain, Y. Chen, and U. Park, 'Scars, marks &amp; tattoos (SMT): Physical attributes for person identification,' Michigan State University, East Lansing, USA, Tech. Rep. CSE 07-22, 2007.\n",
      "- [26] A. K. Jain, J.-E. Lee, and R. Jin, 'Tattoo-ID: Automatic tattoo image retrieval for suspect and victim identification,' in Advances in Multimedia Information Processing. Heidelberg: Springer, 2007, pp. 256-265.\n",
      "- [27] J. D. Allen, N. Zhao, J. Yuan, and X. Liu, 'Unsupervised tattoo segmentation combining bottom-up and top-down cues,' in Proceedings of SPIE Mobile Multimedia/Image Processing, Security, and Applications Conference, S. S. Agaian, S. A. Jassim, and Y. Du, Eds., 2011, pp. 1-7.\n",
      "- [28] B. Heflin, W. Scheirer, and T. E. Boult, 'Detecting and classifying scars, marks, and tattoos found in the wild,' in Proceedings of the IEEE Fifth In-' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [28] B. Heflin, W. Scheirer, and T. E. Boult, 'Detecting and classifying scars, marks, and tattoos found in the wild,' in Proceedings of the IEEE Fifth In-\n",
      "23. ternational Conference on Biometrics: Theory, Applications and Systems (BTAS), 2012, pp. 31-38.\n",
      "- [29] P. Duangphasuk and W. Kurutach, 'Tattoo skin detection and segmentation using image negative method,' in Proceedings of the IEEE 13th International Symposium on Communications and Information Technologies (ISCIT), 2013, pp. 354-359.\n",
      "- [30] J. Kim, A. Parra, H. Li, and E. J. Delp, 'Efficient graph-cut tattoo segmentation,' in Visual Information Processing and Communication VI, A. Said, O. G. Guleryuz, and R. L. Stevenson, Eds. SPIE, 2015, pp. 1-8.\n",
      "- [31] T. Hrka¬¥ c, K. B. S. Ribari¬¥ c, and D. MarÀá ceti¬¥ c, 'Deep learning architectures for tattoo detection and de-identification,' in Proceedings of the IEEE First International Workshop on Sensing, Processing and Learning for Intelligent Machines (SPLINE), 2016, pp. 1-5.\n",
      "- [32] J. Long, E. Shelhamer, and T. Darrell, 'Fully convolutional networks for semantic segmentation,' in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431-3440.\n",
      "- [33] A. Brilhador, A. E. Lazzaretti, and H. S. Lopes, 'A comparative study for open set semantic segmentation methods,' in Anais do 15 Congresso Brasileiro de Intelig√™ncia Computacional. Joinville, Brazil: SBIC, 2021, pp. 1-8.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [33] A. Brilhador, A. E. Lazzaretti, and H. S. Lopes, 'A comparative study for open set semantic segmentation methods,' in Anais do 15 Congresso Brasileiro de Intelig√™ncia Computacional. Joinville, Brazil: SBIC, 2021, pp. 1-8.\n",
      "- [34] I. Nunes, C. Laranjeira, H. Oliveira, and J. A. dos Santos, 'A systematic review on open-set segmentation,' Computers &amp; Graphics, vol. 115, pp. 296-308, 2023.\n",
      "- [35] Y. Tian, D. Su, S. Lauria, and X. Liu, 'Recent advances on loss functions in deep learning for computer vision,' Neurocomputing, vol. 497, pp. 129158, 2022.\n",
      "- [36] Y. Liu, Y. Tang, L. Zhang, L. Liu, M. Song, K. Gong, Y. Peng, J. Hou, and T. Jiang, 'Hyperspectral open set classification with unknown classes rejection towards deep networks,' International Journal of Remote Sensing, vol. 41, no. 16, pp. 6355-6383, 2020.\n",
      "- [37] Z. Cui, W. Longshi, and R. Wang, 'Open set semantic segmentation with statistical test and adaptive threshold,' in Proceedings of the IEEE International Conference on Multimedia and Expo, 2020, pp. 1-6.\n",
      "- [38] H. Oliveira, C. Silva, G. L. Machado, K. Nogueira, and J. A. dos Santos, 'Fully convolutional open set segmentation,' Machine Learning, pp. 1-52, 2021.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [38] H. Oliveira, C. Silva, G. L. Machado, K. Nogueira, and J. A. dos Santos, 'Fully convolutional open set segmentation,' Machine Learning, pp. 1-52, 2021.\n",
      "- [39] D. Hendrycks, S. Basart, M. Mazeika, A. Zou, J. Kwon, M. Mostajabi, J. Steinhardt, and D. Song, 'Scaling out-of-distribution detection for real-world settings,' in Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 17-23 Jul 2022, pp. 8759-8773.\n",
      "- [40] M. Grci¬¥ c, P. Bevandi¬¥ c, and S. ≈†egvi¬¥ c, 'Densehybrid: Hybrid anomaly detection for dense open-set recognition,' in European Conference on Computer Vision. Springer, 2022, pp. 500-517.\n",
      "- [41] X. Guo, J. Liu, T. Liu, and Y. Yuan, 'Handling open-set noise and novel target recognition in domain adaptive semantic segmentation,' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 8, pp. 9846-9861, 2023.\n",
      "- [42] J. Cen, P. Yun, J. Cai, M. Y. Wang, and M. Liu, 'Deep metric learning for open world semantic segmentation,' in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 333-15 342.\n",
      "- [43] J. Hong, W. Li, J. Han, J. Zheng, P. Fang, M. Harandi, and L. Petersson, 'Goss: Towards generalized open-set semantic segmentation,' The Visual Computer, pp. 1-14, 2023.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [43] J. Hong, W. Li, J. Han, J. Zheng, P. Fang, M. Harandi, and L. Petersson, 'Goss: Towards generalized open-set semantic segmentation,' The Visual Computer, pp. 1-14, 2023.\n",
      "- [44] H. Zhang and H. Ding, 'Prototypical matching and open set rejection for zero-shot semantic segmentation,' in Procceding of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6974-6983.\n",
      "- [45] B. Yu, T. Liu, M. Gong, C. Ding, and D. Tao, 'Correcting the triplet selection bias for triplet loss,' in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 71-87.\n",
      "- [46] S. N. Rai, F. Cermelli, B. Caputo, and C. Masone, 'Mask2anomaly: Mask transformer for universal open-set segmentation,' arXiv preprint arXiv:2309.04573, 2023.\n",
      "- [47] M. J. Wilber, E. Rudd, B. Heflin, Y.-M. Lui, and T. E. Boult, 'Exemplar codes for facial attributes and tattoo recognition,' in Proceedings of the IEEE Winter Conference on Applications of Computer Vision, 2014, pp. 205-212.\n",
      "- [48] W. He, J. Wang, L. Wang, R. Pan, and W. Gao, 'A semantic segmentation algorithm for fashion images based on modified mask rcnn,' Multimedia Tools and Applications, vol. 82, no. 18, pp. 28 427-28 444, 2023.\n",
      "- [49] W. Wan, Y. Zhong, T. Li, and J. Chen, 'Rethinking feature distribution for loss functions in image classification,' in Proceedings of the IEEE' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- conference on computer vision and pattern recognition, 2018, pp. 91179126.\n",
      "- [50] K. Crammer and Y. Singer, 'On the algorithmic implementation of multiclass kernel-based vector machines,' Journal of machine learning research, vol. 2, no. Dec, pp. 265-292, 2001.\n",
      "- [51] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll√°r, 'Focal loss for dense object detection,' in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980-2988.\n",
      "- [52] H.-M. Yang, X.-Y. Zhang, F. Yin, Q. Yang, and C.-L. Liu, 'Convolutional prototype network for open set recognition,' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 5, pp. 2358-2370, 2022.\n",
      "- [53] X. Sun, Z. Yang, C. Zhang, K.-V. Ling, and G. Peng, 'Conditional gaussian distribution learning for open set recognition,' in Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 13 480-13 489.\n",
      "- [54] R. Shwartz-Ziv and N. Tishby, 'Opening the black box of deep neural networks via information,' ArXiv preprint, vol. 1703.00810, 2017.\n",
      "- [55] R. K. Srivastava, K. Greff, and J. Schmidhuber, 'Highway networks,' ArXiv preprint, vol. 1505.00387, 2015.\n",
      "- [56] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, 'Densely connected convolutional networks,' in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, USA: IEEE, 2017, pp. 4700-4708.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [56] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, 'Densely connected convolutional networks,' in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, USA: IEEE, 2017, pp. 4700-4708.\n",
      "- [57] M. E. Tipping and C. M. Bishop, 'Mixtures of probabilistic principal component analyzers,' Neural Computation, vol. 11, no. 2, pp. 443-482, 1999.\n",
      "- [58] T. DeVries and G. W. Taylor, 'Improved regularization of convolutional neural networks with cutout,' arXiv preprint arXiv:1708.04552, 2017.\n",
      "- [59] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, 'ImageNet Large Scale Visual Recognition Challenge,' International Journal of Computer Vision, vol. 115, no. 3, pp. 211-252, 2015.\n",
      "- [60] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, 'Cutmix: Regularization strategy to train strong classifiers with localizable features,' in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6023-6032.\n",
      "- [61] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr et al., 'Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers,' in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 6881-6890.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [62] Z. Liu, Y. Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, 'Swin transformer: Hierarchical vision transformer using shifted windows,' in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 012-10 022.\n",
      "- [63] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, 'Segformer: Simple and efficient design for semantic segmentation with transformers,' Advances in Neural Information Processing Systems, vol. 34, pp. 12 077-12 090, 2021.\n",
      "- [64] T. Dozat, 'Incorporating Nesterov Momentum into Adam,' in Proceedings of the 4th International Conference on Learning Representations, 2016, pp. 1-4.\n",
      "- [65] H.-M. Yang, X.-Y. Zhang, F. Yin, and C.-L. Liu, 'Robust classification with convolutional prototype learning,' in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 34743482.\n",
      "- [66] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, 'Learning imbalanced datasets with label-distribution-aware margin loss,' Advances in neural information processing systems, vol. 32, 2019.\n",
      "- [67] M. Yeung, L. Rundo, Y. Nan, E. Sala, C.-B. Sch√∂nlieb, and G. Yang, 'Calibrating the dice loss to handle neural network overconfidence for biomedical image segmentation,' Journal of Digital Imaging, vol. 36, no. 2, pp. 739-752, 2023.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='- [67] M. Yeung, L. Rundo, Y. Nan, E. Sala, C.-B. Sch√∂nlieb, and G. Yang, 'Calibrating the dice loss to handle neural network overconfidence for biomedical image segmentation,' Journal of Digital Imaging, vol. 36, no. 2, pp. 739-752, 2023.\n",
      "- [68] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E. Boult, 'Toward open set recognition,' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 7, pp. 1757-1772, 2013.\n",
      "- [69] A. Bendale and T. E. Boult, 'Towards open set deep networks,' in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1563-1572.\n",
      "- [70] Z. Wang, J. Li, Z. Tan, X. Liu, and M. Li, 'Swin-upernet: A semantic segmentation model for mangroves and spartina alterniflora loisel based on upernet,' Electronics, vol. 12, no. 5, p. 1111, 2023.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='<!-- image -->\n",
      "\n",
      "IEEE AccesS'\n",
      "\n",
      "- [71] B. Olimov, J. Kim, and A. Paul, 'Ref-net: Robust, efficient, and fast network for semantic segmentation applications using devices with limited computational resources,' IEEE Access, vol. 9, pp. 15 084-15 098, 2021.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ANDERSON BRILHADOR received the MSc degree in Computer Science from the Federal University of Technology -Paran√° (UTFPR), Brazil, in 2015. He is pursuing a Ph.D. in Electrical Engineering and Industrial Informatics with UTFPR. He is currently a professor in the computer science program at UTFPR. His research interests include computer vision, machine learning, deep learning, and data mining.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "RODRIGO TCHALSKI DA SILVA received MSc degree in Industrial Computing from the Federal University of Technology -Paran√° (UTFPR), Brazil, in 2022, and he is currently pursuing a Ph.D. degree in Industrial Computing at the UTFPR. His research interests include computer vision, tattoo recognition, machine learning, and complex networks.\n",
      "\n",
      "CARLOS ROBERTO MODINEZ-JUNIOR is currently pursuing an undergraduate degree in Electronic Engineering at Federal University of Technology -Paran√° (UTFPR), Brazil. He has been working with image processing and his research interests include semantic segmentation.' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='CARLOS ROBERTO MODINEZ-JUNIOR is currently pursuing an undergraduate degree in Electronic Engineering at Federal University of Technology -Paran√° (UTFPR), Brazil. He has been working with image processing and his research interests include semantic segmentation.\n",
      "\n",
      "GABRIEL DE ALMEIDA SPADAFORA is currently pursuing an undergraduate degree in Computer Engineering at the Federal University of Technology -Paran√° (UTFPR), Brazil. His research interests include image processing and neural networks.\n",
      "\n",
      "HEITOR SILV√âRIO LOPES received the BSc and MSc degree in Electrical Engineering from Federal University of Technology -Paran√° (UTFPR), in 1984 and 1990, respectively, and his PhD from the Federal University of Santa Catarina in 1996. Currently, he is a tenured full Professor with the Department of Eletronics and the Graduate Program on Electrical Engineering and Applied Computer Science (CPGEI) at UTFPR, Curitiba. His major research interests are in the fields of computer vision, deep learning, evolutionary computation, and data mining.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ANDR√âEUG√äNIOLAZZARETTI(MEMBER, IEEE) received his BSc, MSc, and DSc degrees in Electrical Engineering from the Federal University of Technology -Paran√° in 2007, 2010 and 2015. He is currently a professor with the Department of Electronics at the Federal University of Technology -Paran√°. His research interests include machine learning, deep learning, and digital signal processing. 0' metadata={'source': 'datasets/tattoo.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in chunks: \n",
    "    print(c)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
